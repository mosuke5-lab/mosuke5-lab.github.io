<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>2025 on Goldstine研究所</title>
    <link>https://blog.mosuke.tech/archive/2025/</link>
    <description>Recent content in 2025 on Goldstine研究所</description>
    <generator>Hugo</generator>
    <language>ja</language>
    <lastBuildDate>Fri, 04 Apr 2025 11:23:27 +0900</lastBuildDate>
    <atom:link href="https://blog.mosuke.tech/archive/2025/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Mac mini M4 Pro (64GB) はローカルLLMの夢を見るか？ パフォーマンス検証とAIエージェント連携の現実 </title>
      <link>https://blog.mosuke.tech/entry/2025/04/04/mac-mini-local-llm/</link>
      <pubDate>Fri, 04 Apr 2025 11:23:27 +0900</pubDate>
      <guid>https://blog.mosuke.tech/entry/2025/04/04/mac-mini-local-llm/</guid>
      <description>&lt;p&gt;こんにちは、もーすけです。&lt;br&gt;&#xA;昨今は、LLMの話題は尽きません。claudeやGeminiなどのクラウドLLMがすごいのはわかっていますが、 ローカルLLMがどこまでいけるのか、気軽に用意できるレベルでどこまでいけるのか、、、 とても気になっています。&lt;/p&gt;&#xA;&lt;p&gt;というわけで、新しいMac mini (M4 Pro, 64GBメモリ) を購入してみました！ Apple Silicon搭載Macは、その高い電力効率とユニファイドメモリによって、ローカル環境でのAI/機械学習タスク、特に大規模言語モデル（LLM）の実行において注目されています。&lt;/p&gt;&#xA;&lt;p&gt;今回は、この新しいMac miniでローカルLLMがどの程度実用的に動作するのか、いくつかのモデルサイズや量子化方式を比較しながら検証してみました。さらに、ローカルLLMをAIエージェント（Cline）と連携させて、少し複雑なタスクを実行できるかも試してみました。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
