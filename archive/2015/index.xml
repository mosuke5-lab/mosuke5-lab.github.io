<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>2015 on Goldstine研究所</title>
    <link>https://blog.mosuke.tech/archive/2015/</link>
    <description>Recent content in 2015 on Goldstine研究所</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Mon, 28 Dec 2015 15:00:00 +0900</lastBuildDate>
    
	<atom:link href="https://blog.mosuke.tech/archive/2015/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>2015年振り返り</title>
      <link>https://blog.mosuke.tech/entry/2015/12/28/150042/</link>
      <pubDate>Mon, 28 Dec 2015 15:00:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/12/28/150042/</guid>
      <description>去年に続き、2015年で技術分野でなにがあったか簡単に振り返りました。
2014年を思い返して… - Goldstine研究所
1. Vim::Factoryの開発と公開 
2014年の秋から@mogulla3と定期的にインフラ関連技術の勉強会をやってきていて、
はじめはインプットの勉強会だけを主にやってきていたのですが、
サービスを作る中でインフラ関連技術を駆使し勉強したいと思うようなっていました。
そこで、今年は「vimの設定をブラウザ上で即体感できるサービス Vim::Factory」の開発をしました。
DockerとWebSocketを使って、vimの設定をブラウザで即体感できるサービスを作った - Goldstine研究所
良かった点  新しい技術などを組み合わせながら学習しがいのあるサービスを作れたこと サービスの実現技術を公開しそれなりの反響を得られたこと  悪かった点  内部的なアーキテクチャの変更ばかりに時間を取られサービスそのものの改良ができなかったこと  2. インフラ関連の構築や運用、仕組みづくり 主に仕事の話になりますが、今年はインフラ関連の構築や運用に多く携わった気がします。
（結構、雑な感じに書き残します。）
(1) メールサーバの構築と運用 Postfixを使ったメールサーバの構築と運用に携わりました。
 そもそもメールサーバってなんだっけってところからはじまったのを思い出します。 とにかく運用を楽にしたかったので、その部分に多くの工夫をしました。 Ansibleを使って設定の変更などのリリース作業も簡単することや Zabbixのログ監視もいい感じに機能して、不正なメール送信とかもすぐ検知できるようにしました。  KVMの仮想環境上なので、環境の作りなおしも容易にしました。 冗長性のために２つのリージョンに分散させたりもしました。 目新しいことはないですが、わりと運用が楽な感じに作れたのでほんとによかったなと振り返って思います。  (2) インターネットから社内NWへの入口としてのリバプロサーバ構築 インターネットから社内NWにあるシステムを利用できるようにするために、
リバースプロキシサーバを構築しました。（運用はこれから）
いわゆるDMZ構成におけるリバプロです。
 技術的なところで言うと、corosync+pacemakerを使ってクラスタリングを組みました。  クラスタリングは思ったより奥が深く、どのようなクラスタを組むかかなり苦労しました。 障害時に相手側サーバの電源を落とす、いわゆるフェンシングなどをどう適切に使うかなど。 運用が始まってからそのあたりの実用性が確認できそうです。 ちなみにNginxでリバースプロキシたてました。  (3) 仮想環境構築とか 仮想環境というと、今までVagrantなどのツールとして、AWS、VPSなどのIaaSとして使うばかりでした。
今年は、KVMを利用してプロダクションの仮想基盤を作るなどやりました。
仮想化ってそもそもなんだっけ？というもう少し基礎よりの知識と向き合う機会がありました。
また、主に来年の話になるがOpenStackなどを使ったクラウド環境を「作る・運用する」にも携わっていきそうです。
すこしづつOpenStackをかじり始めました。
(4) Ansibleの活用と布教活動 Ansibleの利用は去年から始めていたことですが、
今年はより活用することと、社内での布教活動を行いました。
活用でいうと、サーバの構築はAnsibleで行ってアプリケーションのデプロイは
別の方法でやる（手動とか…）というふうになっていたので、
Ansibleを使ってアプリケーションのデプロイまですべて行うようにしてきました。
また、こういった取り組みを社内（部内）で広めて他のチームにも活用してもらおうと、</description>
    </item>
    
    <item>
      <title>インフラのデプロイとテストを同時実行できるようにしてHappyになった</title>
      <link>https://blog.mosuke.tech/entry/2015/12/17/192554/</link>
      <pubDate>Thu, 17 Dec 2015 19:25:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/12/17/192554/</guid>
      <description>はじめに
私が開発しているシステムでは、Ansibleでサーバ構築からアプリケーションのデプロイまですべて実行できるようにしています。 そして、serverspecを使って、インフラテストも行っています。
しかし、その運用にいくつか課題点がありました。
その課題点についてと、課題点へ対策したことについて書きます。
課題だったこと (課題1) デプロイとテストをそれぞれ実行していた Ansibleでのデプロイとserverspecのテストはそれぞれ別のコマンドで実行していました。
$ ansible-playbook site.yml -i $ bundle exec rake serverspec  2つ実行することが面倒であり、面倒であるがゆえにserverspecの実行を怠ったりしていました。
これではテストの効果があまり発揮できませんね。
(課題2) sudoパスワードをうまく管理できなかった 上のような課題1について、真っ先に以下の様にコマンドを続けることを思いつきました。
$ ansible -playbook site.yml -i ; bundle exec rake serverspec  ですが、これだとansible実行終了後にserverspecを実行する際にsudoパスワードが再度聞かれるため、
コマンドを打ったまま「放置」ができませんでした。
※もちろん、sudoパスワードを要求しないようにユーザ設定をすればできますが、多くの場合ではセキュリティ上難しかったりすると思います。ssh接続は鍵認証、sudoには必ずパスワードを要求するようにしています。
Ansibleもserverspecにもコマンド実行時にsudoパスワードを記述する方法があります。
Ansibleでは、ansible.cfgにsudo_passwordを記述、あるいはコマンド実行時に--extra-argsでsudoパスワードを指定できます。
serverspecでも環境変数でSUDO_PASSWORDが指定できます。
例 ）
ansible -playbook site.yml -i --extra-args=&#39;ansible_sudo_pass=xxxxxxxx&#39; bundle exec rake serverspec SUDO_PASSWORD=xxxxxxxx  ですが、おわかりの通り、コマンドの履歴にもパスワードが残ります。
なのであまり良い方法ではないと思っています。
(課題3) タスクの実行方法がバラバラ デプロイはansibleコマンドで実行、テストはrakeで実行、他のタスクはシェルスクリプト。。。
といったように、タスクによって実行方法が異なってしまう状況になっていました。
運用的にとても不便でしたので、１つに統一したいと思っていました。
いい感じに同時に実行してくれるRakeタスクを作った 上で述べたような課題点をクリアするように、下記の要件を満たすように工夫をしました。
 デプロイ、テストが同じ形式で実行できる sudoパスワードをベタ書きすることなく実行できる sudoパスワードの入力を一回だけにする  結論は、すべてRakeタスクで実行できるようにしました。</description>
    </item>
    
    <item>
      <title>Ansible、実行速度高速化の実験。ControlMasterとPipeliningについて</title>
      <link>https://blog.mosuke.tech/entry/2015/12/01/181304/</link>
      <pubDate>Tue, 01 Dec 2015 18:13:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/12/01/181304/</guid>
      <description>1. はじめに
Vim::Factoryの開発や、仕事などでAnsibleを使うことが多いのだが、
その実行速度があまりでないことに不満をもっていて、どうしたら早くできるか考えていました。
調べると、ControlMasterを利用してSSHのコネクションを再利用するとか、pipelineの機能を利用するとかでてくる。
が、それによってどのくらいの効果が得られるのかよくわからないし、仕組みもよくわかっていなかったので、仕組みの理解と実行速度の実験をした。
もう少し余談をすると、
ControlMasterを有効にすれば早くなることは前から知っていたが、
最近MacをEl Capitanに変えてから「なんか最近Ansibleはやいな〜」とか思っていて、
「OpenSSHのバージョンもあがったし、まさか。。。」と思って今にいきついている。
ControlMasterについて OpenSSH は、1 つの接続で複数のセッションを共有する(束ねる)「コントロール マスター」と呼ばれる機能を持っています。コントロールマスターを使用すると、 リモートホストに接続する最初のセッションは制御用のセッション(マスターセッショ ンと呼ばれます)として利用され、制御用のソケットを作成します。セッションを 共有する SSH クライアントは、この制御用のソケットを通じてリモートホストと 接続し通信を行います。
出典：「OpenSSH実践入門」
 上記のように、１つの接続で複数のセッションを共有するため、
Ansibleのようにタスク実行ごとにSSH接続するような場合には大きな効果を得ることができる。
Pipeliningについて Pipelining機能の説明の前に、軽くAnsibleの実行までの流れを説明する。
Ansibleは対象サーバにSSHログインしたあと実行するタスクのモジュールをファイルとして転送しそれを実行する。
Pipelining機能をなしの状態だと、このファイル転送とファイルの削除を１タスクごとに行う。
ansibleを-vvvvオプションをつけて実行するとわかるが、
４行目でファイルのPUT（転送）を、５行目の最後の方にrm -rfでディレクトリ・ファイルの削除を行っている。
&amp;lt;192.168.33.100&amp;gt; ESTABLISH CONNECTION FOR USER: deploy &amp;lt;192.168.33.100&amp;gt; REMOTE_MODULE command ls -l /root &amp;lt;192.168.33.100&amp;gt; EXEC ssh -C -tt -vvv -o ControlMaster=auto -o ControlPersist=300s -o ControlPath=&amp;quot;/Users/xxxxx/.ansible/cp/ansible-ssh-%h-%p-%r&amp;quot; -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o User=user -o ConnectTimeout=10 192.168.33.100 /bin/sh -c &#39;mkdir -p $HOME/.</description>
    </item>
    
    <item>
      <title>インフラテスト(serverspec)はじめました</title>
      <link>https://blog.mosuke.tech/entry/2015/11/02/161744/</link>
      <pubDate>Mon, 02 Nov 2015 16:17:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/11/02/161744/</guid>
      <description>※執筆後、業務でもserverspecを利用し始めたのもあり、業務レベルでの実践例も追記している。
運営中のVim::Factoryでserverspecを使ったインフラテストを導入したので、 導入理由や工夫している点、悩んでいる点について記述します。
Vim::Factoryについてはこっちみてね。 DockerとWebSocketを使って、vimの設定をブラウザで即体感できるサービスを作った - Goldstine研究所 1. serverspecってなによ 詳しくは公式サイトや書籍などを参考にして欲しいですが、
「サーバの状態をコードで自動的にテスト・確認するためのツール」です。
Serverspec - Home
例えば、ApacheでWebサーバを組んでいるサーバがあったとして、下記の要件で動いているとします。
  apacheがインストールされていること  apacheが起動していること、自動起動する設定であること ポート80があいていること  この要件をサーバが満たしているかコードでテストします。
上記の例だとこんなコードを書きます。
describe package(&#39;httpd&#39;) do it { should be_installed } end describe service(&#39;httpd&#39;) do it { should be_enabled } it { should be_running } end describe port(80) do it { should be_listening } end  各種テストの立ち位置 
 serverspecは、サーバの状態（正しく設定されたか）を確認するためのテストツールです サーバの振る舞いのテストは別のツールを使うことをおすすめします また、監視も一種のテストと言えます 一般的には監視はその実行頻度の高さから、振る舞いを監視することが多い 監視ツールで、Configファイルが正しいかは見ない  2. なんで導入したの？ serverspecを導入したのには大きく2つの理由があります。</description>
    </item>
    
    <item>
      <title>Ansibleを踏み台サーバ越しに実行する</title>
      <link>https://blog.mosuke.tech/entry/2015/09/25/232751/</link>
      <pubDate>Fri, 25 Sep 2015 23:27:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/09/25/232751/</guid>
      <description>タイトルの通りで、なにも特別なことはない内容。
そして、9月も終わりなのに今月はひとつも記事を書いていなかった。
KVMを使って仮想のゲストサーバを立てたが、
ゲストサーバはホストサーバと通信する用の（外に出る場合にはNAT通信で）IPアドレスしか持っていない状況で、
Ansibleの実行対象としたかったのが背景。
ホストサーバにAnsibleをいれるわけにもいかず、ホストサーバを踏み台にして、
Ansibleを打ちたかったというもの。

 SSHの設定ファイルを作る &#34;Ansibleで&#34; と書いたが要はSSHです。
まずはSSHで踏み台サーバを経由してAnsible実行対象サーバへ接続できるように準備しました。
これはいわゆる「多段SSH」というやつで、以前にもブログに書いたので復習です。
【VPS1台でインフラ勉強】多段SSH設定（おまけ） - Goldstine研究所
一般的には~/.ssh/configにこういった設定は書いたりもしますが、
Ansible実行の場合、端末に依存したくなかったので、
Ansibleレポジトリに別途ファイルを作ることにした。
## sshconfigという名前のファイルにした Host ansible-target HostName 192.168.33.10 User xxxxx ProxyCommand ssh -W %h:%p yyyyy@hostserver  上記のファイルを使って多段SSHできることを確認します。
$ ssh -F sshconfig ansible-target  Ansible実行時にSSH設定ファイルを利用する ここまで来たらとても簡単で、
ansible.cfgに下記を追記し、ansible実行時に上記のsshconfigを読み込まれるようにしました。
ansible.cfg
[ssh_connection] ssh_args = -F sshconfig</description>
    </item>
    
    <item>
      <title>(個人的) YAPC::Asia 2015ふりかえり</title>
      <link>https://blog.mosuke.tech/entry/2015/08/22/223025/</link>
      <pubDate>Sat, 22 Aug 2015 22:30:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/08/22/223025/</guid>
      <description>「ブログを書くまでがYAPC」
というわけで、8/21, 22とYAPCに参加したので、そこでの学んだこと、気になったことなどを振り返ります。
しかし、YAPCで聞いた公演内容を淡々とまとめるなどはしません。
単なる内容のまとめであれば、公開されているスライドなどをみるのが一番だと思いますので。
以下３点を中心に振り返ってみます。
 インフラ関連セッションについて 発表を聞いてよかった、今の自分に一番必要だったことについて 興味を持ったことについて（CONBU）  インフラ関連セッションについて 意図的も半分くらいあるんだけど、なんとなくセッションを選んでいたら、インフラ関連のものが多くなりました。
 世界展開する大規模ウェブサービスのデプロイを支える技術 - YAPC::Asia Tokyo 2015 Consulと自作OSSを活用した100台規模のWebサービス運用 - YAPC::Asia Tokyo 2015 3分でサービスのOSを入れ替える技術 - YAPC::Asia Tokyo 2015 我々はどのように冗長化を失敗したのか - YAPC::Asia Tokyo 2015  デプロイについて 大規模サービスでのデプロイにおいて、一台一台のホストがgit cloneして、bundle installやらせてーとかやると、
時間もかかるし、並列的にgit cloneした際などgitサーバが負荷的に危なくなってくる。
なので、予めライブラリとかすべてインストールされたものをターボールなどにまとめておいて、
それをプルしてくる形式のデプロイについて多くの発表がありました。
このデプロイ方法、並列で数百MBのファイルをダウンロードしても落ちないストーレジ（ようはAWSのS3）を前提に構築されている感がありました。
（というかそう言っていた）
相変わらずAWSはせこいなーと思っています（笑）
うちの環境では真似するのは難しいなーと思う部分もありますが、
それ以前にシステムの規模や用途によってデプロイのあり方も多種多様になることを改めて考えさせられました。
自分の環境にあった最適なデプロイ形態を探す日々がまた始まりそうですが、良いヒントになりそうなのはまちがいなしです。
 式年遷宮インフラストラクチャ Kenjiさんの式年遷宮インフラストラクチャ。 この考え方、ぼくも一度考えたことはありますが、ここまで実践してみた経験談がきけたのは面白かった。
いざというときに切り替わらない、切り替えられないという問題に対して、
自動ではないが、「切替訓練」ということで、定期的に冗長化の系を切り替えることはしてもいいのかなーと思ったりしています。
consulについて 実は、上であげた４つのセッション全てで共通していたのがconsulを使っていた。
Consul by HashiCorpwww.consul.io
正直、consulについて、名前くらいしかしらなかったのに、ここまで利用されていたので、時代に乗り遅れている感を感じた。 でも利用用途をみていくと、iaas環境で力を発揮するっぽいので、概要と用途だけ押さえておくか…といったところ。
今月中の課題です。
発表を聞いてよかった、今の自分に一番必要だったことについて たくさん学びのあったなかで、なんだかんだいっても、koemuさんの発表が今の自分が一番考えていてることであり、
一番必要なことであったように感じた。
辛いことをやめる！から始まる業務改善とInfrastructure as Code - YAPC::Asia Tokyo 2015yapcasia.</description>
    </item>
    
    <item>
      <title>デスクトップUbuntuにVNC接続。ついでにSSHローカルポートフォワードの復習。</title>
      <link>https://blog.mosuke.tech/entry/2015/08/13/000440/</link>
      <pubDate>Thu, 13 Aug 2015 00:04:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/08/13/000440/</guid>
      <description>完全に自分のための備忘録。内容はわりと薄め。
やったこと 最近、自作したPCにUbuntuをいれて使っているのだけど、
デスクトップPCなので、部屋でしか操作することができません。
他の部屋からノートPCでUbuntuを触れたらいいなーと思いその環境を整えることをしました。
主にやったことは以下の通りです。
  VNCサーバ構築について ノートPC（Mac）からのVNC接続について  SSHローカルポートフォワードを使ってのセキュアな接続について   UbuntuでのVNCサーバ構築について 今回利用しているUbuntuは「Ubuntu Desktop 14.04」です。
また、VNCの実現は標準でインストールされているvinoを使って行いました。
ご存知の方も多くいるかもしれませんが、vinoでのVNCは簡易的なもので、サーバ側のユーザがログアウトしていると使えません。
ですので、会社などでの利用には耐えないと思います。
ユーザーをログアウトせずにロック状態にしていれば使えます。
まずはデスクトップ共有の設定をします。
「デスクトップの共有」のアプリケーションを起動します。 
接続毎に要求するようにすると、サーバ側で毎度許可が必要なので、オフにします。
パスワードの設定はしておきましょう。
同じLANをつかんでる人に簡単に奪われてしまいますので。

ちょっと詳細な意味を把握していないのですが、
下記を実行しないとMacで接続すると「互換性のないバージョンです」的なこといわれました…すいません。
$ gsettings set org.gnome.Vino require-encryption false  設定ができたら、きちんとサーバとしてVNC接続を待ち受けているか確認します。
% sudo lsof -i:5900 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME vino-serv 24414 mosuke5 13u IPv6 156661 0t0 TCP *:5900 (LISTEN) vino-serv 24414 mosuke5 14u IPv4 156662 0t0 TCP *:5900 (LISTEN) % ps -ef | grep vino mosuke5 24414 24226 0 12:30 ?</description>
    </item>
    
    <item>
      <title>Packerやる前にKickstartはじめよう</title>
      <link>https://blog.mosuke.tech/entry/2015/07/31/211542/</link>
      <pubDate>Fri, 31 Jul 2015 21:15:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/07/31/211542/</guid>
      <description>1.はじめに
開発環境はVirualboxを使ったVagrantを利用しているが、
本番環境はAWSだったりKVMだったり違う仮想化機構で動作しているなんてことよくあると思います。
そういう環境下でどのように開発環境と本番環境の差分をなくしていますか？
わたしの場合、基本的にAnsibleを使ってプロビジョニングをしていますが、
そのプロビジョニング前のベースが異なってしまって困ることがよくあります。
一般に公開されているVagrantBox使ったら余計な設定が入っていたとか、すでにパッケージが入っていたとか…
そんな問題を解決しようとPackerを使おう！って考えました。
ですが、Packerも当たり前だけど魔法ではなく、Kickstartなどの自動インストールが前提なので、
Packerをやる前にKickstartを学習せよ、、、ということに気づきました。
ということでKickstartをはじめたよってことです。
2.kickstartってなに kickstartはOSのインストールを自動化する仕組みです。
anaconda社が提供するインストールの仕組みでRedhat系のOSが採用しているものです。
ですのでUbuntuだとPreseedっていう別の仕組みだそうです。（詳しくありませんっ）
で、Kickstartでなにができるかというと...
OSのインストールをしたことがある方ならわかるかと思いますが、
普通にDVDなどからインストールすると、
 言語はなににしますかー？ ホスト名なににしますかー？ パッケージはなにをいれますかー？  とか、聞かれて選択していく必要があります。 この作業を自動化できるのがkickstartです。
URLのようなもの。 はじめての自宅サーバ構築 - Fedora/CentOS - CentOS6 のインストール手順kajuhome.com
(おまけ)Ansible, Chef, Puppetとの違い？ kickstartにはプロビジョニング機能もついているため、
AnsibleとかChefとかPuppetとの違いは？住み分けは？と思うかもしれません。
明確に、住み分けが決まっているわけではありませんが、 個人的にはAnsibleやChefを実行する前の最低限の設定をkickstartにやらせようと思っています。
（一般的かとは思いますが…？）
Lee ThompsonのProvisioning Toolchainを参考にKiskstarのやる範囲をまとめると。 
Provisioning Toolchain: Web Performance and Operations - Velocity Online Conference - March 17, 2010 - O&#39;Reilly Mediaen.oreilly.com
3.Hello Kickstart!! Virtualboxを使ってKickstartを試しました。
3-1.用意したもの   Virtualbox  自分の環境はMacで、バージョンは5.</description>
    </item>
    
    <item>
      <title>DockerとWebSocketを使って、vimの設定をブラウザで即体感できるサービスを作った</title>
      <link>https://blog.mosuke.tech/entry/2015/07/19/135844/</link>
      <pubDate>Sun, 19 Jul 2015 13:58:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/07/19/135844/</guid>
      <description>2014年の秋から@mogulla3と定期的にインフラ関連技術の勉強会をやってきましたが、
インプットの勉強会だけでは飽き足らず、いつしかサービスを作る中でインフラ関連技術を駆使し勉強したいと思うように…
そして、普段使っているVimを題材に、
vimの設定をブラウザ上で即体感できるサービス Vim::Factory
を開発しました。
本記事はVim::Factoryの簡単な紹介と技術的な仕組みについて記述しています。
Vim::Factoryはこちら。
http://vimfactory.com/
1. Vim::Factoryについて 1-1. Vim::Factoryってなに？？ Vim::Factoryは、選択したVimの設定を、ブラウザ上で「即体感」できるサービスです。
数多くあり複雑なVimの設定を容易にし、お気に入りのVim探しをサポートすることを目指しています。

1-2. なんで作ったの？ Vimの設定ってたくさんあってどれを選んでいいかわからなかったり、
設定したもののどう変わったかイマイチわからなかったりしませんか？
Vimの設定がどのように反映されるか、もっと簡単に体験したいと考えたからです。
あと、例えばGithubで100star以上をつける人のVimをブラウザ上で体験できたらいいなと思っていて、
それを実現のための第一歩としてこのサービスを作りました。
1-3. このサービスの最大の特徴は？ このサービスの最大の特徴はなんといっても「ブラウザ上でVimが体感できること」です。
今まではVimの設定を試そうと思ったら、ネットで調べて自分のVimに反映させて…という作業が必要でしたが、
ブラウザ上で設定を即体感するという新しい体験を提供するために力を注いできました。
その実現方法については、後述しています。
1-4. 紹介動画 詳しくは、実際に試してもらうのが早いと思いますが、簡単な操作動画を用意してみました。
モバイルからこのサービスはちょっと使えないので、モバイルで読んでいる方は動画でお楽しみください(笑)
www.youtube.com
2. Vim::Factoryの技術について ここからVim::Factoryの技術について一部ではありますがご紹介します。
2-1. ブラウザ上でのVimを実現した技術 ブラウザ上でVimを実現しようと思うと、ぱっと思いつくのはJavaScriptでVimそのものを実装してしまおうというものかもしれません。
ですが、JSでVimを実装することってどれくらい難しいでしょうか？
少なくともぼくにはそんなことはできませんし、できたとして質の悪いものになってしまうと思います。
そこで思いついたのが、一般的なターミナルソフトと同様にサーバ上でvimを起動し、
そのターミナル情報をブラウザ上で表示するという方法です。
この方法であれば自らVimを実装せずともVimを再現できます。イメージは下記のとおりです。

また、サービスとして上記を行うには、接続してきたユーザごとにVimを用意する必要があります。
これらを実現するために利用したのがDockerとWebSocketです。
dockerコンテナ上でVimを起動し、そのターミナル情報をWebSocketでブラウザに送信するようにしました。 
dockerはコンテナ型の仮想化なので起動がとてもはやく、
httpのリクエストが来てからdockerコンテナを立ち上げても十分なほどのはやさをもっています。
2-2. 全体構成 システムの全体構成は以下のような感じです。
※実際の役割は図のとおりですが、サーバはこんなに多くありません。

2-3. 利用した技術とかツールのまとめ 振り返りも兼ねて利用した技術・ツールを一覧にまとめておきます。
 Ruby Sinatra thin node.js Websocket memcached docker nginx centos7 Ansible Vagrant gitlab mackerel slack  esa.</description>
    </item>
    
    <item>
      <title>PostgreSQL環境でFuelPHPのDBマイグレーションを使う</title>
      <link>https://blog.mosuke.tech/entry/2015/06/17/212852/</link>
      <pubDate>Wed, 17 Jun 2015 21:28:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/06/17/212852/</guid>
      <description>今更FuelPHP感はあるのだが、
postgresql利用時のFuelPHPのmigration導入について、注意点をまとめた。
でも、結論は納得がいっていない。
0. 前提 下記の環境で行ったものです。
PHP: 5.5.7
FuelPHP: 1.7
Postgresql: 9.4
1. テーブル文字コードの問題 事象 公式サイトのサンプルの通りはじめにapp/migrations/001_example.phpを作り、migrationを実行した。
app/migrations/001_example.phpの作成
&amp;lt;?php namespace Fuel\Migrations; class Example { function up() { \DBUtil::create_table(&#39;posts&#39;, array( &#39;id&#39; =&amp;gt; array(&#39;type&#39; =&amp;gt; &#39;int&#39;, &#39;constraint&#39; =&amp;gt; 5), &#39;title&#39; =&amp;gt; array(&#39;type&#39; =&amp;gt; &#39;varchar&#39;, &#39;constraint&#39; =&amp;gt; 100), &#39;body&#39; =&amp;gt; array(&#39;type&#39; =&amp;gt; &#39;text&#39;), ), array(&#39;id&#39;)); } function down() { \DBUtil::drop_table(&#39;posts&#39;); } }  マイグレーション実行すると以下のエラーに襲われた。
$ php oil refine migrate Uncaught exception Fuel\Core\Database_Exception: SQLSTATE[42601]: Syntax error: 7 ERROR: syntax error at or near &amp;quot;DEFAULT&amp;quot; LINE 5: )DEFAULT CHARACTER SET utf8; ^ with query: &amp;quot;CREATE TABLE IF NOT EXISTS &amp;quot;migration&amp;quot; ( &amp;quot;type&amp;quot; varchar(25) NOT NULL, &amp;quot;name&amp;quot; varchar(50) NOT NULL, &amp;quot;migration&amp;quot; varchar(100) DEFAULT &#39;&#39; NOT NULL )DEFAULT CHARACTER SET utf8;&amp;quot;  理由 しょっぱなから躓くわけだが…</description>
    </item>
    
    <item>
      <title>Ajaxの嫌いだった部分をJsRenderで心地良くした</title>
      <link>https://blog.mosuke.tech/entry/2015/06/13/231917/</link>
      <pubDate>Sat, 13 Jun 2015 23:19:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/06/13/231917/</guid>
      <description>1. はじめに
ぼくはフロントエンドは本業ではありません。
jsはあまり好きではありません。
そしてAjax通信後にhtmlをアウトプットする際にjsの変数の中にhtmlを書いていくソースコードがもっと好きではありません。(後述)
それをJSテンプレートエンジンを使ってシンプルにしてみたって話です。 （JsRenderの使い方を書いたものではありません。）
2. Ajaxが嫌いだった理由 Ajaxはユーザ体感的にはいいのだけれど、
Ajaxの結果受け取ったjsonなどのデータを使ってhtmlを出力とかやるとソースコードが煩雑になるので嫌いだった。
例としてAjaxで/xxxxxにリクエストを投げて、その結果(jsonデータ)を使ってhtmlを出力するものを考えると。
/* jsonデータは下記が返ってくるとする [ { id: &#39;1&#39;, name: &#39;らーめん&#39;, text: &#39;らーめんはやっぱり濃厚鶏そばです。&#39; }, { id: &#39;2&#39;, name: &#39;うどん&#39;, text: &#39;うどんはやっぱり釜揚げうどんです。&#39; } ] */ $.ajax({ type: &amp;quot;GET&amp;quot;, url: &amp;quot;/xxxxx&amp;quot;, dataType: &amp;quot;json&amp;quot;, success: function(data){ var html = &#39;&#39;; data.forEach(function (e) { html += &#39;&amp;lt;div id=&amp;quot;&#39; + e.id + &#39;&amp;quot;&amp;gt;&#39;; html += &#39;&amp;lt;h1&amp;gt;&#39; + e.name + &#39;&amp;lt;/h1&amp;gt;&#39;; html += &#39;&amp;lt;p&amp;gt;&#39; + e.text + &#39;&amp;lt;/p&amp;gt;&#39;; html += &#39;&amp;lt;/div&amp;gt;&#39;; }); $(&amp;quot;#result&amp;quot;).</description>
    </item>
    
    <item>
      <title>他人の家のインターネットを環境を整えて分かった無線LANルータのこと</title>
      <link>https://blog.mosuke.tech/entry/2015/05/24/220226/</link>
      <pubDate>Sun, 24 May 2015 22:02:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/05/24/220226/</guid>
      <description>他人の家のインターネットを環境を整えて分かった無線LANルータのことがあったのでまとめる。
我が家のインターネット環境は以下のような構成になっている。 
この構成では無線LANルータはL3とL2の両方の機器として働いている。
グローバルIPとプライベートIPの両方を持っており、
プライベートIPからの通信をグローバル側へルーティングする機能と、
LAN内の複数の端末に接続する機能として。
一方、この前、他の人の家のインターネット環境を整えたのだが、
以下のような構成だった。 
１つめの構成と決定的に違うところは、VDSLモデムにルータ機能もついていること。 この場合は無線LANルーターはL2の機器として働いている。
無線LANルーター自体にはIPアドレスはなく、DHCPでIPアドレス管理を行っているのも上位のルータだ。
この構成になるときは、一般的に光IP電話を利用するケースのようだ。
というのも、一般的な無線LANルーターには光IP電話につなぐことができず、
通信会社から貸与されるモデムルータを利用するため。
余談だが、今家では実は下記のような構成にしている。
というのも、家の構造上、VDSLがでているところが納戸のようなところで、
無線LANルーターを設置しても壁が多すぎるために電波が弱くなってしまう。
そのため、リビング側へ無線LANルーターを設置したかったからだ。

とても単純な話だが、
いろんなケースの家庭内インターネットの設定をすることで、
いろいろと気づくこともあった。</description>
    </item>
    
    <item>
      <title>sinatra-assetpackをproduction環境で使う時にはまったー</title>
      <link>https://blog.mosuke.tech/entry/2015/05/08/174732/</link>
      <pubDate>Fri, 08 May 2015 17:47:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/05/08/174732/</guid>
      <description>Sinatraアプリケーションで、JSファイルを圧縮するsinatra-assetpackを利用していて、
production環境で動作させようとしたら動かなくなってしまった問題について調査した。
起こったこと Sinatraを使ってアプリケーションを作っていて、development環境で完成したので、 prorudction環境で動作させようとしたら、jsのエラーが出るようになってしまい、正常に動かなくなった。
アクセスすると、以下のエラーがでる。要はjqueryがないとのこと。
Uncaught ReferenceError: $ is not defined  jQueryはもちろん読み込ませてるし、なんでproduction環境でだけ？？？
ソースコード sinatraのメインアプリケーションであるapp.rbには以下のように、sinatra-assetpackを利用してjsを読み込んでいる。
assets do serve &#39;/js&#39;, from: &#39;public/js&#39; serve &#39;/bower_components&#39;, from: &#39;bower_components&#39; js :app, &#39;/js/app.js&#39;, [ &#39;/js/index.js&#39;, ] js :libs, &#39;/js/libs.js&#39;, [ &#39;/bower_components/jquery/dist/jquery.js&#39;, &#39;/bower_components/bootstrap/dist/js/bootstrap.js&#39;, ] js_compression :jsmin end  layout.erbにはもちろん、libs.jsが先に来るように記述している。
&amp;lt;%= js :libs %&amp;gt; &amp;lt;%= js :app %&amp;gt;   sinatra-assetpackの挙動 productionでのみ発生する事象なので、改めてsinatra-assetpackのproduction環境時の挙動を確認した。
production環境では、複数のjsファイルを1つのファイルにまとめ、圧縮を行う。
development環境 ３つのjsファイルがあったら以下のように３つ別々に読み込まれる。
&amp;lt;script type=&#39;text/javascript&#39; src=&#39;https://blog.mosuke.tech/js/vendor/jquery.283479.js&#39;&amp;gt;&amp;lt;/script&amp;gt; &amp;lt;script type=&#39;text/javascript&#39; src=&#39;https://blog.mosuke.tech/js/vendor/underscore.589491.js&#39;&amp;gt;&amp;lt;/script&amp;gt; &amp;lt;script type=&#39;text/javascript&#39; src=&#39;https://blog.mosuke.tech/js/app/main.589491.js&#39;&amp;gt;&amp;lt;/script&amp;gt;  production環境 ３つあったjsファイルは1つにまとめられ、また圧縮される。</description>
    </item>
    
    <item>
      <title>Ansibleで最新のMySQLをインストールする際にハマったこと。MySQL-shared-compatのこと。</title>
      <link>https://blog.mosuke.tech/entry/2015/04/15/171127/</link>
      <pubDate>Wed, 15 Apr 2015 17:11:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/04/15/171127/</guid>
      <description>CentOS 6.5環境でAnsibleを使って最新のMySQLのセットアップをしようと思った際にハマったことをまとめた。
本質的にはAnsibleというよりLinux RPMパッケージのはなし。
ついでに、しょっぼいgithubを公開しました。
(1) 本記事を書くに至った経緯  Ansibleでmysqlを使ったサーバを構築(CentOS6.5)することになった。   MySQLのバージョンは5.6を採用した。    MySQLの公式rpmをダウンロードしインストールした。  インストールしたもの：MySQL-client, MySQL-devel, MySQL-server, MySQL-shared    MySQL-sharedをインストールする際にデフォルトのmysql-libsと競合  mysql-libsをアンインストールし再インストール AnsibleでMySQLの操作をするにはMySQL-pythonが必要なのでインストール  MySQL-pythonをインストールするにはさっきアンインストールしたmysql-libsが必要…(困った)  MySQL-shared-compatの存在に気づく 備忘録に書いておくか…  (2) MySQL-shared-compatの存在 mysql-libsは多くのパッケージの依存となっており、公式のMySQL5.6をインストールすることで、
他のパッケージがいれられない状況となっていた。
そんな状況を解決するためにMySQL-shared-compatというパッケージが用意されていた。
MySQL-shared-compatは「過去のMySQLバージョン向けの共有クライアントライブラリが納められているもの」だ。
詳細は下記参照をおすすめ。
MySQL-5.5.6から仕様が変わった「MySQL-shared-compat」の中身を徹底解剖 - Y-Ken Studio
ちなみに&#34;compat&#34;という単語がよく使われるが&#34;compatibility&#34;の略で「互換性」とかそういう意味。
(3) Githubで公開しました 内容は今のところ死ぬほど薄いのだが、MySQLをインストールするansibleを公開しました。 mosuke5/mysql-ansible · GitHub
内容はあれだが、特徴としては、インターネット上からRPMをダウンロードしてインストールする際に、
Ansibleでも「ダウンロード」→「インストール」の流れを踏む人が多いが、以下のようにするとシンプルになる。
varsでインストールしたいrpmやその取得先を記述しておいて、task側ではyumでnameにvarsで定義した変数を読むだけでできる。
role/mysql/vars/main.yml
mysql_url: http://ftp.jaist.ac.jp/pub/mysql/Downloads/MySQL-5.6 mysql_ver: &amp;quot;5.6.24-1&amp;quot; mysql_rpms: - MySQL-client-{{ mysql_ver }}.el6.x86_64.rpm - MySQL-shared-compat-{{ mysql_ver }}.el6.x86_64.rpm - MySQL-shared-{{ mysql_ver }}.</description>
    </item>
    
    <item>
      <title>SSHエージェントフォワード後に他のユーザでgit cloneする(鍵を使う)ことに関する考察</title>
      <link>https://blog.mosuke.tech/entry/2015/04/05/212518/</link>
      <pubDate>Sun, 05 Apr 2015 21:25:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/04/05/212518/</guid>
      <description>SSHのエージェントフォワードした後に、接続したユーザとは別のユーザでgit cloneしたいことがあった。
それについて調べていく中で学習したことや検討したことについてまとめた。
0. 前提 ローカルのPC(Mac)上で、Vagrantを使用してCentOS7の仮想サーバ(testsv)を立ち上げている。
&amp;lt;IPアドレス&amp;gt;
ローカルPC：192.168.33.1
仮想サーバ：192.168.33.100
本記事上での「git cloneする」とは、「プライベートのGitレポジトリからSSHを利用してクローンする」ということを指す。
1. SSHのエージェントフォワードを利用したい理由 まず、そもそもなぜSSHのエージェントフォワードをする必要があったのか。
最近では多くの方がご存知かつ利用していることだと思うが、仮想のサーバ上でgitを利用するときによく利用する。
(もちろんそれだけの用途ではありません)
仮想サーバを作るたびにSSHの鍵を生成して、Github等に登録するのが手間なので、
ローカルのPCの鍵を他のサーバへ引き継ぐことでgit clone等を可能にするのだ。
2. SSHエージェントフォワード利用時の挙動 SSHのエージェントフォワードで利用される認証情報は、接続先サーバの/tmp以下に保存されます。
[myuser@localpc ~]$ ssh -A vagrant@192.168.33.100 Last login: Sat Apr 4 xx:xx:xx 2015 from 192.168.33.1 [vagrant@testsv ~]$ [vagrant@testsv ~]$ ls -l /tmp | grep ssh drwx------. 2 vagrant vagrant 23 4月 4 11:35 ssh-skQVHsUCHU  
また、接続ユーザにはSSH_AUTH_SOCKという環境変数ができ、どの認証情報を利用するか記述がされます。
実際に確認してみる。
確認方法は、envコマンドで環境変数一覧を表示し、そのなかで&#34;ssh&#34;を含むものをgrep。
[vagrant@testsv ~]$ env | grep -i ssh SSH_AUTH_SOCK=/tmp/ssh-skQVHsUCHU/agent.9034 SSH_CLIENT=&#39;192.168.33.1 58017 22&#39; SSH_CONNECTION=&#39;192.</description>
    </item>
    
    <item>
      <title>Ruby, thin(bundler利用)を使った環境でのアプリの自動起動設定</title>
      <link>https://blog.mosuke.tech/entry/2015/02/22/211316/</link>
      <pubDate>Sun, 22 Feb 2015 21:13:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/02/22/211316/</guid>
      <description>bunlderを使ったWebアプリをプロダクション環境で動かすときに、アプリの起動をどうやって実現しているだろうか。
Apache Passengerを使う場合には、Apacheの起動がアプリの起動につながるので、 アプリの起動はあまり気にしなかったかもしれない。
しかし、例えばNginx × Unicorn/thinの構成などの場合はUnicornやthinの起動もしなければいけなくなってくる。
（あるいはこのようなケースがあるかは謎だが、Unicornやthinを単体で動かそうとしている場合など）
Unicornやthin（例ではthinを扱うが本質は同じ）の自動起動を実現する際の勘所、注意事項をまとめた。
0. 前提  CentOS6.5上でRubyでのWebアプリケーションを作っている。  アプリケーションサーバはthinを利用している。 また、gemパッケージ管理にbundlerを利用している。  1. 開発環境でよくするアプリの起動 開発環境では、アプリケーションのログの閲覧性なども兼ねて以下のようにアプリを起動していた。
$ bundle exec rackup $ bundle exec thin start  でも、これではいつまでたってもプロダクション環境での利用はできません。
2. 上記方法ではプロダクション環境で利用できない理由 当然のことながら、プロダクション環境ではいちいち手動でコマンドを実行しアプリケーションを立ち上げるわけにはいかない。
例えば、なんらかの理由でサーバが再起動してしまった場合には、
このままではアプリケーションが自動的に立ち上がらないため、サービスの停止につながってしまう。
ではどうするのか？
以下の状態であることがプロダクション環境では理想なのではないだろうか？
 オリジナルアプリケーションもserviceコマンドで起動・停止ができる  他のサービスと同様の操作方法が可能なのでわかりやすい   サーバ立ち上げ時にサービスが自動で起動される  3. 起動スクリプトを作ろう 上記の状態にもっていくためには、起動スクリプトを作らなければならない。
起動スクリプトを作る…！？
「作ったことないし、すぐには作れないよ〜」って思うかもしれないが、
サンプルはたくさんあるし、よく見てみるとそれほど難しくはない。
thinを使ったサンプルを探そうと思うと数は少ないが、Unicornも同じ仕組なので、 &#34;unicorn init script&#34;なんて検索をかけてもいろいろでてくるのでおすすめ。
参考ししたもの
https://gist.github.com/sbeam/3454488
上を参考にしながら、こんな起動スクリプトを作ってみた。（未完成版）
これを/etc/init.d以下へ配置する。
#!/bin/bash ### BEGIN CHKCONFIG INFO # chkconfig: 2345 55 25 # description: sample-app ### END CHKCONFIG INFO SCRIPT_NAME=/etc/init.</description>
    </item>
    
    <item>
      <title>勘違いしやすいFTPとSFTPの転送モードの話</title>
      <link>https://blog.mosuke.tech/entry/2015/02/17/220526/</link>
      <pubDate>Tue, 17 Feb 2015 22:05:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/02/17/220526/</guid>
      <description>FTPやSFTPでの転送モードの話。
ついこの前、WinSCPを利用している人が転送モードを選んでいて、
「SFTPには転送モードはないと思っていたのに、転送モードを選んでいる！？」
と疑問に思ったのでその辺りまとめた。
 FTPのバイナリーモードとアスキーモード 入社しはじめの頃、それまでSFTPしかほとんど使ったことなかったので、
先輩に「FTPではバイナリーモードを使って…」と言われて、意味が理解できなかったときがあったのを思い出す。
FTPにはファイル転送モードが２つあって、ちゃんと理解していないと思わぬところで痛い目にあう。
 バイナリーモード：ファイルの改行コードを変換せず転送する。  アスキーモード：OS側で異なる改行コードを自動的に修正して転送する。  FTPでは標準ではアスキーモードのため、なにも考えずにファイルを送るとファイルが壊れてしまったりする。
昔にミスしたのはWindowsからLinuxへtar.gzファイルをアスキーモードで送って、解凍したらファイルが壊れていたが、
それに気付かず壊れたファイルをサーバへ設置してしまったとか。
SFTPには転送モードはあるのか？ SFTPを普段から使ってる人は転送モードなんて気にしたことあまりないと思う。
FTPでは気にしなければいけない転送モード、SFTPでは気にしなくていいのだろうか？
結論から言うと、SFTPには転送モードはないので、気にする必要はない。
SFTPでは、FTPでいうバイナリーモードでファイル転送をするようだ。
sftpコマンドのマニュアルにも特に転送モードについては記述がないのがわかる。
http://www.unixuser.org/~euske/doc/openssh/jman/sftp.html
SFTPでも転送モードを選択できる場合がある！？ ここで、SFTPにも転送モードはあるぞ！？と疑問を思った人もいるかもしれない。
確かにWinSCPなどファイル転送ソフトを使っていると転送モードを選ぶことができる場合もある。
しかし、勘違いしてはいけないのが、
転送モードを選ぶことができるのはSFTPの機能ではなくてファイル転送ソフトの機能であるということだ。
まとめ ファイル転送でよく使われるFTPやSFTP。
それぞれに違いはあるし、それを利用するソフトウェアによっても違いがある。
何が何を行っているか把握し、思わぬミスを減らしましょう。</description>
    </item>
    
    <item>
      <title>SSHポートフォワード、https接続をするときに間違えやすいこと</title>
      <link>https://blog.mosuke.tech/entry/2015/02/11/172123/</link>
      <pubDate>Wed, 11 Feb 2015 17:21:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/02/11/172123/</guid>
      <description>SSHローカルフォワードの話。
前回は簡単に実践してみたというのを書いたのだが、今度は実際に使ってみてハマった部分があったのでメモ。
SSHでローカルポートフォワードを実際に試す - Goldstine研究所
1. やりたいこと httpsでしか接続を許可していないサーバへ、SSHローカルフォワーディングを使って接続しようした。
（直接疎通性がないためにポートフォワーディングする必要があった。）
2. 行ったこと httpsでしか接続ができないので、ローカル端末のポート5000を接続したいサーバのポート443に飛ばせばおっけーと思って、
下記のようにssh接続とブラウザから接続を行った。
ssh -L5000:web-host:443 user@ssh-host  ※web-host: 今回httpsで接続したサーバ
※ssh-host: ssh接続先サーバ
これでローカルフォワーディングの設定は終わったので、ブラウザから以下に接続するだけで終わりだと思っていた。
http://localhost:5000  が、接続不可…なぜでしょう？
3. 何が間違いだったか 正しくは以下で接続をしなければいけない。httpsが必要。
https://localhost:5000  よーく考えればアタリマエのこと。
URLのはじめの&amp;lt;http(s)&amp;gt;の部分はプロトコルで最後の&amp;lt;:5000&amp;gt;の部分はポート番号。
httpsは443のポートを一般的に使うが、ポート443がhttpsというわけではない。
あたりまえのことだし知っていることなんだけど、見落としがちかもしれない。</description>
    </item>
    
    <item>
      <title>リモートのサーバでdockerを起動させるときの端末割り当て</title>
      <link>https://blog.mosuke.tech/entry/2015/02/07/144208/</link>
      <pubDate>Sat, 07 Feb 2015 14:42:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/02/07/144208/</guid>
      <description>自分がハマったのでメモ。
リモートのサーバでdocker runを実行し(/bin/bash)、ローカル側でシェルを操作したかった。
sshでリモートサーバに接続し、docker runすればいいや、と思い以下を実行してみた。
ssh user@host &#39;docker run -t -i image_name /bin/bash&#39;  そうすると
[root@0c6742f02bd9 ~]# [root@0c6742f02bd9 ~]# ^[[A^[[A^[[C  エンターを押すと2行されるし、矢印キーはキーコードがでてしまう。
これを解消するのには以下のようにすればいい。
ssh -t user@host &#39;docker run -t -i image_name /bin/bash&#39;  -tとはなんなのか、なぜこのような事象が起きたのか、これからしっかり調べる。</description>
    </item>
    
    <item>
      <title>Ansible、コマンド実行結果を&amp;quot;ok&amp;quot;にする（冪等性を保つ方法）</title>
      <link>https://blog.mosuke.tech/entry/2015/02/02/201008/</link>
      <pubDate>Mon, 02 Feb 2015 20:10:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/02/02/201008/</guid>
      <description>Ansibleでソースコードインストールする際とか
すでにインストールされているかのチェックなどで、
シェルコマンドを実行してその結果で判断したい時がある。
ぼくがよくやる例では以下とか。
- name: check httpd installed command: which httpd ignore_errors: true  なんですが...
こうやってしまうと、仮に既にインストールされていて、正常なときでも&#34;changed&#34;と表示されてしまう。
これでは、本当にchangedなものなのか、わからなくなってくる。
これを解決するのにchaged_whenを使うといい。
- name: check httpd installed command: which httpd ignore_errors: true changed_when: false  こうするとコマンドが成功した際には&#34;ok&#34;が表示される。
これで、何も変化がないときにはokとskippingしかでないから、
誰がみても結果がわかりやすいですね！
秘伝のタレ回避！</description>
    </item>
    
    <item>
      <title>dockerで特定ユーザでログインした状態のシェル環境を提供する</title>
      <link>https://blog.mosuke.tech/entry/2015/01/24/213255/</link>
      <pubDate>Sat, 24 Jan 2015 21:32:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/01/24/213255/</guid>
      <description>dockerの一般的な利用の仕方ではあまり想定されないケースかもしれないが、
特定のユーザでログインした状態のコンテナを作りたいという場面に遭遇した。
&amp;lt;やりたいこと&amp;gt;  特定のユーザでログインした状態のシェルを提供すること その際、ユーザの.bash_profile（あるいあは.bashrc）を読み込んだ状態であること  .bash_profileに記載したPATHやaliasを使いたい ユーザのログインシェルを利用したい（カスタマイズされたシェルとか）    docker runに-uオプションがあるし、これで余裕！と思った。
[host] $ sudo docker run -u=user_name -i -t image_name /bin/bash  しかし…以下を確認してみると…
[docker] $ pwd [docker] $ echo $PATH [docker] $ alias  ディレクトリは &#34;/&#34; だし、PATHも通ってない。
どうやら.bash_profileなどは読んでいないようだ。普通にログインした状態とは違う。
dockerで-uでユーザを指定し場合、指定したuserでコマンドを実行するが、
サーバにユーザでログインしてからコマンドを実行するわけではないらしい。
ディレクトリはどうやら-wオプションで解決できるようだが…
[host] $ sudo docker run -u=user_name -w /home/user_name -i -t image_name /bin/bash  [docker] $ pwd /home/user_name  ディレクトリはおっけーだが、当然ながら依然として.bash_profileはダメ。
そこでふと思いついた。-uも-wもいらない。
あの手があるではないか…！！
[host] $ sudo docker run -i -t image_name su - username  suでスイッチユーザすれば.</description>
    </item>
    
    <item>
      <title>スーパーサーバってなに？ xinetdでサービスを常駐起動せずに利用する</title>
      <link>https://blog.mosuke.tech/entry/2015/01/02/013658/</link>
      <pubDate>Fri, 02 Jan 2015 01:36:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/01/02/013658/</guid>
      <description>使用頻度の低いサービスのデーモンをメモリに常駐させておくのは効率が悪い。
そこでスーパーサーバという使用頻度の低いサービスの窓口のサービスのみ起動しておき、要求があったときだけ特定のサービスを起動させることが可能らしい。
ということで、そのスーパーサーバとやらを実際に触ってみた。
スーパーサーバというとinetdとxinetdがあるらしいが、
xinetdはinetdの拡張版で、アクセス制御などの機能を搭載しているとのこと。
今回はxinetdを設定してみる。
1. 事前準備 【環境】
Vagrantで構築したCentOS 6.5
(仮想環境のIPアドレスは192.168.33.10)
まずはスーパーサーバで管理するサービスを考えなければならない。
SSHとかhttpはどう考えてもスーパーサーバの管理するようなものではないだろうし…
FTPやtelnet、POP3なんかのサービスに利用されることが多いそう？（このへんよくわかない）
今回はFTPをスーパーサーバの管理対象とした。
※本来は複数のサービスを管理対象とするからこそ意味がある。
まずはxinetdとvsftpをインストール
$ sudo yum install xinetd vsftpd  
xinetdどうこうの前に、ftp接続がきちんとできるか確認するのでサービスを起動。
$ sudo service vsftpd start  
ローカルPCから接続できることを確認する。
$ ftp 192.168.33.10 Connected to 192.168.33.10. 220 (vsFTPd 2.2.2) Name (192.168.33.10:username):  2. xinetdの設定 xinetdの基本設定は/etc/xinetd.confにかかれており、
xinetdで管理する各サービスの設定は/etc/xinetd.d/配下に書く方式。
ftpの設定を以下の通りにした。
&#34;service&#34;のあとに書くサービス名称は/etc/servicesに定義されているものを記載する。
vsftpとか書いても動かないので注意。
$ sudo vim /etc/xinetd.d/ftp service ftp { disable = no socket_type = stream wait = no user = root server = /usr/sbin/vsftpd log_on_failure += USERID }  設定項目については以下参照。</description>
    </item>
    
    <item>
      <title>2014年を思い返して…</title>
      <link>https://blog.mosuke.tech/entry/2015/01/01/161826/</link>
      <pubDate>Thu, 01 Jan 2015 16:18:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/01/01/161826/</guid>
      <description>2015年になってしまいました。
2014年に技術分野で印象に残ってること３つを思い返してみる。
1. インフラ会 @mogulla3と軽いノリで始めたインフラ会。
普段触らない、あるいは触ったとしてもすでに構築された環境で触ることの多いインフラ技術について
土日を使って自らの手で構築してみるといった会。
10月くらいから初めて以下を実際にやってみた。
 仮想化　：Docker, Vagrant  リバースプロキシ　：Nginx  ロードバランサー　：HAProxy  クラスタリング　：Pacemaker+corosync(heartbeat)  VPN　：SoftEther  構成管理ツール　：Ansible 自作PC  やったことについては全てではないがブログにまとめている。
 【VPS1台でインフラ勉強】サーバ複数台構成、Nginxでリバースプロキシ構築 - Goldstine研究所 【VPS1台でインフラ勉強】HAProxyでロードバランサーを構築 - Goldstine研究所 【VPS1台でインフラ勉強】SoftEtherを使ってVPN構築 - Goldstine研究所 【年末遊び】秋葉原で自作PCパーツ集めて作った - Goldstine研究所  このインフラ会では３つを目標にしてたけど、これが仕事でも本当に役立った。
会もそうだが、目標にしていたマインドは今後もぜひ続けていきたい。
 &#34;なんとなく知っている&#34;をなくす 考える引き出しをふやす 自らの手で実践する  &amp;lt;大変参考になった書籍&amp;gt; 
[24時間365日] サーバ/インフラを支える技術 ?スケーラビリティ、ハイパフォーマンス、省力運用 (WEB+DB PRESS plusシリーズ)
  作者: 安井真伸,横川和哉,ひろせまさあき,伊藤直也,田中慎司,勝見祐己  出版社/メーカー: 技術評論社   発売日: 2008/08/07  メディア: 単行本（ソフトカバー）  購入: 133人 クリック: 2,270回 この商品を含むブログ (289件) を見る     2.</description>
    </item>
    
  </channel>
</rss>