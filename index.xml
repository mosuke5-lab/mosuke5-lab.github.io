<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Goldstine研究所</title>
    <link>https://blog.mosuke.tech/</link>
    <description>Recent content on Goldstine研究所</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Fri, 14 Jul 2017 15:33:38 +0900</lastBuildDate>
    
	<atom:link href="https://blog.mosuke.tech/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Terraform×Rancherでマルチクラウドを一歩すすめる、を話してきた</title>
      <link>https://blog.mosuke.tech/entry/2017/07/14/master_cloud_malticloud/</link>
      <pubDate>Fri, 14 Jul 2017 15:33:38 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2017/07/14/master_cloud_malticloud/</guid>
      <description> 7月12日のMasterCloud#3で「Terraform×Rancherでマルチクラウドを一歩すすめる」という題で話してきました。
本記事ではスライドはもちろん、文面で補足しながら話してきた内容をまとめます。
スライド まずは当日のスライドについて公開します。
内容としてはYAPC::Fukuokaの前夜祭で話してきた内容の続きです。
YAPC::Fukuoka前夜祭でLTしてきた。「Rancherでマルチクラウドをやってみる」
 内容解説 はじめに 私は、AlibabaCloudの日本リージョンの担当などを普段しています。
実はAlibabaCloudは去年の12月から日本リージョン開設しています。
マルチクラウド 最近「マルチクラウド」という言葉をよく聞く気がします。
クラウド事業者としてはもちろん、AWSをはじめとするビッグなところだけでなく他の事象者も目を向けてほしいとも思っていますし、
ユーザ目線でもどんなクラウドサービスも落ちることもあるのでバックアップを用意しておきたいと思っているはずです。
あとは、良いサービスがあればマルチクラウドで使いたいというニーズも増えてきています。
マルチクラウドのメリットは他にもたくさんあると考えています。
1.価格を最適化したい クラウドサービスも裏では物理的なハードウェアで動作します。
なので新しいほうが当然ながら安くいいスペックのものが利用できます。
老舗のクラウドサービスもいいですが、新しいハードウェアが入ってきたクラウドサービスでは、 意外に安くいいスペックのものが使えたりします。
そのほか、キャンペーンなどで安くサーバが手に入る場合もあります。
2.DR,BCPとして活用する 1つのクラウドサービスに依存していると、そのサービスに障害が起きた際にサービスが全断しますよね。
最近でもオブジェクトストレージが落ちるなどいろいろニュースがありました。
特定のクラウドサービスに依存しないようにすることで可用性は高まります。
3.サービスロックインを避ける サービスロックインを避けたい、という考え方もあります。
特定の事業者、サービスのやり方に則ることで効率的になるということは過去からよくあることです。
一方で、次のイノベーションが起きたときに、脱却できず苦しんできた歴史もあったかとおもいます。
クラウドサービスでもそういうことは起きうるでしょう。
4.最適サービスを使いたい 今や、クラウドサービスありすぎな時代です。
自社の環境や組織にあったクラウドサービスを組み合わせ利用することは、 事業もコストも最適化することにつながるかもしれません。
Rancher そんなマルチクラウドのメリットがあるなか、Rancherというソフトウェアに出会いました。
RancherはDockerコンテナのオーケストレーションツールです。
特徴としてDockerが動く環境であれば、クラウドサービスの種類やリージョンなど問わず、 Dockerアプリケーションを動かすホストサーバとして利用できオーケストレーションできるツールです。
Rancherでのマルチクラウド Rancherを利用することで、さまざまなクラウドサービスをまたがってDockerアプリケーションをデプロイできます。
YAPC::Fukuoka前夜祭で自分がはなしてきました。
YAPC::Fukuoka前夜祭でLTしてきた。「Rancherでマルチクラウドをやってみる」
一方、このときに話したマルチクラウドは課題がありました。
すべてのアプリケーションをDockerとして動かすことを前提としている点です。
クラウドサービスは仮想サーバサービスばかりではなくデータベースサービスやストレージサービス、PaaSなどさまざまです。
これらをうまく使ってこそ本当のマルチクラウドではないかなと思っていました。
Terraform そこで思い出したのがTerraformです。
構築したいインフラをTerraformの書式で記述することで、Terraformがその通りにインフラを構築してくれるというものです。
Terraformはマルチベンダーで動作するインフラ管理ツールというのがまた特徴的です。
ちなみにAlibabaCloudもTerraform対応ベンダーなのですよ。
RancherとTerraform RancherとTerraformどちらともマルチクラウド的なツールと言えます。
ですが、そのカバーする領域は全く別物です。
Rancherはアプリケーションの動作環境（仮想サーバ）をマルチクラウド化します。
Terraformはクラウドサービス（マルチベンダー）の提供する様々なサービスの操作や管理を可能にします。
この２つを上手く使うと何か面白いことができるのはないか、そう考えました。
たとえば Rancherを使いつつ、仮想サーバだけでなくクラウド事業者の提供するサービスを組み合わせるとどういうことがきるのか。
例えばだが、サービス基盤はAlibabaCloudを使う。理由はDBサービスが優れているから。最新のCPUインスタンスを利用できるから。
しかし、データ分析はAWSのS3を起点としたワークフローが便利と思っている。
S3にデータを集め、Athenaを使うもよし、AWS上にRancherでHadoopを構築するもよし。
こういった使い方が簡単に出来るようになるのです。
デモンストレーション デモンストレーションでは、下記2つを主に行った。
 Terraformを使ってマルチベンダーのRancherホストを管理する方法 Rancher上で動くコンテナからAlibabaCloudのデータベースサービスを利用する方法  </description>
    </item>
    
    <item>
      <title>YAPC::Fukuoka前夜祭でLTしてきた。「Rancherでマルチクラウドをやってみる」</title>
      <link>https://blog.mosuke.tech/entry/2017/07/01/yapc_fukuoka/</link>
      <pubDate>Sat, 01 Jul 2017 15:18:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2017/07/01/yapc_fukuoka/</guid>
      <description>YAPC::Fukuokaに参加してきた。
今回は前夜祭にも参加してLTもしてきたので報告する。
「本当に最適なインフラ選べてる？Rancherでマルチクラウドをやってみる（超入門編）」
 登壇 実は、前夜祭ではなくてYAPC本体のほうで20分枠のスピーカを応募していたが、まあ流石に選考は通らず、 でもせっかく福岡に行くからには！ということで前夜祭のLTを申し込むことにした。 LTでは、Rancherを使ったマルチクラウドの考え方を話した。
いま自分はAlibaba Cloudの日本リージョンの運営とか日本での宣伝を仕事としてやっているが、 個人的な活動としてAlibabaCloudを表に出したのは初めてだった。
ただ、AlibabaCloudの宣伝だけするのはしたくなくって、 まだまだ日本で考え方が浸透していない「マルチクラウド」をキーワードにした。
僕自身、最近Rancherというコンテナのオーケストレーションツールと出会って、 マルチクラウドがこれからもっと流行っていくのではと感じた。
RancherをネタについでにAlibabaCloudも知ってもらえれば～というモチベーション。
ネタ話 福岡に来るときのフライトをなんと寝坊で逃した。
これはLTたのネタのためじゃないぞ！！？
けど、結果的にネタになったので良かったんじゃないかと思っている。
フライト逃してどうやって福岡に来たの？？
いろいろあったんですが…
はじめ振替できず、仕方なく新規にチケットを買おうと思ったら、なぜか券売機が故障したのもあって、
いろいろごたごたあって、振替をしてくれました。
魔法がかかりました（笑）
続きの話 今回は、マルチクラウドという考え方があるというところ、Rancherというものがどういうものか紹介するLTだった。
もうちょっと踏み込んだ話でどうやってマルチクラウドやっていくか、という続きの話をどこかでしたいと思っている。（というか準備中。）
MasterCloud(7月12日)で話せればなー…と思ってます。 https://mastercloud.connpass.com/event/59832/
さいごに LTは楽しい。福岡は飯がうまい。福岡、最高。</description>
    </item>
    
    <item>
      <title>Hugo、PageSpeed対策で自動で画像を圧縮する</title>
      <link>https://blog.mosuke.tech/entry/2017/06/12/hugo_optimize_image/</link>
      <pubDate>Mon, 12 Jun 2017 23:33:12 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2017/06/12/hugo_optimize_image/</guid>
      <description> はじめに 最近Hugoを使ったブログに移転した。
「はてなブログからHugoに移行。その際に行ったあれこれ。」
せっかくブログを運営するからにはSEOも少しがんばりたい。
PageSpeedで画像を最適化できるよっていわれたので、画像を圧縮させようと思った。
しかし、気がつくと忘れてしまったりするので、
Werckerを使って自動で最適化させることにした。
WerckerでのCI/CD環境 まず、Werckerを使ったCI/CD環境だが、こちらを参考にしてほしい。
「Werckerを使ってHugo+Github PagesのCI/CD環境を整備する」
画像圧縮処理 ブログの中で使う画像は、JPEGとPNGが混じっている。
そのため、両方に対応して画像を圧縮する必要があった。
画像の圧縮ツールはPageSpeedが推奨してきた、
optipngとjpegtranを利用することにした。
 OptiPNG: Advanced PNG Optimizer jpegtran  処理自体はいたってシンプル。
次のシェル(optimize_image.sh)を用意した。
#!/bin/sh find ./static/image/ -name &amp;quot;*.png&amp;quot; | xargs optipng -o5 find ./static/image/ -name &amp;quot;*.jpg&amp;quot; -type f -exec jpegtran -copy none -optimize -outfile {} {} \;  Werckerに組み込む wercker.ymlのbuildの段階で最適化を仕込めばおわり。
ただし、すべての画像ファイルをデプロイのたびに最適化すると時間がかかる。
定期的にローカルで圧縮してgitに更新しておいたほうがいいだろう。
あくまで、忘れてしまった時のためにCIでまかなってくれるというスタンスで利用している。
- script: name: optimize image size code: | sh ./scripts/optimize_image.sh  </description>
    </item>
    
    <item>
      <title>Werckerを使ってHugo&#43;Github PagesのCI/CD環境を整備する</title>
      <link>https://blog.mosuke.tech/entry/2017/06/04/hugo_deployment_with_wercker/</link>
      <pubDate>Sun, 04 Jun 2017 14:47:15 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2017/06/04/hugo_deployment_with_wercker/</guid>
      <description>以前、はてなブログからHugoを使ったサイトに移行したことを書いた。
こちら参照：はてなブログからHugoに移行。その際に行ったあれこれ。
今回、Hugoで記事を更新してからデプロイまでの流れをWerckerを使って自動化したので紹介する。
概要 今までは下記のフローでデプロイを行っていた。
一部シェルスクリプトにして自動化していたが、hugoファイルを管理するレポジトリとGithub Pages用の
２つのレポジトリへのコミットが必要で手間がかかっていた。
 hugoファイル側で記事更新、デザイン変更 更新のコミット、プッシュ hugoコマンドでビルド /public以下のファイルをgithub pages用のレポジトリへコピー github pages用レポジトリーへ移動してコミット、プッシュ CloudFlareのキャッシュ削除  このフローをWerckerを利用して下記のように変更した。
 hugoファイル側で記事更新、デザイン変更 更新のコミット、プッシュ Werckerでビルド Werckerでビルド結果をGithub pages用レポジトリへプッシュ WerckerでCloudFlareのキャッシュ削除やSlack通知  Werckerを使っていわゆるCI/CDのフローを組むことで、
hugo側のファイル・レポジトリ管理をするだけでよくなった。
werckerの設定 wercker.yml まず、wercker.ymlから記述する。
Werckerを利用するには自分のレポジトリにwercker.ymlを配置する必要があり、
このファイルに記述のとおりに自動化処理を行わせる。
本記事ではWerckerの細かい話は割愛するが、以下がwercker.ymlだ。
box: debian build: steps: - install-packages: packages: git - script: name: download theme code: | $(git clone https://github.com/dplesca/purehugo ./themes/purehugo) - arjen/hugo-build: version: &amp;quot;0.20&amp;quot; theme: purehugo flags: --buildDrafts=false after-steps: - slack-notifier: channel: $SLACK_CHANNEL url: $SLACK_URL username: wercker_bot deploy: steps: - install-packages: packages: git ssh-client curl - leipert/git-push: gh_oauth: $GIT_TOKEN repo: mosuke5-lab/mosuke5-lab.</description>
    </item>
    
    <item>
      <title>AWS Summit2017 Day2のぞいてきた。 ネットワーク設計入門メモ</title>
      <link>https://blog.mosuke.tech/entry/2017/05/31/aws_summit_network/</link>
      <pubDate>Wed, 31 May 2017 19:25:20 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2017/05/31/aws_summit_network/</guid>
      <description>はじめに 弊社はプレミアムフライデー導入企業なのだが、最終金曜日に早く帰る必要はなく、月内でどこか1日15時に帰りましょうという制度になっている。
金曜日が仕事の都合上取れなかったので、今月は5/31にプレミアムフライデー（プレミアムウェンズデー）を使った。
ちょうどAWS Summit開催中だったのでのぞきにいった。
雰囲気 想像していた雰囲気とはだいぶ異なっていたというのが第一印象。
想像以上にスーツの年齢層の高い人が多かった。それだけ、AWSもエンタープライズでも注目されるようになったということだろう。
以前に、JAWS2015に参加したことある。もちろんJAWSとAWS Summitでは参加者の層が違う。
が、あれから2年たちAWSに興味を持つ層も大きく変わってきたのを感じた。
セッションも非IT企業への導入事例や、思ったより入門セッションも多かった。
登壇者の話しぶりも「オンプレからの移行」を意識したように感じた。
ブース セッションだけではなく、AWSに関連するサービスを提供している企業がたくさんブースをだしている。
ここでのコミュニケーションが案外一番楽しい。お酒も用意されていて楽しくおしゃべりできた。
Heroku+AWSの組み合わせで利用するケースの話を聞いて、あーなるほどねって感じだった。
HerokuはAWS上で動作しているし、インターネット経由してもそこまで遅くはならない。
そんな利点を使って組み合わせて使う事例なんかきいた。
そのほかは、やっぱりどこもかしこもコンテナ。
コンテナをどう扱うか、どう監視するか、そんなところの話が多かった。
セッション「ネットワーク設計入門」 セッションは１つだけ「ネットワーク設計入門」を聞いた。
普段ならネットワーク設計入門のセッションは聞かないと思っているのだが、
どんなことを話すのか興味あって聞いてみた。
(1)クラウド上のNWの特徴  物理設計はいらない 可用性はすでにセット(VRRPなど気にしなくてよい） プロフラマブルに操作可能  (2)NWサービス NWサービスは意外とすくない。VPCがほぼすべて。
VPCが本質であり、ここが理解できればほぼ問題ない。
 VPC Direct Connect Route53  (3)前提知識 VPC 物理設計はいらないんだけど、
やっぱり物理知っていることがアドバンテージになる。 VPCはリージョンの中のみ。ゾーンはまたぐことができる。
専用線 当たり前だがDCの場所は公開していない。
じゃどうやって接続するか？
相互接続接続ポイントを用意しているからそこにつなぎに来てね、という考え方。
エッジロケーション CDNノードやRoute53が動作しているところ。
リージョンとはまた別にある。
(4)設計をはじめよう AWSのどのサービスを使いたいかでNWの設計方針はかわる。
まずはVPCの中で利用するサービスとVPCの外で使うものがあるのでその区別。
 VPCの中で使うもの  EC2とかRDSとかRedshift、EMR  VPCの外で使うもの  S3、LamdaとかDynamoDB、CloudWatch   次に、外部通信の設計
 VPCと外部を接続する場合  専用線orインターネットVPNorパブリック(ssh/https)  VPCがないけど外部から利用したい場合  httpsでまかなう場合が多い。 実は、VPCがなくてもDicrectConnectは使えるよ。  DirectConnectのパブリック接続    (5)プライベートNW設計のステップ  VPCの作成  VPCのCIDRレンジは変えられないから大きくとっておこう オンプレミスとのレンジも被らないように /16がおすすめ  サブネット作成  インターネットに接続するものとしないもの。ここでサブネット分けよう AZが落ちてもいいように設計しよう。サブネットは２つずつ サブネットサイズは24がおすすめ  VPCコンポーネントの作成  カスタマーGW インターネットGW VPC単位、サブネット単位、インスタンス単位で配置できるコンポーネントがあるよ  インスタンスの配置  セキュリティポリシーを考えよう セキュリティグループとネットワークACLがあるよ セキュリティグループのほうが柔軟  名前解決の検討  (6)ユースケースごとのNW設計 公開サービスの場合  インターネットから接続でいるのはロードバランサーだけにしよう  あるいはあとはメンテナンス用の踏み台サーバだけ  S3を活用するときは、VPCエンドポイントつかえばVPC内部から接続できるよ 管理拠点とはVPNでつなぐことをおすすめするよ 管理拠点とのルーティングはルートテーブル DNSはRoute53使うと便利だよ  ALIASレコードという独自機能 DNSのフェイルオーバー機能  ソーリーページへの転送が可能    社内システム基盤の場合  DirectConnectを使おう  パートナーがたくさんいるので連絡してみよう  Route53はプライベートゾーン、オンプレからVPC内の名前解決にも使えるよ DirectConnect体験ラボあるからつかってみて  https://aws.</description>
    </item>
    
    <item>
      <title>CloudFlare APIを使ってキャッシュを削除する</title>
      <link>https://blog.mosuke.tech/entry/2017/05/29/how_to_use_cloudflare_api/</link>
      <pubDate>Mon, 29 May 2017 20:06:31 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2017/05/29/how_to_use_cloudflare_api/</guid>
      <description>はじめに 最近ブログをはてなブログからHugoへ移行した。
HugoのフロントにCloudFlareを利用している。
ブログ移行についてはこちらを参照。
「はてなブログからHugoに移行。その際に行ったあれこれ。」
コンテンツをアップロードした場合などにCloudFlareのキャッシュを削除したく、
APIを利用して効率よく作業できる環境を整えた。
使い方 CloudFlareのAPIドキュメントはかなり充実している。
キャッシュの全削除については下記に記載がある。
https://api.cloudflare.com/#zone-purge-all-files
利用方法をみると、DELETE /zones/:identifier/purge_cacheとあるが、
:identifierがなんのことかはじめわからずはじめ苦戦した。
identifierの確認 identifierは下記APIで確認できる。
このAPIで返ってくるはじめのidがidenitiferだ。
curl -X GET &amp;quot;https://api.cloudflare.com/client/v4/zones \ ?name=&amp;lt;your site&amp;gt; \ &amp;amp;status=active \ &amp;amp;page=1 \ &amp;amp;per_page=20 \ &amp;amp;order=status \ &amp;amp;direction=desc \ &amp;amp;match=all&amp;quot; \ -H &amp;quot;X-Auth-Email: &amp;lt;your email&amp;gt;&amp;quot; \ -H &amp;quot;X-Auth-Key: &amp;lt;your api key&amp;gt;&amp;quot; \ -H &amp;quot;Content-Type: application/json&amp;quot;  { &amp;quot;result&amp;quot;: [ { &amp;quot;id&amp;quot;: &amp;quot;xxxxxxxxxxxxxxxxxxxxxxx&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;mosuke.tech&amp;quot;, &amp;quot;status&amp;quot;: &amp;quot;active&amp;quot;, &amp;quot;paused&amp;quot;: false, &amp;quot;type&amp;quot;: &amp;quot;full&amp;quot;, &amp;quot;development_mode&amp;quot;: 0, &amp;quot;name_servers&amp;quot;: [ &amp;quot;rudy.</description>
    </item>
    
    <item>
      <title>はてなブログからHugoに移行。その際に行ったあれこれ。</title>
      <link>https://blog.mosuke.tech/entry/2017/05/28/blog_migration/</link>
      <pubDate>Sun, 28 May 2017 13:02:14 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2017/05/28/blog_migration/</guid>
      <description>1.はじめに 2017年5月27日に2014年2月から約3年3ヶ月程度使ってきたはてなブログからHugoを使ったブログへ移行をした。
長らく使いやすいブログを提供してきたはてなさんにはとても感謝している。
はてなブログはとても魅力なブログプラットフォームであると感じているし、いまでもそう思うのだけれどいくつかの判断をした結果Hugoへの移行を決めた。
本記事では、移行を決めた理由や移行する際に行ったこと、Hugoの実行環境などを紹介する。
2.Hugoに移行した理由 Hugoに移行した理由というか、はてなブログから別のところに移行しようとした理由になるのだが、
端的に言うと以下のとおりだ。
 常時SSL化したかった HTTP/2に対応したかった 独自ドメイン(mosuke.tech)を利用したかった  Hugo以外にももちろん他のツールやサイトも検討を行った。
 はてなブログPro jekyll medium  まず、はてなブログProだが、もっとも手間がかからず独自ドメイン利用もできてよかったのだが、
SSL化とHTTP/2化はやはり難しかったので外部を検討した。
次にGithub製のJelyllだが、Github Pagesとの相性もよくはじめに検討はじめたものだった。
Ruby製ということもあり、自分に馴染みのあるツールで最有力候補だった。
しかし、後発のHugoの完成度の高さ、コンパイルの速さ、気に入ったテンプレートがあった、という理由でHugoに劣った。
最後にmediumだが、自前で構築することなくやりたいことのすべてを実現していた。
正直一番いいのではないかとも思う（笑）
最終的には、よりカスタマイズ度の高いHugoを選んだ。特にこれといった理由はない。
ちょうどGo言語をやってみたいモチベーションがあったので、これをきっかけに勉強がはかどればいいなぁくらいの気持ちはあった。
3.移行に際して行ったこと 3-1.Hugoでのサイト構築、アーキテクチャ Github上でHugoを管理し、コンパイルしてできたPublicファイルを、Github Pages対応の別のレポジトリで管理。
独自ドメイン利用、SSL対応、HTTP/2対応するためにフロントにCloudFlareを利用した。
後述するが、CloudFlareはとても便利なツールだが、キャッシュの扱いは気をつけてなければいけない。
図にすると以下のとおりだ。
3-2.記事の移行 はてなブログはそのままのこし、新規に書くブログからHugoへ移行することも検討したが、
せっかくなのではてなブログ時代に書いた記事もすべて移行することを決めた。
はてなブログからデータのエクスポートができる。
エクスポートしたファイルを簡単なスクリプトを作ってHugoファイルへの変換を行った。（Github mosuke5/hatena-blog-parser）
正直このツールは汎用的なものではない。このスクリプトだけではうまく行かない部分も多数ある。
いくぶんかsedなど使って（たまに手動編集・・・）して整えた。。
はてなブログのエクスポートファイルのフォーマット -------- AUTHOR: mosuke5 TITLE: 万能じゃない。オブジェクトストレージの仕組みと利用を正しく理解する BASENAME: 2017/03/18/182252 STATUS: Publish ALLOW COMMENTS: 1 CONVERT BREAKS: 0 DATE: 03/18/2017 18:22:52 CATEGORY: オブジェクトストレージ CATEGORY: ObjectStorage CATEGORY: クラウド CATEGORY: S3 IMAGE: https://cdn-ak.</description>
    </item>
    
    <item>
      <title>(備忘録) 運用サイトのドメインとサーバ</title>
      <link>https://blog.mosuke.tech/entry/2017/04/23/154957/</link>
      <pubDate>Sun, 23 Apr 2017 15:49:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2017/04/23/154957/</guid>
      <description>完全備忘録。自分でもわからなくなってきたので。 公開すればきっと更新もする。
docs.google.com</description>
    </item>
    
    <item>
      <title>万能じゃない。オブジェクトストレージの仕組みと利用を正しく理解する</title>
      <link>https://blog.mosuke.tech/entry/2017/03/18/182252/</link>
      <pubDate>Sat, 18 Mar 2017 18:22:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2017/03/18/182252/</guid>
      <description>1.はじめに
Amazon S3をはじめとして、オブジェクトストレージが身近になってきています。
各クラウドベンダーはオブジェクトストレージサービスを提供しています。
 Amazon S3 Azure Blob Storage Google Cloud Storage Alibaba Cloud OSS Cloud n ObjectStorage IDCF オブジェクトストレージ  ですが、オブジェクトストレージをストレージの魔法として理解されているケースも多いように感じます。
原点に振り返ってそもそもオブジェクトストレージとはなんなのか。
どんな特徴を持っているストレージなのか。
気になってまとめました。
2.オブジェクトストレージとは オブジェクトストレージとは一言で言うと、
「オブジェクト単位（ファイル単位）で出し入れのできる、ネットワークストレージ」です。
オブジェクトストレージでは直接にストレージ上のファイルをRead/Writeすることはできません。
いうなれば、FTPサーバに近い存在と言えます。
今やクラウド上のストレージの代名詞として扱われるオブジェクトストレージですが、
実はファイルの出し入れしかできないストレージなのです！？！？
3.特徴 では、そんな出し入れしかできないFTPサーバに似たオブジェクトストレージですが、
その本当の特徴はどこにあるのでしょうか。
特徴1: ディレクリ構造の排除 1つ目の特徴としては、ディレクトリ構造でファイルを管理しないことです。
ディレクトリ構造は、もしストレージサーバのハードディスク容量がいっぱいになり、
ファイルを別のディスクに移動する場合、そのディレクトリパスも変更しなければいけません。
クラウドサービスのようなたくさんのユーザが利用し拡張性の求められる場面では、ディレクトリ構造は適さないのです。
そこで、オブジェクトストレージではディレクトリ構造ではなく、階層のないフラットな関係でファイルが保存されます。
すべてのファイルにIDが付与され、そのIDがどこに保管されているか別で管理する仕組みとなっています。
特徴2: 分散保存 2つ目の特徴は「分散保存」です。
オブジェクトストレージでは、ファイルを分散保存するアーキテクチャによって、
ファイルの冗長化と大量のファイルへのアクセスさばくことを可能にしています。
詳しくは次の「オブジェクトストレージのアーキテクチャ」の項目でご紹介します。
特徴3: アプリケーションからの利用を意識 3つ目の特徴はアプリケーションでの利用を強く意識していることです。
この項目は製品によって異なる部分もありますが、主な点を３つあげます。
(1)メタ情報管理 従来のファイルシステムでのファイルへのメタ情報は、ファイルのサイズや更新日付などが一般的でした。
オブジェクトストレージでは更にファイルの有効期限などを設定することができ、インフラ管理を容易にします。
(2)HTTPプロトコルを使ったインターフェイス  オブジェクトストレージでは、ファイルのアップロード、ダウンロードなどすべての操作はHTTPプロトコルを利用します。
HTTPのような汎用的なプロトコルを採用することにより、サーバからはもちろん、モバイル端末など幅広いデバイスから利用が可能です。
(3)Web公開機能 更には、保存したオブジェクトに対してURLを割り当てて公開することもできます。
静的なWebサイトの公開や、cssやJavaScript、画像ファイルなどを直接オブジェクトストレージへ取得しにいくこともできます。
4.オブジェクトストレージのアーキテクチャ  オブジェクトストレージとひとまとめにいっても、製品によってその実現方法は様々で異なります。
しかし、ここでは一例として利用されるアーキテクチャについて紹介します。
※ここで紹介するアーキテクチャがオブジェクトストレージのすべてのアーキテクチャを表すものではありません。また、わかりやすくするためかなり簡略化して記載しています。</description>
    </item>
    
    <item>
      <title>参加してきた、MSPJマイグレーションコンペ2017winter</title>
      <link>https://blog.mosuke.tech/entry/2017/02/20/184220/</link>
      <pubDate>Mon, 20 Feb 2017 18:42:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2017/02/20/184220/</guid>
      <description>先日、2017年2月18日に「MSPJマイグレーションコンペティション2017winter」に参加してきた。
MSPJマイグレーションコンペティション2017winterとは、
日本MSP協会コンペティショングループが主催する、 次代を担う若手運用技術者同士の交流・競争を通して日本のMSP事業者における運用技術の向上を目指したコンペティション。
もう少し平たく言うと、MSP事業者の本当の業務に近い形でのコンペを通じて、スキルアップを図りましょうというものだ。
自分はMSPの人じゃないけど参加は全然できた。
connpass.com
競技ルール 今回の競技のお題は、
「AWS上で動作しているレガシーなRedmineをAzure上に移行する」というものだ。
このコンペの特徴としては、実際にMSPでの業務に則し、お客さんから曖昧な要望を受けている部分や、
お客さん側にしかない権限については、お客さんと調整する必要があること。
例えば、環境の移行する際にはDNSの切り替えが必要だったのですが、DNSの設定権限は我々にはなくて、
Slackを利用して、DNS設定変更依頼や作業周知を出さなければいけなかった。
このあたりはとてもユニークなポイント。
お客さんからは移行について以下のような曖昧な要望をもらっていた。
&amp;lt;要望&amp;gt; - 今の環境を新しい環境に完全移行して欲しいです。 - 実施した内容と結果については報告が欲しいです。 - システムを止めるときは利用者に告知が必要なので連絡が欲しいです。 - 昔から使っている古い環境なので、バージョンアップして欲しいです。 - できれば利用者に影響を出さないように切り替えたいです。 - できればサーバに関する資料があるとありがたいです。 - できれば今はまったくバックアップを取っていないのでバックアップを取れるようにしたいです - できれば今後は利用者が増えるのでシステムを冗長化したいです。 - できれば新しいインフラエンジニアに引継ぎするために必要な情報がまとまっていると嬉しいです。 &amp;lt;担当者のコメント&amp;gt; - 前任のインフラエンジニアが辞めちゃったのでこのシステムもう分かる人がいなくって。 - 結構前から使っているので環境も古くなっているみたいで、OSのサポートがもうすぐ切れるって話を聞いたものですから、セキュリティとか色々心配で何とかしたいんです。 - みんなこのシステムを結構便利に使っていてくれているようだから、システムを切り替えるときは連絡しないとなぁ。 - そうそう、近々新しいインフラエンジニアが入社予定だから、その方に引き継げるようになっていると嬉しいですね。  ちなみにチームについては、当日の参加者で適当に3人チームを作って行った。
一緒の参加者が同じチームにならないように調整された。
構成把握 開始後、まずやったことが環境・構成の把握。
ざっと下記のような感じ。ログインしてすぐに、pstree みて大体の構成を把握した。
 インフラ: AWS(EC2) OS: CentOS5.2 Webサーバ: Apache2.0 + Passenger DB: MySQL5.1  Ruby: 1.9  Redmine: 2.3  DNS; Route53で管理。権限はお客さんのみ サーバ構成: サーバ1台のシングル構成  移行作戦 細かなバージョンはおいておいて、最終的に目指す構成は下記のようにした。</description>
    </item>
    
    <item>
      <title>Cookpad TechConf2017にいってきたメモ</title>
      <link>https://blog.mosuke.tech/entry/2017/01/23/233756/</link>
      <pubDate>Mon, 23 Jan 2017 23:37:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2017/01/23/233756/</guid>
      <description>はじめに
CookpadTechConf2017に参加してきた。
昨年は抽選に外れていけなかったのでよかった。
techconf.cookpad.com
おなじみCookpadが年に一回行っているテクノロジーカンファレンス。
１年間のクックパッドでの取り組みを発表する場。
完全メモ書きではあるが、ご活用ください。
 クックパッドの取り組み セッションはたくさんあったが、クックパッドが今年１年間で取り組んできた大きな内容は以下３つと感じた。
 海外進出  機械学習への取り組み スケールへの対応  海外進出の話は今まではほとんど聞いたことなかったので、本格的に力を入れ始めたというところだろう。
機械学習への取り組みは去年からと明確にいっていたのでこちらもそう。
最後のスケールへの対応は今までもたくさん発表してきたが、そこに大きな波がもう１つやってきた。後ほど。
海外進出 Go Global  宗教、言語、気候によってサービスを変える  同じ言語圏であっても気候が違えば違う食文化がある 例えばスペイン語圏でも、スペインと南米では全く食文化が異なる   どうやってサービスを作っていくか  あたりまえ品質 グローバリゼーションとローカライゼーション   検索のローカライズははてしなくどろくさい 翻訳もとんでもなく大変   Amazonの例だが、Kindleで&#34;OR&#34;で誤訳があった  「あるいは」のorなのか、「オレゴン州」のorなのか   プレミアムという言葉も実は難しい  プレミアムというのは地方によってはプレミアムでないものを差別する用語として使われることもある プライムやプレミアム、エクストリームなど使い分ける     国際チームは国籍がみんなばらばら  それぞれの考え方も文化も異なる 最終的なアウトプットだけ共有しあとはまかせるというスタイルをとっている    Global Infrastracture  多くのリージョンへ展開している  Choose your Country - Cookpad でも中国は進出してない   クックパッドはAWS絶対 中国市場ではやっぱり厳しさがあるようにみえる     グローバルアプリはスクラッチで新規開発  いまは普通のRailsアプリ 日本のCookpadとはUIも全く異なる   グローバルのインフラについて   us-east-1のサーバを利用 データベースはAurora for MySQL  Elasticache利用 nginx, unicornの構成 CDNはFastly   地域によってトラフィックピークが全く異なる  日本だとバレンタインシーズン  イスラムではラマダンと呼ばれるシーズンがある  しばらく断食していたその後に食事を楽しむ時期      Go Global - #CookpadTechconf 2017 // Speaker Deck</description>
    </item>
    
    <item>
      <title>クラウド上でのWordPressのスケールアウトを考える</title>
      <link>https://blog.mosuke.tech/entry/2017/01/04/223544/</link>
      <pubDate>Wed, 04 Jan 2017 22:35:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2017/01/04/223544/</guid>
      <description>複数台サーバでのWordPressの構築・運用について考える。
実際に、とあるクラウドで分散環境のWordPressを構築したのでその知見をまとめる。
なるべく特定のクラウドに特価しないものとして記載したい。
やりたいこと  スケールアウトできるWordPress環境を作る  SSLに対応する HTTP/2に対応する  AWSなどのクラウド環境で構築する  アーキテクチャ まず先に全体のアーキテクチャ図から示す。
これから各項目について解説していく。

 SSL・HTTP/2への対応 まずSSLへの対応だが、通常ならばロードバランサをSSLの終端とし下記のような構成にすることが多いだろう。
この場合はロードバランサをL7のものとして稼働させる。

しかし、HTTP/2に対応しようと思うと事情は異なってくる。
（もちろん、最近ではAWSのALBのようにHTTP/2に対応する製品がでてきているのは承知だが。）
現在のパブリッククラウドで利用できるロードバランサの多くはまだHTTP/2に対応していない。
そのため、ロードバランサをL4として稼働させ、配下のWebサーバにてHTTP/2を処理する必要がでてくる。
この場合、ロードバランサはTCPでポート443を待ち受けるようにし、配下のWebサーバへポート443のままでフォワードすればいい。

クラウド環境ではWebサーバがスケールすることは前提にいれることがおおい。
そのため、この場合のSSL証明書はN台に対応した製品を買う必要がある。
例えば以下のような製品など。
 SureServer for クラウド｜Cybertrust.ne.jp  データベースの分離 分散環境でのWordpressでは共通したコンテンツを配信するため、データベースはもちろんWebサーバとは分離したほうがいい。
それぞれのWebサーバは共通のデータベースを見に行くべきだ。
データベースを自前で冗長化しても構わないが、それなりの運用コストがかかることは容易に想像がつくので、
クラウドのマネージドデータベースサービスを利用した。
 Amazon RDS（クラウドでのリレーショナルデータベースサービス） | AWS ApsaraDB for RDS - データベースホスティング | Alibaba Cloud  管理画面 管理画面のみを分離するアーキテクチャも考えられるが、ここではそうしないこととする。
管理画面へのログインセッションの保持は、別途KVS(RedisやMemcached)に保存してもいいと思う。
ですが、WordPress4.0以降ではデフォルトではMySQLへセッションを保存するので必須の対応ではないといえる。
github.com
記事で使うアップロード画像などの対応 管理画面から記事を投稿するとする。
記事のデータはデータベースに保存されるためどのWebサーバからも記事を参照できる。
しかし、記事に含まれる画像データはどうだろうか。
通常のWordpressでは管理画面サーバの/wp-content/uploads以下に画像を保存する。
複数台Webサーバがある状態で、たまたまアクセスしているサーバに画像を保存しても、他のサーバからは参照することができない。
これに対するソリューションはいくつかあるだろう。
例えば、rsyncなどを使って他のサーバと画像ファイルを同期するとか、画像用のストレージを用意しNFSで参照するとか。
冗長化の観点からもここはオブジェクトストレージのサービスを利用するのがいいだろう。
例えば、下記のような製品だ。</description>
    </item>
    
    <item>
      <title>「嵐」 2016年振り返り</title>
      <link>https://blog.mosuke.tech/entry/2016/12/25/142744/</link>
      <pubDate>Sun, 25 Dec 2016 14:27:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2016/12/25/142744/</guid>
      <description>だいぶお久しぶりのブログ。 そして恒例の年振り返りブログ。ついに2016年も終わってしまう。
というわけで2016年を振り返りたいと思う。
一応このサイトは技術ブログのはずなんだけど、プライベートのことが大いに混ざったポエムになってしまった。
なんの変哲もないはじまり 2016年のはじめのほう。振り返ってもとくに振り返ることがあまりないくらい変哲もない日々だった。
嵐の前の静けさだったと今思う。
TDD研修、プロマネ研修 後述するが8月までは、社内システムのエンジニアとして活動していた。
いつもどおり、内製開発のチームをどう強くするかばかり考え働いていた。
そんななか、新しい開発チームの模索のために部署を代表としてTDD（テスト駆動開発）の研修や、
プロジェクトマネージャーの研修など受けさせてもらった。
そのなかでもTDD研修はとても印象深い。
なぜなら、私が気に入っていた「Rubyによるデザインパターン」の翻訳者の一人が講師だったからだ。
さらには、研修の受講生は2人しかおらず、徹底してTDDを実践できた。
こんな恵まれた外部研修があるもんかと、感心したのを思い出す。

Rubyによるデザインパターン
  作者: Russ Olsen,ラス・オルセン,小林健一,菅野裕,吉野雅人,山岸夢人,小島努  出版社/メーカー: ピアソン桐原  発売日: 2009/04/01  メディア: 単行本  購入: 13人 クリック: 220回 この商品を含むブログ (67件) を見る     Gem公開、ATP-stat開発 会社のとあるプロジェクトでRails使うことになり、
趣味がてらにRailsを使ってATP-statというサービスを開発していた。
正直、ほんとに趣味で作ったものでクオリティは全く高くないし、最近はメンテもできていない。
AtpStat
また、このサービスに利用する仕組みをAtpScraperというGemにして公開した。
これは、せっかく受けたTDD研修を活かして、テスト駆動開発で行ったりしていた。
github.com
嵐 と、まあさほどいままでと変わらない感じの日々を送っていた。
だが、6月くらいから嵐が吹き始める。
プライベートでかなりショッキングなことがあった。ショッキングであると同時に生活がだいぶ変わるようなこと。 それだけでもショッキングではあるのだけれど、その出来事はなにかのトリガーを引いてしまったようだ。
世界線が変動したのかもしれない。
異動 まず、ショッキングな出来事からわずか1週間後に、社内転職の結果がでた。
詳しくは下記のブログを読んでほしいが、パブリッククラウドを提供する新規事業を行う会社に行くことになったのだ。
社内システム開発からパブリッククラウドの会社へジョインします - Goldstine研究所
仕事環境の変化 仕事の環境は劇的に変わった。正直何もかもが違うといっても過言ではない。
変わった項目をまとめてみた。
   項目 前の仕事 今の仕事     システム種別 ネットワーク監視システム パブリッククラウド   職種 システムエンジニア セールスエンジニア？   組織規模 大組織の中の１部署で100人程度 会社全体で数十人   チーム エンジニアのみ エンジニア、営業、マーケティングなど   お客様 社内 社外   人 ほぼ100%日本人 半分くらい外国人   英語 利用しない 飛び交ってる    実際の仕事 セールスエンジニア 最初に断っておくが、今の職種をセールスエンジニアというと少し語弊はあるのだが、</description>
    </item>
    
    <item>
      <title>三葉よ、サーバーレス、それもまた結び。</title>
      <link>https://blog.mosuke.tech/entry/2016/10/02/212420/</link>
      <pubDate>Sun, 02 Oct 2016 21:24:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2016/10/02/212420/</guid>
      <description>タイトルちょっとふざけました。 (が、半分本気。最後の方でわかる。)
ServerlessConf Tokyoに参加してきた。
今年8月からパブリッククラウドの事業に異動していて、
開発者の立場よりクラウド提供側の立場として参加してきたので、また面白かった。
せっかくなので、自分なりにサーバレスについてまとめる。
新しいことというよりは、自分の中での整理した感じ。
1. サーバレスってなんだっけ カンファレンスの中でもサーバレスの定義についてはいろいろな意見がでていた。
Martin Fowlerのブログではサーバレスの定義として下記２つが書いてある。
 BaaS (Backend as a Service) : ex) firebase FaaS (Function as a Service) : ex) AWS Lambda  martinfowler.com
ですが、ここでは焦点を絞って話すためにもFaaSということにしておく。
主にFaaSについて話したいのと、BaaSもいれてしまうとSaaSもサーバレスとかややこしいことになるので。
 AWS Lambda なんといってもサーバレスの概念を推し進めたのはAWS Lambdaでしょう。
説明はいまさら不要だと思うが、少しだけ。
コードを AWS Lambda にアップロードすると、サービスが AWS インフラストラクチャを使用してコードの実行を代行するコンピューティングサービスです。コードをアップロードして、Lambda 関数と呼ばれる関数を作成することで、AWS Lambda がコードを実行するサーバーのプロビジョニングおよび管理を行います。(https://aws.amazon.com/jp/lambda/details/)
 課金モデルは関数呼び出した回数、および実行に利用したコンピュートのスペックによって決まる。
また特徴的なことは、AWSの他のサービスで発生したイベントをトリガーに実行できること。
例えば、Amazon S3にファイルアップロードされたことをトリガーにLambdaを実行できるのだ。
サーバレスの特徴 サーバレス自体そしてサーバレスで実装することの特徴しては下記がある。
  クラウド上のイベントを契機に実行できる 実行環境は、immutableで時間が立つと消える 実行環境は独立していて、コードは基本的にstatelessである 上記のようにimmutableでstatelessな構造につくるからこそスケールしやすい  2. どんな用途で利用しているか  クラウド基盤のイベントをトリガーとして 個人的に一番強力だと思っている使い方。上で説明したとおりだが、クラウド上のプロダクトに対してのイベントをトリガーに処理を行うことができる。</description>
    </item>
    
    <item>
      <title>ISUCON6予選で惨敗した. 足りなかったのは&#39;Courage&#39;</title>
      <link>https://blog.mosuke.tech/entry/2016/09/19/172009/</link>
      <pubDate>Mon, 19 Sep 2016 17:20:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2016/09/19/172009/</guid>
      <description>Appleのkeynoteで話題の&amp;rsquo;Courage&amp;lsquo;使ってみた笑
ISUCON6予選で惨敗した。(18000点くらい)
端的に言って、とても未熟だった。
とはいえ、とてもいい思い出になったのでまとめる。
メンバー スリーエムというチーム名で、@mogulla3と@mintsu123と一緒に出場した。
ふたりともぼくよりもアプリの改善などは10倍くらい優秀なエンジニアなので、
ぼくはインフラとか総務的な立ち回りをして、２人がチューニングに集中できるようにすることを心がけていた。
準備 準備は３週間の間に土日どちらかに集まってISUCONの過去問を解いたり戦略について事前に打ち合わせしてた。
 プライベートレポジトリの用意(Gitlab) チャットルームの用意(Slack) ISUCON4とISUCON5の予選の過去問解き  土日集まったときには戦略や振り返りを重視 実際の過去問ときは平日に各々が空いた時間などにやってた   基本戦略を準備  なんの技術を主に使うか だれが何を担当するか 定形作業の手順化 その他ナレッジなど    採用した技術   PHP 7.0  php-fpm Openresty(nginx) 1.11  MySQL 5.7 Redis 3.2  当日 出だしはとても順調だった。
Azure担当だったぼくはすぐにサーバをデプロイし、OSバージョンを確認した。
予想通りのUbuntu 16.04であったので、準備したとおり必要なミドルウェアのインストールをすませた。
ほぼ定石と言える下記（定形作業と呼んでいた）もすぐにこなすことができた。
 調査のための各種ログ出力化 Nginxでの静的ファイルの配信、キャッシュ化 Kataribeインストールと実行  MySQLのインデックスの付与と設定見直し  php-fpmのUnixドメインソケット化 デプロイの仕組みの整理 不要デーモンの停止  この状態でもスコアは0のままであり、少し焦りを感じたが、
ここからが本番のチューニング開始である。
Kataribeの結果から、GET /が改善ポイントであることは明らかなのはわかっていた。
Top 20 Sort By Count Count Total Mean Stddev Min P50.</description>
    </item>
    
    <item>
      <title>社内システム開発からパブリッククラウドの会社へジョインします</title>
      <link>https://blog.mosuke.tech/entry/2016/07/29/180000/</link>
      <pubDate>Fri, 29 Jul 2016 18:00:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2016/07/29/180000/</guid>
      <description>本日、2016年7月29日をもって、新卒から３年４ヶ月働いてきた部署が最後となり、8月1日から異動（出向）する。
社内転職制度を使って、自らの希望でパブリッククラウド事業の会社へジョインすることになった。
（新規事業を行う部署へ異動となり、そこから別会社へ出向という扱い）
グループ内の異動ではあるが、違う会社・事業で、職種も変わるので、今の部署でやってきたことをまとめて残しておこうと思う。
私は通信会社のネットワーク運用部隊に所属している（いた）。
ネットワーク運用部隊なのだが、私の部署はネットワーク運用を自動化したり運用を楽にするためのシステム開発を担うところで、下記のような仕事をしてきた。
1. ベンダーコントロールという仕事 システム開発にはうちでは外注物も内製物（後述）もある。
業務の都合上、システムの種類によってはSIベンダーへ発注をして作ることがあった。
ベンダーコントロールなんて言ったりするが、発注でのシステム開発の業務では下記のようなことをしてきた。
 要求仕様の検討 見積もり依頼と価格交渉 発注、スケジュール調整 社内での業務調整 受入試験、検収  運用  仕事のほとんどは、社内外の人との調整（コミュニケーション）だ。
エンジニアとしては一見つまらなそうな仕事にみえるかもしれない。
しかし、この仕事から様々なコミュニケーションを学び、それはいろんな場面で役に立っている。
例えばだが、以下の様なコミュニケーションがあったりした。
 要求を他者にしっかり、わかりやすく伝える 仕様や価格についての折衝をする システムの利用部門との業務調整をする 作業の手順について精査し指摘する ミスなど良くないことが起きた際には、今後の対策はどうするか相手側に考えさせるよう導く 場合によっては厳しく叱ることもする（感情的に怒るわけではない）  特に価格の折衝などは、SIerや購買部と激しく激突することもあり今でもとても印象に残っている。
こういった業務はビジネスマンとしてとても大事なことを学んだと思うし、内製での開発業務でもとても活きてきている。
外注はいいけど・・・ 社内リソースが少なくても同時並行でいろんなシステムの開発ができるし外注はいい。
一方で外注開発について、もどかしさや非効率さなどもたくさん経験してきた。
まず、なにをやるにもお金と時間がかかることだ。
一度納品されてしまったものについて、なんらかの改修をしたい場合、
その改修規模を問わず、見積もり→発注→開発・改修→納品のプロセスを通さなければならない。
if文を１行追加するだけだろ…って思うようなものでも数百万で数週間かかることだってあった。
そして、プロセスの効率化が難しいことだ。
ベンダーが開発したシステムをリリースするには、発注側の会社に度々きてリリース作業を行う。
勝手に発注側のシステムをアップデートすることはありえないので、必ずリリース作業には社員が立ち会わなければいけない。
そのとき、リリース作業が自動化されていないことも多く（発注時の要求によってもちろん異なる）、
何時間もかけて数十台のサーバにデプロイしたりしなければいけなかったりするので大変だ。
これは当たり前だがとても効率が悪いし時間の無駄だ。
だがこれを改善しようと思うとまたお金がかかるわけである。
扱っているシステムが、業務システムなのでアップデートの頻度がおおくないこともあるので、
はじめからデプロイの自動化などを要件にいれることは少ないのである。
これらはSIの開発をディスっているわけではない。（要求も悪いのはわかる。）
これは仕方ないこととして、そのメリット・デメリットをきちんと理解した上で選択、要求をしなれけばいけないということだ。
2. 内製開発の仕事 外注開発とは別にシステムの内製での開発業務も多くおこなってきた。
社内的には外注開発から内製開発に徐々に切り替えの最中であった。
ちなみに開発言語はRuby（RailsやPadrino）やPHP（FuelPHP）なんか使っていた。
業務システムの他にもメールサーバやリバースプロキシサーバなど基盤システムも構築してきた。
2015年の振り返りブログに雑だが少し書いていた。
2015年振り返り - Goldstine研究所 開発組織の改善活動 また、開発組織を改善するための活動をおおく行ってきた。
どこの組織でもある問題だと思うが、うちもまた「属人化」「秘伝のタレ」などといった類の悩みをたくさん抱えていた。
うちはソフトウェア企業ではないし、システムを外注で作る部署も多い。
そのため、新卒や異動してくる人などがソフトウェアエンジニア思考の人ばかりではない。というかむしろ少数派。
だからこそ、よりいっそう「属人化」「秘伝のタレ」が弊害となる。
わかりやすいところでいうと下記のようなことをやったりして開発組織の改善をしてきた。</description>
    </item>
    
    <item>
      <title>【めも】httpヘッダー、x-forwarded-forとか任意のヘッダーとか</title>
      <link>https://blog.mosuke.tech/entry/2016/07/26/215939/</link>
      <pubDate>Tue, 26 Jul 2016 21:59:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2016/07/26/215939/</guid>
      <description>ただのめも。
もともとApache+PHPで動作していたシステムに、リバースプロキシ（Nginx）を前段に挟むことになった。（理由はここではどうでもいいので書かない）
つまり、Nginx-&amp;gt;Apache-&amp;gt;PHPという構成になった。
よくあることだが、Apacheからみるとすべてリバースプロキシから通信がきているので、 接続元のIPアドレスがすべてリバースプロキシのものになる。
HTTPヘッダーに接続元のIPアドレスを追加しアプリ側（PHP）で受け取ろうとしたときのめも。
リバースプロキシ側でHTTPヘッダー追加 まず、そもそもデフォルトのNginxの設定では接続元のIPアドレスをHTTPヘッダーに含まれない。
ググればすぐに設定方法自体はでてくる。
X-Forwarded-Forというヘッダー名にNginxでもっている変数$proxy_add_x_forwarded_forをつっこむ。
proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;  設定は簡単なんだけど、そもそもX-Forwarded-Forなんていうヘッダーあったっけ。。。？
Wikipediaでみる。
X-Forwarded-For (XFF) とは、HTTPヘッダフィールドの一つ。HTTPプロキシサーバまたは負荷分散装置（ロードバランサ）を経由してウェブサーバに接続するクライアントの送信元IPアドレスを特定する際のデファクトスタンダードである。 （略）RFCの標準的なヘッダフィールドではないが、IETFのネットワーク作業部会 (Network Working Group) は2011年10月より同種のHTTPヘッダForwardedの標準化作業を開始した[1]。
 なるほど、RFCの標準ではないけど、一般的なものなんですね。
 phpでX-Forwarded-Forを受け取る というわけで、おりゃ！
echo $_SERVER[&#39;X-Forwarded-For&#39;];  エラー...
※普段PHP使ってないのがバレますね。
サーバ変数とりあえず、全部はきだす。
&amp;lt;?php var_dump($_SERVER); # array(x) { [&amp;quot;HTTP_X_FORWARDED_FOR&amp;quot;] =&amp;gt; string(12) &amp;quot;192.168.33.1&amp;quot; ...... }  HTTP先頭についてて、大文字になってて、ハイフンがアンスコに変わっている。
あたりまえだけどこれはPHPの仕様でいいんだよな...？
&amp;lt;?php var_dump(getallheaders()); # array(x) { [&amp;quot;X-Forwarded-For&amp;quot;] =&amp;gt; string(12) &amp;quot;192.168.33.1&amp;quot; ...... }  サーバ変数にいれるときに、変わるんだわ。
念のためtcpdumpで軽く確認してみる $ sudo yum install tcpdump $ tcpdump dst port 80 -X # ながいんで適当に端折りました 11:04:01.</description>
    </item>
    
    <item>
      <title>Vim::Factory、LTではなす</title>
      <link>https://blog.mosuke.tech/entry/2016/07/03/224531/</link>
      <pubDate>Sun, 03 Jul 2016 22:45:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2016/07/03/224531/</guid>
      <description>7月6日の会社のエンジニアイベントでLTするやつ、先にあげておく。
自分の中でのネタとしては古いけど、話すのはなんだかんだ初。</description>
    </item>
    
    <item>
      <title>IkaLog環境整えた。画面が突然映らなくなる事象とかについて</title>
      <link>https://blog.mosuke.tech/entry/2016/06/25/113009/</link>
      <pubDate>Sat, 25 Jun 2016 11:30:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2016/06/25/113009/</guid>
      <description>昔まで、人の家でスプラトゥーンをやったりたまに借りたりして楽しんでいたが、
ついにというか今更というか自宅用に買ってしまった。
ついでにという感じで、お金を注ぎ込んで、IkaLogを楽しめる環境も作った。
IkaLogあるととても楽しい。
IkaLog環境はほぼ下記ブログを参考にしたので、詳しいことはかかないが、
全体図やハマったポイントなどを中心にまとめておく。
雑記：Mac + IkaLog + stat.inkで戦績管理 - 新人SEの学習記録
完成後写真 
図にしてわかりやすくすると 
突然画面が映らなくなることが起きた！！ ゲームを楽しんでいた時突然、ディスプレーに画面が映らなくなることが度々あった。
ケーブルを抜き差ししても直らない。
ただ時間が経つと映るようになる・・・
原因が最初わからなかったのだが、
どうやらHDMIスプリッターの電源供給が追いつかなくなった時になっていた。
HDMIスプリッターはHDMIからある程度給電することができるので、はじめはそのまま利用していた。
だが、IkaLogを回し始めるとどうやら給電が追いつかなくなるようであった。
説明書にも下記のように書いてあって、「あっ」って感じだった
入力機器側の電力不足により正常に表示できない場合に使用します。
 USB経由で別途電源供給をするようになってから事象が発生しなくなった！
※画面が映らなくなってガチマッチ何度負けたことか・・・</description>
    </item>
    
    <item>
      <title>Nginxの仕組みについて入門</title>
      <link>https://blog.mosuke.tech/entry/2016/06/04/180122/</link>
      <pubDate>Sat, 04 Jun 2016 18:01:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2016/06/04/180122/</guid>
      <description>普段Nginxを使ってはいるものの、その仕組やなぜNignxを使うべきなのかというところがあまりわかっていなかったので、 改めてNginxを入門しその仕組などについて調べてみた。
勉強不足もあって、間違ってることもあるかもしれないがその際は教えて下さい。
1. C10K問題 まず、Nginxを理解する上でC10K問題について知る必要がある。
C10K問題とは下記の問題のことだ。
ハードウェアの性能上は問題がなくても、クライアント数があまりにも多くなるとサーバがパンクする問題のこと。 C は「Client（クライアント）」の頭文字、10K は「1 万台」を意味する。「クライアント 1 万台問題」ともいわれる。
（C10K 問題とは - はてなキーワード より引用）
 2. 従来のWebサーバのアーキテクチャ  prefork Apacheのデフォルトのアーキテクチャなどで採用されている。
特徴としては、１リクエストに対して１プロセスが処理する。
すなわち同時接続が 1000であれば 1000個のプロセスが必要となる。

worker リクエストはスレッドが処理する。
スレッドは１プロセスのなかで複数立ち上げることができる。
プロセス数は少なくて済むが、同時接続が1000であれば1000個のスレッドが必要となる。
Apacheで指定可能なアーキテクチャの１つ。

いずれにしても、1000の同時接続があれば、1000のプロセスやスレッドが必要ということだ。
プロセス/スレッド数が多くなってくると「コンテキストスイッチ」が多く発生し、処理が遅くなってしまう。
 コンテキストスイッチとは コンテキストスイッチとは、コンピュータの処理装置（CPU）が現在実行している処理の流れ（プロセス、スレッド）を一時停止し、別のものに切り替えて実行を再開すること。
（コンテキストスイッチとは｜コンテキストスイッチング｜context switch - 意味/定義 ： IT用語辞典 より引用）
 3. Nginxのアーキテクチャ  NginxがC10K 問題を解決するために開発されたともいわれている。
上記のような課題を解決するようなアーキテクチャを採用している。

ワーカプロセスがリクエストを処理する。
前述のpreforkやworkerと決定的に違うところは、１つのワーカプロセスが複数のリクエストを処理するということだ。
また、マスタープロセスはワーカプロセスの立ち上げや、制御、管理などが主な仕事。
Nginxを終了あるいは再起動、設定ファイルを再読み込みして変更を反映するなど。
下記のような仕組みがNginxの高速化（C10K問題対応）を可能としている。
シングルスレッド 接続ごとにプロセスやスレッドを立ち上げていては、C10K問題に対応できない。
そこで、Nginxは１つのスレッドで処理を行うようにしている。
それにより、コンテキストスイッチと呼ばれるCPUの切り替え処理が発生しない。
正確に言うと、完全に１スレッドで処理するわけではなく図にあるようにワーカプロセス自体はマルチプロセス化できる。
イベント駆動 通常のプログラムは上から書かれた順に実行されますよね。
イベ ント駆動で動作するプログラムは何かしらのイベントが発生するまで待機し、発生したイベントの種類に応じて実行される。</description>
    </item>
    
    <item>
      <title>今度こそ入門するtmux</title>
      <link>https://blog.mosuke.tech/entry/2016/04/15/002004/</link>
      <pubDate>Fri, 15 Apr 2016 00:20:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2016/04/15/002004/</guid>
      <description>tmuxの入門は何度目だろうか…
SoftwareDesignの2015年7月号を見ながら、何度目かの入門をしたのでメモする。
ブログに書いたので、これで運用にのりそうだ。
tmuxってなに？ 公式ページ(http://tmux.github.io/)から引用すると。
tmux is a terminal multiplexer. What is a terminal multiplexer? It lets you switch easily between several programs in one terminal, detach them (they keep running in the background) and reattach them to a different terminal. And do a lot more.
（訳）tmuxはターミナルマルチプレクサです。ターミナルマルチプレクサは複数のプログラムを容易に1つのターミナル上で切り替えられるようにします。またそれらのプログラムをバックグラウンドで動かしながらデタッチしたり、別のターミナルにアタッチすることができます。さらにいろいろなことができます。
 tmuxを使ってみる インストール 今回Macの環境で行っています。
今日時点ではバージョン2.2まででていますが、brewでは2.1がインストールされました。
$ brew install tmux $ tmux -V tmux 2.1  起動 $ tmuxと入力すると、tmuxが起動し$ exitで抜けることができます。
これから説明していきますが、tmuxの機能を利用するときプレフィックスキーを入力する必要があります。
デフォルトではプレフィックスキーはCtrl-bになっています。
ウィンドウの作成、切り替え １つのtmuxの中にウィンドウ（タブ機能と考えてください）を作って切り替えたりします。</description>
    </item>
    
    <item>
      <title>HerokuをRailsアプリのステージング環境として使う</title>
      <link>https://blog.mosuke.tech/entry/2016/03/28/181636/</link>
      <pubDate>Mon, 28 Mar 2016 18:16:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2016/03/28/181636/</guid>
      <description>趣味で作っているアプリ(Rails)があるのだが、はじめHerokuで運用しようと検討していたが、 データ量が多いためすぐにHobbyプランでは対応できなくなってしまった。
仕方ないので、別のアプリで持っているVPSをProduction環境とすることにしたのだが、
せっかくなのでherokuもうまく使いたいなーと思ってステージング環境として使うことにした。
そのときにやったことをメモする。
Staging環境の設定を作る Railsではデフォルトではdevelopment, test, productionの環境を持っている。
そこに今回、stagingという環境を追加した。
そして、herokuではデフォルトではproductionを利用するようになっているのでstagingに切り替えるだけ。
Rails側はstaging環境追加。
$ vim config/database.yml # 下記を追加した staging: &amp;lt;&amp;lt;: *default adapter: postgresql encoding: unicode database: pool: 5 username: password: $ vim config/enviroments/staging.rb # 基本的にはproduction.rbをコピーし、必要に応じて設定を変更 $ vim config/secrets.yml # 必要に応じてstagingを追加  heroku側はステージングへの切り替え。
$ heroku config --app app-name # デフォではproductionになってる RAILS_ENV: production $ heroku config:set RAILS_ENV=staging --app app-name $ heroku config --app app-name RAILS_ENV: staging  Basic認証 ステージング環境なので、外部から簡単にアクセス出来ないようにBasic認証をかけた。
いくつかやり方があると思うが、app/controllers/application_controller.rbに設定を追加した。
ユーザ名とパスワードをベタ書きだとGithubに上げてる場合は丸見えになってしまうので環境変数でやりましょう。
http_basic_authenticate_with :name =&amp;gt; ENV[&#39;BASIC_AUTH_USERNAME&#39;], :password =&amp;gt; ENV[&#39;BASIC_AUTH_PASSWORD&#39;] if Rails.</description>
    </item>
    
    <item>
      <title>Github Pagesを今更作った話と、独自ドメイン適応時の通信のこと</title>
      <link>https://blog.mosuke.tech/entry/2016/03/19/232437/</link>
      <pubDate>Sat, 19 Mar 2016 23:24:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2016/03/19/232437/</guid>
      <description>今更ながらgithubページでプロフィールサイトを作った。
(もう少しデザインブラッシュアップしたい…)
mosuke tech
また、独自ドメイン（カスタムドメイン）の設定やその仕組みをおっている時に、
独自ドメイン適応時の通信について面白いことがわかったのでかく。
求めていたもの  ブログとか作ったプロダクトをまとめたかった 極力シンプルでメンテしやすいものがよかった でもレスポンシブなデザインがよかった  どのようにつくったか 上の条件を満たすために以下の２パターンのどっちで作ろうか迷った。
 Jekyllなどを使いテンプレートを使って作る  CSSフレームワークのみ利用し、あとは自分で作る  １ページだけのシンプルなプロフィールサイトがよかったので、
Jekyllとか複雑なものは利用したくないと考えた。
また、CSSフレームワークだが、いつもよくTwitterBootstrapを使っているので、
それ以外のものでやってみようと考えた。
その結果、行き着いたのがSkeletonだった。
Skeletonは&#34;A dead simple, responsive boilerplate.&#34;をうたっているものでよかった。
カスタムドメインの設定 デフォルトだとmosuke5.github.ioのURLが利用できる。
しかし、この前mosuke.techといういい感じのドメインを安く手に入れてたので、カスタムドメインの設定を行うことにした。
カスタムドメインの設定方法は以下のみだ。
  レポジトリにCNAMEファイルを作成する $ echo mosuke.tech &amp;gt; CNAME   DNSの設定：Aレコードに192.30.252.153と192.30.252.154を追加  (本家ドキュメント)
Setting up an apex domain - User Documentation
カスタムドメイン設定時の通信について カスタムドメインがどのような仕組みで実現されているのか気になって、
いろいろと調べている時にあることに気づいた。
Aレコードに登録したIPアドレスを逆引きで調べると、pages.github.comが出てくるが、
pages.github.comをdigすると違う結果が返ってくる。
$ dig -x 192.30.252.153 ;; ANSWER SECTION: 153.252.30.192.in-addr.arpa. 3600 IN PTR pages.</description>
    </item>
    
    <item>
      <title>DBのViewの使いどころの検討</title>
      <link>https://blog.mosuke.tech/entry/2016/03/16/175431/</link>
      <pubDate>Wed, 16 Mar 2016 17:54:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2016/03/16/175431/</guid>
      <description>ある実装についてどのように実装するか悩んだ。
その悩んだ過程や実装案をメモする。似たようなケースの実装案として参考になればと思う。
状況 ある申請に対して、検査Aと検査Bを行い、その検査状態・検査日時を管理・閲覧するシステムがあるとする。 申請のテーブルは下記とする。
 id: int name: varchar(20) # どうでもいいので無視していい check_a: datetime # 検査すると実施した日時がはいる check_b: datetime # 検査すると実施した日時がはいる  UIとして下記のように表示したいと考えている。
   番号 状態 名前 検査A 検査B     1 検査完了 xxxxxx 2016/03/01 10:00  2016/03/01 12:00   2 検査未完了 xxxxxx -  -    3 検査途中 xxxxxx 2016/03/01 13:00  -    4 検査途中 xxxxxx -  2016/03/01 15:00     ※SQLアンチパターンだろ！？ そもそもSQLアンチパターンの「マルチカラムアトリビュート」じゃないの？と思うかもしれない。</description>
    </item>
    
    <item>
      <title>【錦織圭も分析】ATP TennisのデータスクレイパーGem作った</title>
      <link>https://blog.mosuke.tech/entry/2016/03/06/140531/</link>
      <pubDate>Sun, 06 Mar 2016 14:05:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2016/03/06/140531/</guid>
      <description>背景
このブログは基本的に技術ブログとしてやっているので、
テニスの話題ははじめてだが、テニスがすごく好きだ。 特に、プロの試合の観戦はとても好きだ。
2005年からずーっと見てて、欠かさずすべての試合結果はチェックしている。
数年前から、テニス選手をある指標から分析したいなと思っていた。 例えば、
 ランキングは高くないけど、爆発するとトップ選手に勝つ力を持っているかどうか 自分より格下の選手に負けることが少ないかどうか 小さい大会ではあまり勝てないけど、大舞台で勝てる選手かどうか  これから伸びそうな選手かどうかなどをデータ的にみたかった。
ATPの公式サイトにもいくつか統計情報が載っているが、自分が大事と思う指標で分析したいと思っていた。
まずは、データを取得しなければいけないので、それでスクレイピングのライブラリを作るにいたった。
AtpScraper AtpScraperは男子のプロテニスの公式サイトである、ATPのサイトから情報をスクレイピングしてくれるRubyライブラリだ。
github.com
現時点では、ぼくが今欲している機能しか実装していないが、下記ができる。
 シングルスのランキングの取得 プレイヤーの対戦レコードの取得  一例だが、どんなふうに利用できるかのせる。
シングルスのランキング取得 デフォルトだとトップ100のランキングを取得する。
pry(main)&amp;gt; AtpScraper::Get.singles_ranking =&amp;gt; [{:ranking=&amp;gt;&amp;quot;1&amp;quot;, :player_name=&amp;gt;&amp;quot;Novak Djokovic&amp;quot;, :player_url_name=&amp;gt;&amp;quot;novak-djokovic&amp;quot;, :player_id=&amp;gt;&amp;quot;d643&amp;quot;, :points=&amp;gt;&amp;quot;16580&amp;quot;}, {:ranking=&amp;gt;&amp;quot;2&amp;quot;, :player_name=&amp;gt;&amp;quot;Andy Murray&amp;quot;, :player_url_name=&amp;gt;&amp;quot;andy-murray&amp;quot;, :player_id=&amp;gt;&amp;quot;mc10&amp;quot;, :points=&amp;gt;&amp;quot;8765&amp;quot;}, {:ranking=&amp;gt;&amp;quot;3&amp;quot;, :player_name=&amp;gt;&amp;quot;Roger Federer&amp;quot;, :player_url_name=&amp;gt;&amp;quot;roger-federer&amp;quot;, :player_id=&amp;gt;&amp;quot;f324&amp;quot;, :points=&amp;gt;&amp;quot;8295&amp;quot;}, (略)  下記のようにすれば、50位から60位のランキングを取得できる。
（※引数の指定の仕方がアレなのは突っ込まないこと）
pry(main)&amp;gt; AtpScraper::Get.singles_ranking(&amp;quot;50-60&amp;quot;) =&amp;gt; [{:ranking=&amp;gt;&amp;quot;50&amp;quot;, :player_name=&amp;gt;&amp;quot;Lukas Rosol&amp;quot;, :player_url_name=&amp;gt;&amp;quot;lukas-rosol&amp;quot;, :player_id=&amp;gt;&amp;quot;r685&amp;quot;, :points=&amp;gt;&amp;quot;897&amp;quot;}, {:ranking=&amp;gt;&amp;quot;51&amp;quot;, :player_name=&amp;gt;&amp;quot;Federico Delbonis&amp;quot;, :player_url_name=&amp;gt;&amp;quot;federico-delbonis&amp;quot;, :player_id=&amp;gt;&amp;quot;d874&amp;quot;, :points=&amp;gt;&amp;quot;895&amp;quot;}, (略)   錦織圭の対戦レコード取得 錦織圭の2016年の対戦レコードを見たいと思えば下記のように取得できる。</description>
    </item>
    
    <item>
      <title>社内マリカー大会やりました！運営の工夫どころとかまとめ</title>
      <link>https://blog.mosuke.tech/entry/2016/02/07/210300/</link>
      <pubDate>Sun, 07 Feb 2016 21:03:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2016/02/07/210300/</guid>
      <description>弊社では、年に一度、社内のコミュニケーション促進で、いろんな部署が集まって行う懇親会がある。
そこでは、恒例行事として何かしらのゲームを行っている。
そのゲームにて今年マリカー大会を行ったので、その報告と工夫した点などまとめておく。
 マリオカートを選んだ理由 パーティとか懇親会のゲームイベントというと、
ビンゴ大会だったりクイズ大会、ジェスチャーゲームだったり行うのが一般的かと思う。
ですが、今回そういう場でマリオカート大会を選ぶに至ったのには下記のような理由があった。
 準備が楽なものがよかった！ 順位が決められるものが良かった（景品の都合上） 楽しさに進行役の力量が影響しないものがよかった…  まず、はじめはクイズ大会などを行おうかと検討していましたが、
そのクイズの問題を考えるのが結構たいへんであることに気づき、準備が楽なものがいいなと思った。
そして、運営的な理由だが、ゲームの結果を順位を簡単に決められる必要があった。
最後に、なんといっても自分が進行役だったので、
自分の笑いを取る力や進行技術で楽しさが左右されたらこまるなーとおもっていた。
これらを満たすことができると思ったのが「マリカー」だった！
環境 参加者 ８つのチームから合計200人程度参加。
年齢層も結構バラバラ。若手からお偉いさんまでいる。
映像 WiiUの画像は、HDMIを使ってプロジェクターで大型スクリーンに投影した。
ルール 200人全員にマリオカートに参加してもらうことは難しかったので、
各チームで６人のゲーム参加者を事前に選んで頂いて、チームを代表として参加してもらうこととした。
（マリカー大会をやることは事前には言いませんでした。）
WiiUでは４人対戦までしかできないので、下記のような形式で対戦を進めた。
 8チームを4チーム×２分けて予選 下位２チームが下位決定戦 上位２チームが決勝戦  またなるべく多くの人がゲームに参加して欲しかったので、
特別ルールとして、コースを１周したらチームのほかの人に交代するというリレー形式とした。
準備したもの   WiiU本体(ゲームパッド含む)  マリオカート8のソフト  Wiiリモコン 4つ www.amazon.co.jp   運営上の工夫 運営上幾つかの工夫を行ったので紹介する。
キャラクターについて 利用するキャラクターや乗り物は、予めこちらでチームごとにランダムで割り振った。
そうすることで、当日にキャラを選ぶ時間を短縮できるし、マリカーの知識の差がなくなる。
キャラクターは運ということにした。
コントローラについて ４人対戦だと画面が４つに分割される。
自分がどの画面を操作すべきかわからなくなるので、画面位置がわかるようにコントローラにシールを貼った。
この対応は必須です。

アイテムや操作など CPUなしで４人対戦で行った。 ４人対戦で普通のアイテム設定だと、「コイン」とか「バナナ」とか良いアイテムが出づらくつまらない。
アイテムの設定は「ダイナミック」にすることで、４人でも強力アイテムがでてバトルが白熱する。
カートの操作方法（曲がり方）は十字キーではなくハンドル操作とした。
普段ゲームに慣れている人でも、ハンドル操作だと不慣れなことも多くみんなうまい具合に下手になって白熱した。
初心者への対応 アイテムの細かい使い方などいちいち説明できないので、
初心者へは「アクセル」と「曲がること」のみ教えた。</description>
    </item>
    
    <item>
      <title>Vagrant×Ansible環境の３つのスタイルとそのメリット・デメリットについて</title>
      <link>https://blog.mosuke.tech/entry/2016/01/25/222150/</link>
      <pubDate>Mon, 25 Jan 2016 22:21:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2016/01/25/222150/</guid>
      <description>Vagrant×Ansibleで開発環境を作っているみなさんは、 どんなふうにそれを実現していますか？
きっといくつかのやり方、考え方があると思います。
例えば、ご自身のPCにAnsibleをインストールしてVagrantの仮想環境にプロビジョニングしているとか。
Vagrantで立てた仮想環境にAnsibleを入れて実行するとか。
本ブログでは下記３つのVagrant×Ansibleのやり方に注目し、
そのメリット・デメリットについて自分なりにまとめます。
 ホストにAnsibleをインストールして利用する ゲストにAnsibleをインストールして利用する Ansible実行用のゲストを作って利用する  1. ホストにAnsibleをインストールして利用する 概要 1番よくあるやり方だと思う。
ご自身のPCにAnsibleを入れてVagrantで立ち上げたゲストにプロビジョニングを行う。

メリット なんといっても直感的でシンプル。
3.との比較になるが、Ansibleを実行する環境を別途用意する必要がないのでホストのリソースにも優しい。
また、2.との比較でゲストに無駄なものが入らない点でよい。
デメリット ただ、そもそもwimdowsユーザはホストにAnsibleをインストールすることはできない。
つまり、複数人での開発をしていて、人によって端末が異なる場合には向いていない。
個人の端末の設定などにも大きく依存するため、チーム開発向きでないといえる。
2. ゲストにAnsibleをインストールして利用する 概要 Vagrantで立ち上げたゲストの中にAnsibleをインストールし、自身へプロビジョニングするやりかた。

この方式はshin1x1さんもおすすめしている。
Vagrant + Ansible で開発環境を作るなら ansible_local プロビジョナがいい！ - Shin x Hatena Blog メリット （詳しくは上のブログを読むといいと思う。）
1.のデメリットで述べた、端末への依存度をなくすことができる。
vagrantの新機能であるansible_localも利用できるのでプロビジョニングが楽だ。
3.と比べてAnsible実行環境を用意しなくて済む。
デメリット メリットだけみると1.の問題点を解決していて最高のようにみえる。
しかし、ゲストに本来インストールされるべきものでないものがはいる点は忘れてはいけない。
Ansibleを使ってプロダクション環境にデプロイするユーザにとっては大きな問題だと思う。
開発環境とプロダクション環境での差分が広がってしまう。
また、Ansibleと並行してserverspecを使ってる人も多いと思うが、
そうなると今度はゲスト側にRubyもいれるのか？など様々な疑問が湧いてくる。
3. Ansible実行用のゲストを作って利用する 概要 Ansible実行用のゲストをVagrantで立ち上げて、開発用のゲストへプロビジョニングにする。 
メリット 2.のデメリットで述べた、ゲストへの不要なもののインストールを防ぐことが可能。
例えばserverspecでRubyが必要な場合もAnsible実行用のゲストに入れればいい。
必要なものをインストールしたAnsible実行用のイメージファイルをチーム内で共有すれば端末依存もセットアップの手間も省ける。
デメリット なんといっても、ゲストを二つは立ちあげる必要があること。
これは少し面倒だ。
まとめ</description>
    </item>
    
    <item>
      <title>2015年振り返り</title>
      <link>https://blog.mosuke.tech/entry/2015/12/28/150042/</link>
      <pubDate>Mon, 28 Dec 2015 15:00:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/12/28/150042/</guid>
      <description>去年に続き、2015年で技術分野でなにがあったか簡単に振り返りました。
2014年を思い返して… - Goldstine研究所
1. Vim::Factoryの開発と公開 
2014年の秋から@mogulla3と定期的にインフラ関連技術の勉強会をやってきていて、
はじめはインプットの勉強会だけを主にやってきていたのですが、
サービスを作る中でインフラ関連技術を駆使し勉強したいと思うようなっていました。
そこで、今年は「vimの設定をブラウザ上で即体感できるサービス Vim::Factory」の開発をしました。
DockerとWebSocketを使って、vimの設定をブラウザで即体感できるサービスを作った - Goldstine研究所
良かった点  新しい技術などを組み合わせながら学習しがいのあるサービスを作れたこと サービスの実現技術を公開しそれなりの反響を得られたこと  悪かった点  内部的なアーキテクチャの変更ばかりに時間を取られサービスそのものの改良ができなかったこと  2. インフラ関連の構築や運用、仕組みづくり 主に仕事の話になりますが、今年はインフラ関連の構築や運用に多く携わった気がします。
（結構、雑な感じに書き残します。）
(1) メールサーバの構築と運用 Postfixを使ったメールサーバの構築と運用に携わりました。
 そもそもメールサーバってなんだっけってところからはじまったのを思い出します。 とにかく運用を楽にしたかったので、その部分に多くの工夫をしました。 Ansibleを使って設定の変更などのリリース作業も簡単することや Zabbixのログ監視もいい感じに機能して、不正なメール送信とかもすぐ検知できるようにしました。  KVMの仮想環境上なので、環境の作りなおしも容易にしました。 冗長性のために２つのリージョンに分散させたりもしました。 目新しいことはないですが、わりと運用が楽な感じに作れたのでほんとによかったなと振り返って思います。  (2) インターネットから社内NWへの入口としてのリバプロサーバ構築 インターネットから社内NWにあるシステムを利用できるようにするために、
リバースプロキシサーバを構築しました。（運用はこれから）
いわゆるDMZ構成におけるリバプロです。
 技術的なところで言うと、corosync+pacemakerを使ってクラスタリングを組みました。  クラスタリングは思ったより奥が深く、どのようなクラスタを組むかかなり苦労しました。 障害時に相手側サーバの電源を落とす、いわゆるフェンシングなどをどう適切に使うかなど。 運用が始まってからそのあたりの実用性が確認できそうです。 ちなみにNginxでリバースプロキシたてました。  (3) 仮想環境構築とか 仮想環境というと、今までVagrantなどのツールとして、AWS、VPSなどのIaaSとして使うばかりでした。
今年は、KVMを利用してプロダクションの仮想基盤を作るなどやりました。
仮想化ってそもそもなんだっけ？というもう少し基礎よりの知識と向き合う機会がありました。
また、主に来年の話になるがOpenStackなどを使ったクラウド環境を「作る・運用する」にも携わっていきそうです。
すこしづつOpenStackをかじり始めました。
(4) Ansibleの活用と布教活動 Ansibleの利用は去年から始めていたことですが、
今年はより活用することと、社内での布教活動を行いました。
活用でいうと、サーバの構築はAnsibleで行ってアプリケーションのデプロイは
別の方法でやる（手動とか…）というふうになっていたので、
Ansibleを使ってアプリケーションのデプロイまですべて行うようにしてきました。
また、こういった取り組みを社内（部内）で広めて他のチームにも活用してもらおうと、</description>
    </item>
    
    <item>
      <title>インフラのデプロイとテストを同時実行できるようにしてHappyになった</title>
      <link>https://blog.mosuke.tech/entry/2015/12/17/192554/</link>
      <pubDate>Thu, 17 Dec 2015 19:25:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/12/17/192554/</guid>
      <description>はじめに
私が開発しているシステムでは、Ansibleでサーバ構築からアプリケーションのデプロイまですべて実行できるようにしています。 そして、serverspecを使って、インフラテストも行っています。
しかし、その運用にいくつか課題点がありました。
その課題点についてと、課題点へ対策したことについて書きます。
課題だったこと (課題1) デプロイとテストをそれぞれ実行していた Ansibleでのデプロイとserverspecのテストはそれぞれ別のコマンドで実行していました。
$ ansible-playbook site.yml -i $ bundle exec rake serverspec  2つ実行することが面倒であり、面倒であるがゆえにserverspecの実行を怠ったりしていました。
これではテストの効果があまり発揮できませんね。
(課題2) sudoパスワードをうまく管理できなかった 上のような課題1について、真っ先に以下の様にコマンドを続けることを思いつきました。
$ ansible -playbook site.yml -i ; bundle exec rake serverspec  ですが、これだとansible実行終了後にserverspecを実行する際にsudoパスワードが再度聞かれるため、
コマンドを打ったまま「放置」ができませんでした。
※もちろん、sudoパスワードを要求しないようにユーザ設定をすればできますが、多くの場合ではセキュリティ上難しかったりすると思います。ssh接続は鍵認証、sudoには必ずパスワードを要求するようにしています。
Ansibleもserverspecにもコマンド実行時にsudoパスワードを記述する方法があります。
Ansibleでは、ansible.cfgにsudo_passwordを記述、あるいはコマンド実行時に--extra-argsでsudoパスワードを指定できます。
serverspecでも環境変数でSUDO_PASSWORDが指定できます。
例 ）
ansible -playbook site.yml -i --extra-args=&#39;ansible_sudo_pass=xxxxxxxx&#39; bundle exec rake serverspec SUDO_PASSWORD=xxxxxxxx  ですが、おわかりの通り、コマンドの履歴にもパスワードが残ります。
なのであまり良い方法ではないと思っています。
(課題3) タスクの実行方法がバラバラ デプロイはansibleコマンドで実行、テストはrakeで実行、他のタスクはシェルスクリプト。。。
といったように、タスクによって実行方法が異なってしまう状況になっていました。
運用的にとても不便でしたので、１つに統一したいと思っていました。
いい感じに同時に実行してくれるRakeタスクを作った 上で述べたような課題点をクリアするように、下記の要件を満たすように工夫をしました。
 デプロイ、テストが同じ形式で実行できる sudoパスワードをベタ書きすることなく実行できる sudoパスワードの入力を一回だけにする  結論は、すべてRakeタスクで実行できるようにしました。</description>
    </item>
    
    <item>
      <title>Ansible、実行速度高速化の実験。ControlMasterとPipeliningについて</title>
      <link>https://blog.mosuke.tech/entry/2015/12/01/181304/</link>
      <pubDate>Tue, 01 Dec 2015 18:13:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/12/01/181304/</guid>
      <description>1. はじめに
Vim::Factoryの開発や、仕事などでAnsibleを使うことが多いのだが、
その実行速度があまりでないことに不満をもっていて、どうしたら早くできるか考えていました。
調べると、ControlMasterを利用してSSHのコネクションを再利用するとか、pipelineの機能を利用するとかでてくる。
が、それによってどのくらいの効果が得られるのかよくわからないし、仕組みもよくわかっていなかったので、仕組みの理解と実行速度の実験をした。
もう少し余談をすると、
ControlMasterを有効にすれば早くなることは前から知っていたが、
最近MacをEl Capitanに変えてから「なんか最近Ansibleはやいな〜」とか思っていて、
「OpenSSHのバージョンもあがったし、まさか。。。」と思って今にいきついている。
ControlMasterについて OpenSSH は、1 つの接続で複数のセッションを共有する(束ねる)「コントロール マスター」と呼ばれる機能を持っています。コントロールマスターを使用すると、 リモートホストに接続する最初のセッションは制御用のセッション(マスターセッショ ンと呼ばれます)として利用され、制御用のソケットを作成します。セッションを 共有する SSH クライアントは、この制御用のソケットを通じてリモートホストと 接続し通信を行います。
出典：「OpenSSH実践入門」
 上記のように、１つの接続で複数のセッションを共有するため、
Ansibleのようにタスク実行ごとにSSH接続するような場合には大きな効果を得ることができる。
Pipeliningについて Pipelining機能の説明の前に、軽くAnsibleの実行までの流れを説明する。
Ansibleは対象サーバにSSHログインしたあと実行するタスクのモジュールをファイルとして転送しそれを実行する。
Pipelining機能をなしの状態だと、このファイル転送とファイルの削除を１タスクごとに行う。
ansibleを-vvvvオプションをつけて実行するとわかるが、
４行目でファイルのPUT（転送）を、５行目の最後の方にrm -rfでディレクトリ・ファイルの削除を行っている。
&amp;lt;192.168.33.100&amp;gt; ESTABLISH CONNECTION FOR USER: deploy &amp;lt;192.168.33.100&amp;gt; REMOTE_MODULE command ls -l /root &amp;lt;192.168.33.100&amp;gt; EXEC ssh -C -tt -vvv -o ControlMaster=auto -o ControlPersist=300s -o ControlPath=&amp;quot;/Users/xxxxx/.ansible/cp/ansible-ssh-%h-%p-%r&amp;quot; -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o User=user -o ConnectTimeout=10 192.168.33.100 /bin/sh -c &#39;mkdir -p $HOME/.</description>
    </item>
    
    <item>
      <title>インフラテスト(serverspec)はじめました</title>
      <link>https://blog.mosuke.tech/entry/2015/11/02/161744/</link>
      <pubDate>Mon, 02 Nov 2015 16:17:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/11/02/161744/</guid>
      <description>※執筆後、業務でもserverspecを利用し始めたのもあり、業務レベルでの実践例も追記している。
運営中のVim::Factoryでserverspecを使ったインフラテストを導入したので、 導入理由や工夫している点、悩んでいる点について記述します。
Vim::Factoryについてはこっちみてね。 DockerとWebSocketを使って、vimの設定をブラウザで即体感できるサービスを作った - Goldstine研究所 1. serverspecってなによ 詳しくは公式サイトや書籍などを参考にして欲しいですが、
「サーバの状態をコードで自動的にテスト・確認するためのツール」です。
Serverspec - Home
例えば、ApacheでWebサーバを組んでいるサーバがあったとして、下記の要件で動いているとします。
  apacheがインストールされていること  apacheが起動していること、自動起動する設定であること ポート80があいていること  この要件をサーバが満たしているかコードでテストします。
上記の例だとこんなコードを書きます。
describe package(&#39;httpd&#39;) do it { should be_installed } end describe service(&#39;httpd&#39;) do it { should be_enabled } it { should be_running } end describe port(80) do it { should be_listening } end  各種テストの立ち位置 
 serverspecは、サーバの状態（正しく設定されたか）を確認するためのテストツールです サーバの振る舞いのテストは別のツールを使うことをおすすめします また、監視も一種のテストと言えます 一般的には監視はその実行頻度の高さから、振る舞いを監視することが多い 監視ツールで、Configファイルが正しいかは見ない  2. なんで導入したの？ serverspecを導入したのには大きく2つの理由があります。</description>
    </item>
    
    <item>
      <title>Ansibleを踏み台サーバ越しに実行する</title>
      <link>https://blog.mosuke.tech/entry/2015/09/25/232751/</link>
      <pubDate>Fri, 25 Sep 2015 23:27:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/09/25/232751/</guid>
      <description>タイトルの通りで、なにも特別なことはない内容。
そして、9月も終わりなのに今月はひとつも記事を書いていなかった。
KVMを使って仮想のゲストサーバを立てたが、
ゲストサーバはホストサーバと通信する用の（外に出る場合にはNAT通信で）IPアドレスしか持っていない状況で、
Ansibleの実行対象としたかったのが背景。
ホストサーバにAnsibleをいれるわけにもいかず、ホストサーバを踏み台にして、
Ansibleを打ちたかったというもの。

 SSHの設定ファイルを作る &#34;Ansibleで&#34; と書いたが要はSSHです。
まずはSSHで踏み台サーバを経由してAnsible実行対象サーバへ接続できるように準備しました。
これはいわゆる「多段SSH」というやつで、以前にもブログに書いたので復習です。
【VPS1台でインフラ勉強】多段SSH設定（おまけ） - Goldstine研究所
一般的には~/.ssh/configにこういった設定は書いたりもしますが、
Ansible実行の場合、端末に依存したくなかったので、
Ansibleレポジトリに別途ファイルを作ることにした。
## sshconfigという名前のファイルにした Host ansible-target HostName 192.168.33.10 User xxxxx ProxyCommand ssh -W %h:%p yyyyy@hostserver  上記のファイルを使って多段SSHできることを確認します。
$ ssh -F sshconfig ansible-target  Ansible実行時にSSH設定ファイルを利用する ここまで来たらとても簡単で、
ansible.cfgに下記を追記し、ansible実行時に上記のsshconfigを読み込まれるようにしました。
ansible.cfg
[ssh_connection] ssh_args = -F sshconfig</description>
    </item>
    
    <item>
      <title>(個人的) YAPC::Asia 2015ふりかえり</title>
      <link>https://blog.mosuke.tech/entry/2015/08/22/223025/</link>
      <pubDate>Sat, 22 Aug 2015 22:30:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/08/22/223025/</guid>
      <description>「ブログを書くまでがYAPC」
というわけで、8/21, 22とYAPCに参加したので、そこでの学んだこと、気になったことなどを振り返ります。
しかし、YAPCで聞いた公演内容を淡々とまとめるなどはしません。
単なる内容のまとめであれば、公開されているスライドなどをみるのが一番だと思いますので。
以下３点を中心に振り返ってみます。
 インフラ関連セッションについて 発表を聞いてよかった、今の自分に一番必要だったことについて 興味を持ったことについて（CONBU）  インフラ関連セッションについて 意図的も半分くらいあるんだけど、なんとなくセッションを選んでいたら、インフラ関連のものが多くなりました。
 世界展開する大規模ウェブサービスのデプロイを支える技術 - YAPC::Asia Tokyo 2015 Consulと自作OSSを活用した100台規模のWebサービス運用 - YAPC::Asia Tokyo 2015 3分でサービスのOSを入れ替える技術 - YAPC::Asia Tokyo 2015 我々はどのように冗長化を失敗したのか - YAPC::Asia Tokyo 2015  デプロイについて 大規模サービスでのデプロイにおいて、一台一台のホストがgit cloneして、bundle installやらせてーとかやると、
時間もかかるし、並列的にgit cloneした際などgitサーバが負荷的に危なくなってくる。
なので、予めライブラリとかすべてインストールされたものをターボールなどにまとめておいて、
それをプルしてくる形式のデプロイについて多くの発表がありました。
このデプロイ方法、並列で数百MBのファイルをダウンロードしても落ちないストーレジ（ようはAWSのS3）を前提に構築されている感がありました。
（というかそう言っていた）
相変わらずAWSはせこいなーと思っています（笑）
うちの環境では真似するのは難しいなーと思う部分もありますが、
それ以前にシステムの規模や用途によってデプロイのあり方も多種多様になることを改めて考えさせられました。
自分の環境にあった最適なデプロイ形態を探す日々がまた始まりそうですが、良いヒントになりそうなのはまちがいなしです。
 式年遷宮インフラストラクチャ Kenjiさんの式年遷宮インフラストラクチャ。 この考え方、ぼくも一度考えたことはありますが、ここまで実践してみた経験談がきけたのは面白かった。
いざというときに切り替わらない、切り替えられないという問題に対して、
自動ではないが、「切替訓練」ということで、定期的に冗長化の系を切り替えることはしてもいいのかなーと思ったりしています。
consulについて 実は、上であげた４つのセッション全てで共通していたのがconsulを使っていた。
Consul by HashiCorpwww.consul.io
正直、consulについて、名前くらいしかしらなかったのに、ここまで利用されていたので、時代に乗り遅れている感を感じた。 でも利用用途をみていくと、iaas環境で力を発揮するっぽいので、概要と用途だけ押さえておくか…といったところ。
今月中の課題です。
発表を聞いてよかった、今の自分に一番必要だったことについて たくさん学びのあったなかで、なんだかんだいっても、koemuさんの発表が今の自分が一番考えていてることであり、
一番必要なことであったように感じた。
辛いことをやめる！から始まる業務改善とInfrastructure as Code - YAPC::Asia Tokyo 2015yapcasia.</description>
    </item>
    
    <item>
      <title>デスクトップUbuntuにVNC接続。ついでにSSHローカルポートフォワードの復習。</title>
      <link>https://blog.mosuke.tech/entry/2015/08/13/000440/</link>
      <pubDate>Thu, 13 Aug 2015 00:04:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/08/13/000440/</guid>
      <description>完全に自分のための備忘録。内容はわりと薄め。
やったこと 最近、自作したPCにUbuntuをいれて使っているのだけど、
デスクトップPCなので、部屋でしか操作することができません。
他の部屋からノートPCでUbuntuを触れたらいいなーと思いその環境を整えることをしました。
主にやったことは以下の通りです。
  VNCサーバ構築について ノートPC（Mac）からのVNC接続について  SSHローカルポートフォワードを使ってのセキュアな接続について   UbuntuでのVNCサーバ構築について 今回利用しているUbuntuは「Ubuntu Desktop 14.04」です。
また、VNCの実現は標準でインストールされているvinoを使って行いました。
ご存知の方も多くいるかもしれませんが、vinoでのVNCは簡易的なもので、サーバ側のユーザがログアウトしていると使えません。
ですので、会社などでの利用には耐えないと思います。
ユーザーをログアウトせずにロック状態にしていれば使えます。
まずはデスクトップ共有の設定をします。
「デスクトップの共有」のアプリケーションを起動します。 
接続毎に要求するようにすると、サーバ側で毎度許可が必要なので、オフにします。
パスワードの設定はしておきましょう。
同じLANをつかんでる人に簡単に奪われてしまいますので。

ちょっと詳細な意味を把握していないのですが、
下記を実行しないとMacで接続すると「互換性のないバージョンです」的なこといわれました…すいません。
$ gsettings set org.gnome.Vino require-encryption false  設定ができたら、きちんとサーバとしてVNC接続を待ち受けているか確認します。
% sudo lsof -i:5900 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME vino-serv 24414 mosuke5 13u IPv6 156661 0t0 TCP *:5900 (LISTEN) vino-serv 24414 mosuke5 14u IPv4 156662 0t0 TCP *:5900 (LISTEN) % ps -ef | grep vino mosuke5 24414 24226 0 12:30 ?</description>
    </item>
    
    <item>
      <title>Packerやる前にKickstartはじめよう</title>
      <link>https://blog.mosuke.tech/entry/2015/07/31/211542/</link>
      <pubDate>Fri, 31 Jul 2015 21:15:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/07/31/211542/</guid>
      <description>1.はじめに
開発環境はVirualboxを使ったVagrantを利用しているが、
本番環境はAWSだったりKVMだったり違う仮想化機構で動作しているなんてことよくあると思います。
そういう環境下でどのように開発環境と本番環境の差分をなくしていますか？
わたしの場合、基本的にAnsibleを使ってプロビジョニングをしていますが、
そのプロビジョニング前のベースが異なってしまって困ることがよくあります。
一般に公開されているVagrantBox使ったら余計な設定が入っていたとか、すでにパッケージが入っていたとか…
そんな問題を解決しようとPackerを使おう！って考えました。
ですが、Packerも当たり前だけど魔法ではなく、Kickstartなどの自動インストールが前提なので、
Packerをやる前にKickstartを学習せよ、、、ということに気づきました。
ということでKickstartをはじめたよってことです。
2.kickstartってなに kickstartはOSのインストールを自動化する仕組みです。
anaconda社が提供するインストールの仕組みでRedhat系のOSが採用しているものです。
ですのでUbuntuだとPreseedっていう別の仕組みだそうです。（詳しくありませんっ）
で、Kickstartでなにができるかというと...
OSのインストールをしたことがある方ならわかるかと思いますが、
普通にDVDなどからインストールすると、
 言語はなににしますかー？ ホスト名なににしますかー？ パッケージはなにをいれますかー？  とか、聞かれて選択していく必要があります。 この作業を自動化できるのがkickstartです。
URLのようなもの。 はじめての自宅サーバ構築 - Fedora/CentOS - CentOS6 のインストール手順kajuhome.com
(おまけ)Ansible, Chef, Puppetとの違い？ kickstartにはプロビジョニング機能もついているため、
AnsibleとかChefとかPuppetとの違いは？住み分けは？と思うかもしれません。
明確に、住み分けが決まっているわけではありませんが、 個人的にはAnsibleやChefを実行する前の最低限の設定をkickstartにやらせようと思っています。
（一般的かとは思いますが…？）
Lee ThompsonのProvisioning Toolchainを参考にKiskstarのやる範囲をまとめると。 
Provisioning Toolchain: Web Performance and Operations - Velocity Online Conference - March 17, 2010 - O&#39;Reilly Mediaen.oreilly.com
3.Hello Kickstart!! Virtualboxを使ってKickstartを試しました。
3-1.用意したもの   Virtualbox  自分の環境はMacで、バージョンは5.</description>
    </item>
    
    <item>
      <title>DockerとWebSocketを使って、vimの設定をブラウザで即体感できるサービスを作った</title>
      <link>https://blog.mosuke.tech/entry/2015/07/19/135844/</link>
      <pubDate>Sun, 19 Jul 2015 13:58:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/07/19/135844/</guid>
      <description>2014年の秋から@mogulla3と定期的にインフラ関連技術の勉強会をやってきましたが、
インプットの勉強会だけでは飽き足らず、いつしかサービスを作る中でインフラ関連技術を駆使し勉強したいと思うように…
そして、普段使っているVimを題材に、
vimの設定をブラウザ上で即体感できるサービス Vim::Factory
を開発しました。
本記事はVim::Factoryの簡単な紹介と技術的な仕組みについて記述しています。
Vim::Factoryはこちら。
http://vimfactory.com/
1. Vim::Factoryについて 1-1. Vim::Factoryってなに？？ Vim::Factoryは、選択したVimの設定を、ブラウザ上で「即体感」できるサービスです。
数多くあり複雑なVimの設定を容易にし、お気に入りのVim探しをサポートすることを目指しています。

1-2. なんで作ったの？ Vimの設定ってたくさんあってどれを選んでいいかわからなかったり、
設定したもののどう変わったかイマイチわからなかったりしませんか？
Vimの設定がどのように反映されるか、もっと簡単に体験したいと考えたからです。
あと、例えばGithubで100star以上をつける人のVimをブラウザ上で体験できたらいいなと思っていて、
それを実現のための第一歩としてこのサービスを作りました。
1-3. このサービスの最大の特徴は？ このサービスの最大の特徴はなんといっても「ブラウザ上でVimが体感できること」です。
今まではVimの設定を試そうと思ったら、ネットで調べて自分のVimに反映させて…という作業が必要でしたが、
ブラウザ上で設定を即体感するという新しい体験を提供するために力を注いできました。
その実現方法については、後述しています。
1-4. 紹介動画 詳しくは、実際に試してもらうのが早いと思いますが、簡単な操作動画を用意してみました。
モバイルからこのサービスはちょっと使えないので、モバイルで読んでいる方は動画でお楽しみください(笑)
www.youtube.com
2. Vim::Factoryの技術について ここからVim::Factoryの技術について一部ではありますがご紹介します。
2-1. ブラウザ上でのVimを実現した技術 ブラウザ上でVimを実現しようと思うと、ぱっと思いつくのはJavaScriptでVimそのものを実装してしまおうというものかもしれません。
ですが、JSでVimを実装することってどれくらい難しいでしょうか？
少なくともぼくにはそんなことはできませんし、できたとして質の悪いものになってしまうと思います。
そこで思いついたのが、一般的なターミナルソフトと同様にサーバ上でvimを起動し、
そのターミナル情報をブラウザ上で表示するという方法です。
この方法であれば自らVimを実装せずともVimを再現できます。イメージは下記のとおりです。

また、サービスとして上記を行うには、接続してきたユーザごとにVimを用意する必要があります。
これらを実現するために利用したのがDockerとWebSocketです。
dockerコンテナ上でVimを起動し、そのターミナル情報をWebSocketでブラウザに送信するようにしました。 
dockerはコンテナ型の仮想化なので起動がとてもはやく、
httpのリクエストが来てからdockerコンテナを立ち上げても十分なほどのはやさをもっています。
2-2. 全体構成 システムの全体構成は以下のような感じです。
※実際の役割は図のとおりですが、サーバはこんなに多くありません。

2-3. 利用した技術とかツールのまとめ 振り返りも兼ねて利用した技術・ツールを一覧にまとめておきます。
 Ruby Sinatra thin node.js Websocket memcached docker nginx centos7 Ansible Vagrant gitlab mackerel slack  esa.</description>
    </item>
    
    <item>
      <title>PostgreSQL環境でFuelPHPのDBマイグレーションを使う</title>
      <link>https://blog.mosuke.tech/entry/2015/06/17/212852/</link>
      <pubDate>Wed, 17 Jun 2015 21:28:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/06/17/212852/</guid>
      <description>今更FuelPHP感はあるのだが、
postgresql利用時のFuelPHPのmigration導入について、注意点をまとめた。
でも、結論は納得がいっていない。
0. 前提 下記の環境で行ったものです。
PHP: 5.5.7
FuelPHP: 1.7
Postgresql: 9.4
1. テーブル文字コードの問題 事象 公式サイトのサンプルの通りはじめにapp/migrations/001_example.phpを作り、migrationを実行した。
app/migrations/001_example.phpの作成
&amp;lt;?php namespace Fuel\Migrations; class Example { function up() { \DBUtil::create_table(&#39;posts&#39;, array( &#39;id&#39; =&amp;gt; array(&#39;type&#39; =&amp;gt; &#39;int&#39;, &#39;constraint&#39; =&amp;gt; 5), &#39;title&#39; =&amp;gt; array(&#39;type&#39; =&amp;gt; &#39;varchar&#39;, &#39;constraint&#39; =&amp;gt; 100), &#39;body&#39; =&amp;gt; array(&#39;type&#39; =&amp;gt; &#39;text&#39;), ), array(&#39;id&#39;)); } function down() { \DBUtil::drop_table(&#39;posts&#39;); } }  マイグレーション実行すると以下のエラーに襲われた。
$ php oil refine migrate Uncaught exception Fuel\Core\Database_Exception: SQLSTATE[42601]: Syntax error: 7 ERROR: syntax error at or near &amp;quot;DEFAULT&amp;quot; LINE 5: )DEFAULT CHARACTER SET utf8; ^ with query: &amp;quot;CREATE TABLE IF NOT EXISTS &amp;quot;migration&amp;quot; ( &amp;quot;type&amp;quot; varchar(25) NOT NULL, &amp;quot;name&amp;quot; varchar(50) NOT NULL, &amp;quot;migration&amp;quot; varchar(100) DEFAULT &#39;&#39; NOT NULL )DEFAULT CHARACTER SET utf8;&amp;quot;  理由 しょっぱなから躓くわけだが…</description>
    </item>
    
    <item>
      <title>Ajaxの嫌いだった部分をJsRenderで心地良くした</title>
      <link>https://blog.mosuke.tech/entry/2015/06/13/231917/</link>
      <pubDate>Sat, 13 Jun 2015 23:19:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/06/13/231917/</guid>
      <description>1. はじめに
ぼくはフロントエンドは本業ではありません。
jsはあまり好きではありません。
そしてAjax通信後にhtmlをアウトプットする際にjsの変数の中にhtmlを書いていくソースコードがもっと好きではありません。(後述)
それをJSテンプレートエンジンを使ってシンプルにしてみたって話です。 （JsRenderの使い方を書いたものではありません。）
2. Ajaxが嫌いだった理由 Ajaxはユーザ体感的にはいいのだけれど、
Ajaxの結果受け取ったjsonなどのデータを使ってhtmlを出力とかやるとソースコードが煩雑になるので嫌いだった。
例としてAjaxで/xxxxxにリクエストを投げて、その結果(jsonデータ)を使ってhtmlを出力するものを考えると。
/* jsonデータは下記が返ってくるとする [ { id: &#39;1&#39;, name: &#39;らーめん&#39;, text: &#39;らーめんはやっぱり濃厚鶏そばです。&#39; }, { id: &#39;2&#39;, name: &#39;うどん&#39;, text: &#39;うどんはやっぱり釜揚げうどんです。&#39; } ] */ $.ajax({ type: &amp;quot;GET&amp;quot;, url: &amp;quot;/xxxxx&amp;quot;, dataType: &amp;quot;json&amp;quot;, success: function(data){ var html = &#39;&#39;; data.forEach(function (e) { html += &#39;&amp;lt;div id=&amp;quot;&#39; + e.id + &#39;&amp;quot;&amp;gt;&#39;; html += &#39;&amp;lt;h1&amp;gt;&#39; + e.name + &#39;&amp;lt;/h1&amp;gt;&#39;; html += &#39;&amp;lt;p&amp;gt;&#39; + e.text + &#39;&amp;lt;/p&amp;gt;&#39;; html += &#39;&amp;lt;/div&amp;gt;&#39;; }); $(&amp;quot;#result&amp;quot;).</description>
    </item>
    
    <item>
      <title>他人の家のインターネットを環境を整えて分かった無線LANルータのこと</title>
      <link>https://blog.mosuke.tech/entry/2015/05/24/220226/</link>
      <pubDate>Sun, 24 May 2015 22:02:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/05/24/220226/</guid>
      <description>他人の家のインターネットを環境を整えて分かった無線LANルータのことがあったのでまとめる。
我が家のインターネット環境は以下のような構成になっている。 
この構成では無線LANルータはL3とL2の両方の機器として働いている。
グローバルIPとプライベートIPの両方を持っており、
プライベートIPからの通信をグローバル側へルーティングする機能と、
LAN内の複数の端末に接続する機能として。
一方、この前、他の人の家のインターネット環境を整えたのだが、
以下のような構成だった。 
１つめの構成と決定的に違うところは、VDSLモデムにルータ機能もついていること。 この場合は無線LANルーターはL2の機器として働いている。
無線LANルーター自体にはIPアドレスはなく、DHCPでIPアドレス管理を行っているのも上位のルータだ。
この構成になるときは、一般的に光IP電話を利用するケースのようだ。
というのも、一般的な無線LANルーターには光IP電話につなぐことができず、
通信会社から貸与されるモデムルータを利用するため。
余談だが、今家では実は下記のような構成にしている。
というのも、家の構造上、VDSLがでているところが納戸のようなところで、
無線LANルーターを設置しても壁が多すぎるために電波が弱くなってしまう。
そのため、リビング側へ無線LANルーターを設置したかったからだ。

とても単純な話だが、
いろんなケースの家庭内インターネットの設定をすることで、
いろいろと気づくこともあった。</description>
    </item>
    
    <item>
      <title>sinatra-assetpackをproduction環境で使う時にはまったー</title>
      <link>https://blog.mosuke.tech/entry/2015/05/08/174732/</link>
      <pubDate>Fri, 08 May 2015 17:47:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/05/08/174732/</guid>
      <description>Sinatraアプリケーションで、JSファイルを圧縮するsinatra-assetpackを利用していて、
production環境で動作させようとしたら動かなくなってしまった問題について調査した。
起こったこと Sinatraを使ってアプリケーションを作っていて、development環境で完成したので、 prorudction環境で動作させようとしたら、jsのエラーが出るようになってしまい、正常に動かなくなった。
アクセスすると、以下のエラーがでる。要はjqueryがないとのこと。
Uncaught ReferenceError: $ is not defined  jQueryはもちろん読み込ませてるし、なんでproduction環境でだけ？？？
ソースコード sinatraのメインアプリケーションであるapp.rbには以下のように、sinatra-assetpackを利用してjsを読み込んでいる。
assets do serve &#39;/js&#39;, from: &#39;public/js&#39; serve &#39;/bower_components&#39;, from: &#39;bower_components&#39; js :app, &#39;/js/app.js&#39;, [ &#39;/js/index.js&#39;, ] js :libs, &#39;/js/libs.js&#39;, [ &#39;/bower_components/jquery/dist/jquery.js&#39;, &#39;/bower_components/bootstrap/dist/js/bootstrap.js&#39;, ] js_compression :jsmin end  layout.erbにはもちろん、libs.jsが先に来るように記述している。
&amp;lt;%= js :libs %&amp;gt; &amp;lt;%= js :app %&amp;gt;   sinatra-assetpackの挙動 productionでのみ発生する事象なので、改めてsinatra-assetpackのproduction環境時の挙動を確認した。
production環境では、複数のjsファイルを1つのファイルにまとめ、圧縮を行う。
development環境 ３つのjsファイルがあったら以下のように３つ別々に読み込まれる。
&amp;lt;script type=&#39;text/javascript&#39; src=&#39;https://blog.mosuke.tech/js/vendor/jquery.283479.js&#39;&amp;gt;&amp;lt;/script&amp;gt; &amp;lt;script type=&#39;text/javascript&#39; src=&#39;https://blog.mosuke.tech/js/vendor/underscore.589491.js&#39;&amp;gt;&amp;lt;/script&amp;gt; &amp;lt;script type=&#39;text/javascript&#39; src=&#39;https://blog.mosuke.tech/js/app/main.589491.js&#39;&amp;gt;&amp;lt;/script&amp;gt;  production環境 ３つあったjsファイルは1つにまとめられ、また圧縮される。</description>
    </item>
    
    <item>
      <title>Ansibleで最新のMySQLをインストールする際にハマったこと。MySQL-shared-compatのこと。</title>
      <link>https://blog.mosuke.tech/entry/2015/04/15/171127/</link>
      <pubDate>Wed, 15 Apr 2015 17:11:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/04/15/171127/</guid>
      <description>CentOS 6.5環境でAnsibleを使って最新のMySQLのセットアップをしようと思った際にハマったことをまとめた。
本質的にはAnsibleというよりLinux RPMパッケージのはなし。
ついでに、しょっぼいgithubを公開しました。
(1) 本記事を書くに至った経緯  Ansibleでmysqlを使ったサーバを構築(CentOS6.5)することになった。   MySQLのバージョンは5.6を採用した。    MySQLの公式rpmをダウンロードしインストールした。  インストールしたもの：MySQL-client, MySQL-devel, MySQL-server, MySQL-shared    MySQL-sharedをインストールする際にデフォルトのmysql-libsと競合  mysql-libsをアンインストールし再インストール AnsibleでMySQLの操作をするにはMySQL-pythonが必要なのでインストール  MySQL-pythonをインストールするにはさっきアンインストールしたmysql-libsが必要…(困った)  MySQL-shared-compatの存在に気づく 備忘録に書いておくか…  (2) MySQL-shared-compatの存在 mysql-libsは多くのパッケージの依存となっており、公式のMySQL5.6をインストールすることで、
他のパッケージがいれられない状況となっていた。
そんな状況を解決するためにMySQL-shared-compatというパッケージが用意されていた。
MySQL-shared-compatは「過去のMySQLバージョン向けの共有クライアントライブラリが納められているもの」だ。
詳細は下記参照をおすすめ。
MySQL-5.5.6から仕様が変わった「MySQL-shared-compat」の中身を徹底解剖 - Y-Ken Studio
ちなみに&#34;compat&#34;という単語がよく使われるが&#34;compatibility&#34;の略で「互換性」とかそういう意味。
(3) Githubで公開しました 内容は今のところ死ぬほど薄いのだが、MySQLをインストールするansibleを公開しました。 mosuke5/mysql-ansible · GitHub
内容はあれだが、特徴としては、インターネット上からRPMをダウンロードしてインストールする際に、
Ansibleでも「ダウンロード」→「インストール」の流れを踏む人が多いが、以下のようにするとシンプルになる。
varsでインストールしたいrpmやその取得先を記述しておいて、task側ではyumでnameにvarsで定義した変数を読むだけでできる。
role/mysql/vars/main.yml
mysql_url: http://ftp.jaist.ac.jp/pub/mysql/Downloads/MySQL-5.6 mysql_ver: &amp;quot;5.6.24-1&amp;quot; mysql_rpms: - MySQL-client-{{ mysql_ver }}.el6.x86_64.rpm - MySQL-shared-compat-{{ mysql_ver }}.el6.x86_64.rpm - MySQL-shared-{{ mysql_ver }}.</description>
    </item>
    
    <item>
      <title>SSHエージェントフォワード後に他のユーザでgit cloneする(鍵を使う)ことに関する考察</title>
      <link>https://blog.mosuke.tech/entry/2015/04/05/212518/</link>
      <pubDate>Sun, 05 Apr 2015 21:25:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/04/05/212518/</guid>
      <description>SSHのエージェントフォワードした後に、接続したユーザとは別のユーザでgit cloneしたいことがあった。
それについて調べていく中で学習したことや検討したことについてまとめた。
0. 前提 ローカルのPC(Mac)上で、Vagrantを使用してCentOS7の仮想サーバ(testsv)を立ち上げている。
&amp;lt;IPアドレス&amp;gt;
ローカルPC：192.168.33.1
仮想サーバ：192.168.33.100
本記事上での「git cloneする」とは、「プライベートのGitレポジトリからSSHを利用してクローンする」ということを指す。
1. SSHのエージェントフォワードを利用したい理由 まず、そもそもなぜSSHのエージェントフォワードをする必要があったのか。
最近では多くの方がご存知かつ利用していることだと思うが、仮想のサーバ上でgitを利用するときによく利用する。
(もちろんそれだけの用途ではありません)
仮想サーバを作るたびにSSHの鍵を生成して、Github等に登録するのが手間なので、
ローカルのPCの鍵を他のサーバへ引き継ぐことでgit clone等を可能にするのだ。
2. SSHエージェントフォワード利用時の挙動 SSHのエージェントフォワードで利用される認証情報は、接続先サーバの/tmp以下に保存されます。
[myuser@localpc ~]$ ssh -A vagrant@192.168.33.100 Last login: Sat Apr 4 xx:xx:xx 2015 from 192.168.33.1 [vagrant@testsv ~]$ [vagrant@testsv ~]$ ls -l /tmp | grep ssh drwx------. 2 vagrant vagrant 23 4月 4 11:35 ssh-skQVHsUCHU  
また、接続ユーザにはSSH_AUTH_SOCKという環境変数ができ、どの認証情報を利用するか記述がされます。
実際に確認してみる。
確認方法は、envコマンドで環境変数一覧を表示し、そのなかで&#34;ssh&#34;を含むものをgrep。
[vagrant@testsv ~]$ env | grep -i ssh SSH_AUTH_SOCK=/tmp/ssh-skQVHsUCHU/agent.9034 SSH_CLIENT=&#39;192.168.33.1 58017 22&#39; SSH_CONNECTION=&#39;192.</description>
    </item>
    
    <item>
      <title>Ruby, thin(bundler利用)を使った環境でのアプリの自動起動設定</title>
      <link>https://blog.mosuke.tech/entry/2015/02/22/211316/</link>
      <pubDate>Sun, 22 Feb 2015 21:13:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/02/22/211316/</guid>
      <description>bunlderを使ったWebアプリをプロダクション環境で動かすときに、アプリの起動をどうやって実現しているだろうか。
Apache Passengerを使う場合には、Apacheの起動がアプリの起動につながるので、 アプリの起動はあまり気にしなかったかもしれない。
しかし、例えばNginx × Unicorn/thinの構成などの場合はUnicornやthinの起動もしなければいけなくなってくる。
（あるいはこのようなケースがあるかは謎だが、Unicornやthinを単体で動かそうとしている場合など）
Unicornやthin（例ではthinを扱うが本質は同じ）の自動起動を実現する際の勘所、注意事項をまとめた。
0. 前提  CentOS6.5上でRubyでのWebアプリケーションを作っている。  アプリケーションサーバはthinを利用している。 また、gemパッケージ管理にbundlerを利用している。  1. 開発環境でよくするアプリの起動 開発環境では、アプリケーションのログの閲覧性なども兼ねて以下のようにアプリを起動していた。
$ bundle exec rackup $ bundle exec thin start  でも、これではいつまでたってもプロダクション環境での利用はできません。
2. 上記方法ではプロダクション環境で利用できない理由 当然のことながら、プロダクション環境ではいちいち手動でコマンドを実行しアプリケーションを立ち上げるわけにはいかない。
例えば、なんらかの理由でサーバが再起動してしまった場合には、
このままではアプリケーションが自動的に立ち上がらないため、サービスの停止につながってしまう。
ではどうするのか？
以下の状態であることがプロダクション環境では理想なのではないだろうか？
 オリジナルアプリケーションもserviceコマンドで起動・停止ができる  他のサービスと同様の操作方法が可能なのでわかりやすい   サーバ立ち上げ時にサービスが自動で起動される  3. 起動スクリプトを作ろう 上記の状態にもっていくためには、起動スクリプトを作らなければならない。
起動スクリプトを作る…！？
「作ったことないし、すぐには作れないよ〜」って思うかもしれないが、
サンプルはたくさんあるし、よく見てみるとそれほど難しくはない。
thinを使ったサンプルを探そうと思うと数は少ないが、Unicornも同じ仕組なので、 &#34;unicorn init script&#34;なんて検索をかけてもいろいろでてくるのでおすすめ。
参考ししたもの
https://gist.github.com/sbeam/3454488
上を参考にしながら、こんな起動スクリプトを作ってみた。（未完成版）
これを/etc/init.d以下へ配置する。
#!/bin/bash ### BEGIN CHKCONFIG INFO # chkconfig: 2345 55 25 # description: sample-app ### END CHKCONFIG INFO SCRIPT_NAME=/etc/init.</description>
    </item>
    
    <item>
      <title>勘違いしやすいFTPとSFTPの転送モードの話</title>
      <link>https://blog.mosuke.tech/entry/2015/02/17/220526/</link>
      <pubDate>Tue, 17 Feb 2015 22:05:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/02/17/220526/</guid>
      <description>FTPやSFTPでの転送モードの話。
ついこの前、WinSCPを利用している人が転送モードを選んでいて、
「SFTPには転送モードはないと思っていたのに、転送モードを選んでいる！？」
と疑問に思ったのでその辺りまとめた。
 FTPのバイナリーモードとアスキーモード 入社しはじめの頃、それまでSFTPしかほとんど使ったことなかったので、
先輩に「FTPではバイナリーモードを使って…」と言われて、意味が理解できなかったときがあったのを思い出す。
FTPにはファイル転送モードが２つあって、ちゃんと理解していないと思わぬところで痛い目にあう。
 バイナリーモード：ファイルの改行コードを変換せず転送する。  アスキーモード：OS側で異なる改行コードを自動的に修正して転送する。  FTPでは標準ではアスキーモードのため、なにも考えずにファイルを送るとファイルが壊れてしまったりする。
昔にミスしたのはWindowsからLinuxへtar.gzファイルをアスキーモードで送って、解凍したらファイルが壊れていたが、
それに気付かず壊れたファイルをサーバへ設置してしまったとか。
SFTPには転送モードはあるのか？ SFTPを普段から使ってる人は転送モードなんて気にしたことあまりないと思う。
FTPでは気にしなければいけない転送モード、SFTPでは気にしなくていいのだろうか？
結論から言うと、SFTPには転送モードはないので、気にする必要はない。
SFTPでは、FTPでいうバイナリーモードでファイル転送をするようだ。
sftpコマンドのマニュアルにも特に転送モードについては記述がないのがわかる。
http://www.unixuser.org/~euske/doc/openssh/jman/sftp.html
SFTPでも転送モードを選択できる場合がある！？ ここで、SFTPにも転送モードはあるぞ！？と疑問を思った人もいるかもしれない。
確かにWinSCPなどファイル転送ソフトを使っていると転送モードを選ぶことができる場合もある。
しかし、勘違いしてはいけないのが、
転送モードを選ぶことができるのはSFTPの機能ではなくてファイル転送ソフトの機能であるということだ。
まとめ ファイル転送でよく使われるFTPやSFTP。
それぞれに違いはあるし、それを利用するソフトウェアによっても違いがある。
何が何を行っているか把握し、思わぬミスを減らしましょう。</description>
    </item>
    
    <item>
      <title>SSHポートフォワード、https接続をするときに間違えやすいこと</title>
      <link>https://blog.mosuke.tech/entry/2015/02/11/172123/</link>
      <pubDate>Wed, 11 Feb 2015 17:21:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/02/11/172123/</guid>
      <description>SSHローカルフォワードの話。
前回は簡単に実践してみたというのを書いたのだが、今度は実際に使ってみてハマった部分があったのでメモ。
SSHでローカルポートフォワードを実際に試す - Goldstine研究所
1. やりたいこと httpsでしか接続を許可していないサーバへ、SSHローカルフォワーディングを使って接続しようした。
（直接疎通性がないためにポートフォワーディングする必要があった。）
2. 行ったこと httpsでしか接続ができないので、ローカル端末のポート5000を接続したいサーバのポート443に飛ばせばおっけーと思って、
下記のようにssh接続とブラウザから接続を行った。
ssh -L5000:web-host:443 user@ssh-host  ※web-host: 今回httpsで接続したサーバ
※ssh-host: ssh接続先サーバ
これでローカルフォワーディングの設定は終わったので、ブラウザから以下に接続するだけで終わりだと思っていた。
http://localhost:5000  が、接続不可…なぜでしょう？
3. 何が間違いだったか 正しくは以下で接続をしなければいけない。httpsが必要。
https://localhost:5000  よーく考えればアタリマエのこと。
URLのはじめの&amp;lt;http(s)&amp;gt;の部分はプロトコルで最後の&amp;lt;:5000&amp;gt;の部分はポート番号。
httpsは443のポートを一般的に使うが、ポート443がhttpsというわけではない。
あたりまえのことだし知っていることなんだけど、見落としがちかもしれない。</description>
    </item>
    
    <item>
      <title>リモートのサーバでdockerを起動させるときの端末割り当て</title>
      <link>https://blog.mosuke.tech/entry/2015/02/07/144208/</link>
      <pubDate>Sat, 07 Feb 2015 14:42:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/02/07/144208/</guid>
      <description>自分がハマったのでメモ。
リモートのサーバでdocker runを実行し(/bin/bash)、ローカル側でシェルを操作したかった。
sshでリモートサーバに接続し、docker runすればいいや、と思い以下を実行してみた。
ssh user@host &#39;docker run -t -i image_name /bin/bash&#39;  そうすると
[root@0c6742f02bd9 ~]# [root@0c6742f02bd9 ~]# ^[[A^[[A^[[C  エンターを押すと2行されるし、矢印キーはキーコードがでてしまう。
これを解消するのには以下のようにすればいい。
ssh -t user@host &#39;docker run -t -i image_name /bin/bash&#39;  -tとはなんなのか、なぜこのような事象が起きたのか、これからしっかり調べる。</description>
    </item>
    
    <item>
      <title>Ansible、コマンド実行結果を&amp;quot;ok&amp;quot;にする（冪等性を保つ方法）</title>
      <link>https://blog.mosuke.tech/entry/2015/02/02/201008/</link>
      <pubDate>Mon, 02 Feb 2015 20:10:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/02/02/201008/</guid>
      <description>Ansibleでソースコードインストールする際とか
すでにインストールされているかのチェックなどで、
シェルコマンドを実行してその結果で判断したい時がある。
ぼくがよくやる例では以下とか。
- name: check httpd installed command: which httpd ignore_errors: true  なんですが...
こうやってしまうと、仮に既にインストールされていて、正常なときでも&#34;changed&#34;と表示されてしまう。
これでは、本当にchangedなものなのか、わからなくなってくる。
これを解決するのにchaged_whenを使うといい。
- name: check httpd installed command: which httpd ignore_errors: true changed_when: false  こうするとコマンドが成功した際には&#34;ok&#34;が表示される。
これで、何も変化がないときにはokとskippingしかでないから、
誰がみても結果がわかりやすいですね！
秘伝のタレ回避！</description>
    </item>
    
    <item>
      <title>dockerで特定ユーザでログインした状態のシェル環境を提供する</title>
      <link>https://blog.mosuke.tech/entry/2015/01/24/213255/</link>
      <pubDate>Sat, 24 Jan 2015 21:32:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/01/24/213255/</guid>
      <description>dockerの一般的な利用の仕方ではあまり想定されないケースかもしれないが、
特定のユーザでログインした状態のコンテナを作りたいという場面に遭遇した。
&amp;lt;やりたいこと&amp;gt;  特定のユーザでログインした状態のシェルを提供すること その際、ユーザの.bash_profile（あるいあは.bashrc）を読み込んだ状態であること  .bash_profileに記載したPATHやaliasを使いたい ユーザのログインシェルを利用したい（カスタマイズされたシェルとか）    docker runに-uオプションがあるし、これで余裕！と思った。
[host] $ sudo docker run -u=user_name -i -t image_name /bin/bash  しかし…以下を確認してみると…
[docker] $ pwd [docker] $ echo $PATH [docker] $ alias  ディレクトリは &#34;/&#34; だし、PATHも通ってない。
どうやら.bash_profileなどは読んでいないようだ。普通にログインした状態とは違う。
dockerで-uでユーザを指定し場合、指定したuserでコマンドを実行するが、
サーバにユーザでログインしてからコマンドを実行するわけではないらしい。
ディレクトリはどうやら-wオプションで解決できるようだが…
[host] $ sudo docker run -u=user_name -w /home/user_name -i -t image_name /bin/bash  [docker] $ pwd /home/user_name  ディレクトリはおっけーだが、当然ながら依然として.bash_profileはダメ。
そこでふと思いついた。-uも-wもいらない。
あの手があるではないか…！！
[host] $ sudo docker run -i -t image_name su - username  suでスイッチユーザすれば.</description>
    </item>
    
    <item>
      <title>スーパーサーバってなに？ xinetdでサービスを常駐起動せずに利用する</title>
      <link>https://blog.mosuke.tech/entry/2015/01/02/013658/</link>
      <pubDate>Fri, 02 Jan 2015 01:36:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/01/02/013658/</guid>
      <description>使用頻度の低いサービスのデーモンをメモリに常駐させておくのは効率が悪い。
そこでスーパーサーバという使用頻度の低いサービスの窓口のサービスのみ起動しておき、要求があったときだけ特定のサービスを起動させることが可能らしい。
ということで、そのスーパーサーバとやらを実際に触ってみた。
スーパーサーバというとinetdとxinetdがあるらしいが、
xinetdはinetdの拡張版で、アクセス制御などの機能を搭載しているとのこと。
今回はxinetdを設定してみる。
1. 事前準備 【環境】
Vagrantで構築したCentOS 6.5
(仮想環境のIPアドレスは192.168.33.10)
まずはスーパーサーバで管理するサービスを考えなければならない。
SSHとかhttpはどう考えてもスーパーサーバの管理するようなものではないだろうし…
FTPやtelnet、POP3なんかのサービスに利用されることが多いそう？（このへんよくわかない）
今回はFTPをスーパーサーバの管理対象とした。
※本来は複数のサービスを管理対象とするからこそ意味がある。
まずはxinetdとvsftpをインストール
$ sudo yum install xinetd vsftpd  
xinetdどうこうの前に、ftp接続がきちんとできるか確認するのでサービスを起動。
$ sudo service vsftpd start  
ローカルPCから接続できることを確認する。
$ ftp 192.168.33.10 Connected to 192.168.33.10. 220 (vsFTPd 2.2.2) Name (192.168.33.10:username):  2. xinetdの設定 xinetdの基本設定は/etc/xinetd.confにかかれており、
xinetdで管理する各サービスの設定は/etc/xinetd.d/配下に書く方式。
ftpの設定を以下の通りにした。
&#34;service&#34;のあとに書くサービス名称は/etc/servicesに定義されているものを記載する。
vsftpとか書いても動かないので注意。
$ sudo vim /etc/xinetd.d/ftp service ftp { disable = no socket_type = stream wait = no user = root server = /usr/sbin/vsftpd log_on_failure += USERID }  設定項目については以下参照。</description>
    </item>
    
    <item>
      <title>2014年を思い返して…</title>
      <link>https://blog.mosuke.tech/entry/2015/01/01/161826/</link>
      <pubDate>Thu, 01 Jan 2015 16:18:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2015/01/01/161826/</guid>
      <description>2015年になってしまいました。
2014年に技術分野で印象に残ってること３つを思い返してみる。
1. インフラ会 @mogulla3と軽いノリで始めたインフラ会。
普段触らない、あるいは触ったとしてもすでに構築された環境で触ることの多いインフラ技術について
土日を使って自らの手で構築してみるといった会。
10月くらいから初めて以下を実際にやってみた。
 仮想化　：Docker, Vagrant  リバースプロキシ　：Nginx  ロードバランサー　：HAProxy  クラスタリング　：Pacemaker+corosync(heartbeat)  VPN　：SoftEther  構成管理ツール　：Ansible 自作PC  やったことについては全てではないがブログにまとめている。
 【VPS1台でインフラ勉強】サーバ複数台構成、Nginxでリバースプロキシ構築 - Goldstine研究所 【VPS1台でインフラ勉強】HAProxyでロードバランサーを構築 - Goldstine研究所 【VPS1台でインフラ勉強】SoftEtherを使ってVPN構築 - Goldstine研究所 【年末遊び】秋葉原で自作PCパーツ集めて作った - Goldstine研究所  このインフラ会では３つを目標にしてたけど、これが仕事でも本当に役立った。
会もそうだが、目標にしていたマインドは今後もぜひ続けていきたい。
 &#34;なんとなく知っている&#34;をなくす 考える引き出しをふやす 自らの手で実践する  &amp;lt;大変参考になった書籍&amp;gt; 
[24時間365日] サーバ/インフラを支える技術 ?スケーラビリティ、ハイパフォーマンス、省力運用 (WEB+DB PRESS plusシリーズ)
  作者: 安井真伸,横川和哉,ひろせまさあき,伊藤直也,田中慎司,勝見祐己  出版社/メーカー: 技術評論社   発売日: 2008/08/07  メディア: 単行本（ソフトカバー）  購入: 133人 クリック: 2,270回 この商品を含むブログ (289件) を見る     2.</description>
    </item>
    
    <item>
      <title>SSHでローカルポートフォワードを実際に試す</title>
      <link>https://blog.mosuke.tech/entry/2014/12/31/170545/</link>
      <pubDate>Wed, 31 Dec 2014 17:05:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2014/12/31/170545/</guid>
      <description>SSH・・・
いろんなことができる&amp;rdquo;らしい&amp;rdquo;ということは知っていたし、
例えばポートフォワーディングなんていうこともできるのも知っている。
でもそれがどんなものなのか自分の手で試したことはないし、
なんとなくただのサーバログインツールとして利用していた。
サーバインフラ技術に興味を持つようになっていろんなことを勉強していると、
ポートフォワーディングは必須な技術であることも感じ、
年末の持て余す時間を使ってSSHのローカルポートフォワードを試してみたのでメモする。
環境とやりたいこと 
ローカルPCからexample.jpは以下で接続できる状態。
[local pc] $ ssh username@example.jp  またWebサーバのポート番号は80で、VPSのホストサーバからはWebサーバへ接続できる。
CUIなのでわかりづらいがcurlを打つとHello Worldが返ってきている。
[example.jp] $ curl 192.168.33.10 &amp;lt;html&amp;gt;&amp;lt;body&amp;gt;&amp;lt;h1&amp;gt;Hello World! Virtual Web Server&amp;lt;/h1&amp;gt; &amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;  ローカルポートフォワードの実行 ローカルPCから以下を実行する。
以下のコマンドは、
ローカルPCのポート8888での接続はexample.jpからみて「192.168.33.10のポート80」に飛ばす
といったもの。
[local pc] $ ssh -L8888:192.168.33.10:80 username@example.jp  この状態でブラウザからhttp://localhost:8888にアクセスすると
なんと、ローカルホストにつないだはずが、example.jp上の仮想Webサーバのhtmlがうつっているではないか…

セキュリティはどうなの？ ローカルの端末からhttpで仮想のWebサーバに接続しているけど、セキュリティはだいじょうぶなのだろうか？
http通信だから暗号化はされていないのだろうか？
なんて疑問もあるが、それは問題ない。
SSHでローカルPCからリモートホスト(example.jp)までトンネルをはり、
そのなかをhttpで通信しているから全く問題なしといえる。
ローカルポートフォワードを利用する場合は、おそらく表には出したくないけど、
ローカルから接続したいなどの要件の時に使うと思うけど、SSHできちんと暗号化しているので安心。
最後に SSHでできることはこの他にもたくさんある。
ポートフォワードだけみても他にも「リモートポートフォワード」や「ダイナミックポートフォワード」などがある。
リモートポートなどはいまぱっと試せる環境がなかったのでおいおい実践してみる…
SSHをただのログインツールだけに留めるのは今年で終わりにしよう…</description>
    </item>
    
    <item>
      <title>【年末遊び】秋葉原で自作PCパーツ集めて作った</title>
      <link>https://blog.mosuke.tech/entry/2014/12/29/154251/</link>
      <pubDate>Mon, 29 Dec 2014 15:42:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2014/12/29/154251/</guid>
      <description>自作PCを作ったのでその材料と参考資料などを簡単にまとめる。
すべての部品は秋葉原のツクモで揃えた。
パソコン・ゲームPC・自作パソコンなら【TSUKUMO】 わけがあって荷物はすべて手で運んだのだが、かなり重くなるので郵送をおすすめする。
以下買ったパーツ一式や用意した道具。
ちなみに全部で54000円程度でした。（お店で買うとセット割などがあったため）
材料・パーツ一式 マザーボード 
ASUSTeK Intel H97チップセット搭載マザーボード H97-PRO 【ATX】
  出版社/メーカー: Asustek   発売日: 2014/05/17  メディア: Personal Computers この商品を含むブログを見る      CPU 
Intel CPU Core-i3-4160 3.60GHz 3Mキャッシュ LGA1150 BX80646I34160 【BOX】
  出版社/メーカー: インテル   発売日: 2014/07/27  メディア: Personal Computers この商品を含むブログを見る    
 メモリ 価格.com - Team TED38192M1600C11DC [DDR3 PC3-12800 4GB 2枚組] 価格比較</description>
    </item>
    
    <item>
      <title>【VPS1台でインフラ勉強】SoftEtherを使ってVPN構築</title>
      <link>https://blog.mosuke.tech/entry/2014/12/07/212156/</link>
      <pubDate>Sun, 07 Dec 2014 21:21:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2014/12/07/212156/</guid>
      <description>VPS1台でインフラ勉強シリーズでVPN構築を行ったのでそのメモ。
1. 実施したこと VPSのホストサーバをVPSサーバとし、その上で仮想で立ち上げたサーバ(ローカルネットワーク)に外部から接続できるようにすること。
 
  2. 環境 ・VPNソフトウェア：SoftEther VPN プロジェクト - SoftEther VPN プロジェクト
・メモリ：１GB
・CPU：仮想２コア
・HDD：100GB
・OS：CentOS7
・サーバ仮想化：Vagrant(Utuntu13)
 3. SoftEtherのインストール インストール手順は公式ドキュメント通りなので簡単にコマンドのみ記述しておく。
7.3 Linux へのインストールと初期設定 - SoftEther VPN プロジェクト
【ダウンロードしたもの】
ここから環境に応じて対象のソフトウェアを選ぶ
SoftEther ダウンロード センター
・コンポーネント：SoftEther VPN Server
・プラットフォーム：Linux
・CPU：Intel x86 / AMD64(64bit)
## ダウンロード $ wget ttp://jp.softether-download.com/files/softether/v4.08-9449-rtm-2014.06.08-tree/Linux/SoftEther%20VPN%20Server/64bit%20-%20Intel%20x64%20or%20AMD64/softether-vpnserver-v4.08-9449-rtm-2014.06.08-linux-x64-64bit.tar.gz ## 解凍 $ tar zxvf softether-vpnserver-v4.08-9449-rtm-2014.06.08-linux-x64-64bit.tar.gz ## 実行可能ファイル作成 $ cd vpnserver/ $ sudo make ## /usr/localへ配置 $ sudo mv vpnserver /usr/local ## パーミッション変更 ### 基本的には600。実行ファイルのみ700 $ cd /usr/local/vpnserver $ sudo chown root:root .</description>
    </item>
    
    <item>
      <title>Ansible, sudoパスワード要求を忘れただけでめんどくなる</title>
      <link>https://blog.mosuke.tech/entry/2014/11/28/001748/</link>
      <pubDate>Fri, 28 Nov 2014 00:17:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2014/11/28/001748/</guid>
      <description>AnsibleをVagrant上でずっと使ってて、Playbookも完成したし本番サーバへ&amp;hellip;
と思ったところである初歩的な罠にハマった。
本番環境へPalybook実行！！
$ ansible-playbook playbook.yml -i hosts  あれ、GATHERING FACTSで10分以上も待たされた&amp;hellip;
しかも、エラー出た&amp;hellip;
GATHERING FACTS failed to parse [ sudo via ansible, key= ..... ]  sudoできていない&amp;hellip;？
playbook内のsudo: yesを外して実行。
GATHERING FACTSは通過。
しかし、当たり前だがsudo で実行すべき部分で失敗&amp;hellip;
とても単純なことに気づいた&amp;hellip;
・Vagrant環境ではsudoのパスワードを要求されない
・本番環境はsudoのパスワードを要求されること
・sudoのパスワードを入力するようにしていなかったこと
というわけで-Kをつけて実行
$ ansible-playbook playbook.yml -i hosts -K  うまくいった&amp;hellip;
完全なる私のミスなんだが、ただ-Kオプションを忘れるだけで、一回の実行に10分ほども待たされるのは…。
しかもGATHERING FACTSで止まっているときはCtl+Cで中断も聞かなかった。
要注意ですね。。。
ちなみに、こんな方法で解決もできる。
sudoのパスワードを聞かれなくして対応。
# visudo user_name ALL=(ALL) NOPASSWD: ALL</description>
    </item>
    
    <item>
      <title>GithubクローンのGitlabの導入とその際のちょっとした注意点</title>
      <link>https://blog.mosuke.tech/entry/2014/11/22/190648/</link>
      <pubDate>Sat, 22 Nov 2014 19:06:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2014/11/22/190648/</guid>
      <description>もろもろな理由のためにGithubが利用できないことも多くあると思う。
というわけでGithubクローンのGitlabを試しに立ててみたが、簡単すぎでした…
GitLab | Open source software to collaborate on code
環境
さくらVPS 1Gプラン
OS：CentOS7
インストール
基本的にはドキュメントに書いてある以下のとおりで終わり。
$ curl -O https://downloads-packages.s3.amazonaws.com/centos-7.0.1406/gitlab-7.5.1_omnibus.5.2.0.ci-1.el7.x86_64.rpm $ sudo yum install openssh-server $ sudo systemctl enable sshd $ sudo systemctl start sshd $ sudo yum install postfix $ sudo systemctl enable postfix $ sudo systemctl start postfix $ sudo rpm -i gitlab-7.5.1_omnibus.5.2.0.ci-1.el7.x86_64.rpm $ sudo gitlab-ctl reconfigure $ sudo firewall-cmd --permanent --add-service=http # open up the firewall for HTTP and SSH requests $ sudo systemctl reload firewalld  しかし１つ気をつけないといけないことがある。</description>
    </item>
    
    <item>
      <title>後からGitレポジトリを共有設定に。sharedオプションの仕組みについて</title>
      <link>https://blog.mosuke.tech/entry/2014/11/20/230334/</link>
      <pubDate>Thu, 20 Nov 2014 23:03:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2014/11/20/230334/</guid>
      <description>Gitレポジトリを作って、複数人で開発をしていた。
しかし、レポジトリの中に作成されるファイルやディレクトリが個人のグループになってしまい、
Push, PullするときにPermission errorで怒られまくる。
ユーザには共通のグループを作っていたのに…なんでだっけ…
気づけばレポジトリを作るとき以下のようにしていた。
$ git init --bare  複数人で共有するときには以下のようにするべきであった。
$ git init --bare --shared  では、そもそもgitのsharedオプションをつけると何が裏で起こっているのか。
調べると「setgid」というキーワードに辿り着いた。
setgidの権限を付けておくと、そのディレクトリに作成されたファイルの所有グループは、そのディレクトリの所有グループになる。
以下のようにchmodでsetgidを付けることができる。
$ chmod g+s dir_name  setgidがつくとあまり馴染みのない権限がつく。
「drwxrwsr-x」
$ ls -l drwxrwsr-x 4 user group 136 11 16 22:49 test_dir  そして、すでに共有設定なしで作ってしまったレポジトリでは以下のように対応可能。
（新しくレポジトリつくるのはめんどいので…）
##Gigレポジトリ内のディレクトリに $ chmod -R g+s ./branches $ chmod -R g+s ./hooks $ chmod -R g+s ./info $ chmod -R g+s ./objects $ chmod -R g+s .</description>
    </item>
    
    <item>
      <title>Ansible、コマンドでワイルドカードを使うときの注意</title>
      <link>https://blog.mosuke.tech/entry/2014/11/18/225542/</link>
      <pubDate>Tue, 18 Nov 2014 22:55:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2014/11/18/225542/</guid>
      <description>AnsibleのPlaybookを書いていると、ワイルドカードを含んだコマンドを実行したい時がある。
そんなときあるところでハマった。
Apacheをソースインストールして、パスを/usr/sbinにリンクを貼ろうとして以下を実行した。
- command: ln -s /usr/local/httpd/bin/* /usr/sbin  /usr/sbin内に「*」というリンクが貼られてしまった。
* -&amp;gt; /usr/local/httpd/bin  どうやらcommandモジュールはワイルドカードに対応していないよう。
ワイルドカードを使いたいときはshellモジュールを利用すると良い。
- shell: ln -s /usr/local/httpd/bin/* /usr/sbin  また、*というリンクを消すときは要注意（笑）
$ rm ./*  とやってしまうとあたりまえだがやばいので
$ rm ./¥*  こうですね…</description>
    </item>
    
    <item>
      <title>Ansible、ソースインストールする際のPalybookの書き方</title>
      <link>https://blog.mosuke.tech/entry/2014/11/16/153223/</link>
      <pubDate>Sun, 16 Nov 2014 15:32:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2014/11/16/153223/</guid>
      <description>最近、Ansibleを使い始めたのだが、yumやapt-getでインストールできるものはいいけど、
どうしてもソースインストールが必要な場合がある。
ソースインストールを行う際のPlaybookの書き方と注意点をまとめた。
まず、あたりまえだが、ソースインストールを行うには以下のフローを踏まなければいけな。
1. ソースファイルの取得(tarで固められていると仮定)
2. tarファイルの解凍
3. 解答してできたディレクトリへ移動
4. configure
5. make
6. make install
また、Ansibleでは何回もPlaybookを実行していくため、
すでにインストールされている場合は、インストールをスキップする必要がある。
yumやapt-getで管理されていれば上記を心配することはないのだが、やはりソースインストールだとこの壁がある。
※パッケージ化しろよ！というツッコミは禁止
今回は例として、ubuntu13にemacsをソースインストールするのを例としてみた。
環境 【Ansible実行側】
さくらVPSの1G
OS: Centos 7
IPアドレス:192.168.33.1
【設定対象側】
上記さくらVPS上にたてたVagrantの仮想サーバ
OS: Ubuntu 13.10
IPアドレス:192.168.33.100
 Playbook 以下playbookの例。
--- - hosts: 192.168.33.100 user: vagrant sudo: yes vars: src_dir: /usr/local/src emacs_ver: emacs-23.4 tasks: ## emacsのソースファイルを取得済みか確認 - name: check exist emacs source file command: ls -l {{src_dir}}/{{emacs_ver}}.tar.gz ignore_errors: True register: result1 ## emacsのソースファイル取得。ただし、すでに取得済みならスキップ - name: get emacs source file command: chdir={{src_dir}} wget http://mirror.</description>
    </item>
    
    <item>
      <title>【VPS1台でインフラ勉強】多段SSH設定（おまけ）</title>
      <link>https://blog.mosuke.tech/entry/2014/11/09/172745/</link>
      <pubDate>Sun, 09 Nov 2014 17:27:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2014/11/09/172745/</guid>
      <description>VPS1台でインフラ勉強の会で、VPSのホストサーバ上に仮想でさらにいつくかのサーバを立てたが、
仮想のサーバにアクセスするには、ホストサーバにアクセスしてから更にSSHをしなければならない。
これが面倒だったので多段SSHの設定をして、一発でSSH接続できるようにした。
以下の図で言うと、web10, web11(192.168.33.10&amp;frasl;11)に一発でSSHできるようにする。

クライアントPC側に以下の設定をした。
$ vim ~/.ssh/config host gateway HostName xxxxx.xxx User username Host web10 HostName 192.168.33.10 User vagrant ProxyCommand ssh -W %h:%p gateway Host web11 HostName 192.168.33.11 User vagrant ProxyCommand ssh -W %h:%p gateway ##これで以下で接続可能 $ ssh web10 $ ssh web11  簡単でした。</description>
    </item>
    
    <item>
      <title>【VPS1台でインフラ勉強】HAProxyでロードバランサーを構築</title>
      <link>https://blog.mosuke.tech/entry/2014/11/09/171436/</link>
      <pubDate>Sun, 09 Nov 2014 17:14:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2014/11/09/171436/</guid>
      <description>前回の【VPS1台でインフラ勉強】サーバ複数台構成、Nginxでリバースプロキシ構築に続き、同様の環境を用いて、ロードバランサ構築を行った。
ロードバランサの構築にはHAProxyを利用した。
1. 環境 前回同様で、さくらVPSの1GBのプラン1台のみ。
・メモリ：１GB
・CPU：仮想２コア
・HDD：100GB
・OS：CentOS7
・サーバ仮想化：Vagrant(Utuntu13)
・ロードバランサ：HAProxy - The Reliable, High Performance TCP/HTTP Load Balancer
 2. 構成図 
 3. ロードバランサの構築 ■ホストサーバ側の設定
#HAProxyインストール $ sudo yum install haproxy #設定はすごく簡単で以下のファイルのみ。実際に $ sudo vim /etc/haproxy/haproxy.cfg #--------------------------------------------------------------------- # Example configuration for a possible web application. See the # full configuration options online. # # http://haproxy.1wt.eu/download/1.4/doc/configuration.txt # #--------------------------------------------------------------------- #--------------------------------------------------------------------- # Global settings #--------------------------------------------------------------------- global log 127.0.0.1 local6 debug chroot /var/lib/haproxy pidfile /var/run/haproxy.</description>
    </item>
    
    <item>
      <title>自宅サーバ公開時などのDDNS、固定IPについて整理</title>
      <link>https://blog.mosuke.tech/entry/2014/10/19/170854/</link>
      <pubDate>Sun, 19 Oct 2014 17:08:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2014/10/19/170854/</guid>
      <description>自宅サーバを公開するときに使うDDNSや固定IP。
それが必要な理由について図解的にまとめ。それだけ。</description>
    </item>
    
    <item>
      <title>【VPS1台でインフラ勉強】サーバ複数台構成、Nginxでリバースプロキシ構築</title>
      <link>https://blog.mosuke.tech/entry/2014/10/09/230555/</link>
      <pubDate>Thu, 09 Oct 2014 23:05:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2014/10/09/230555/</guid>
      <description>ロードバランシングとかクラスタリングとかリバースプロキシとか、
業務でも使っているし、概念とかはわかってるけど、自分で構築したことはやっぱりない。
自分で構築してみたいなーと思いつつもあたりまえだけど、サーバやネットワーク機器をそう簡単に調達もできない。
お金も当然ない。
というわけで、さくらVPSで仮想化つかってロードバランシングとかクラスタリングとかリバースプロキシとか勉強しましょうという「サーバインフラ会」を友人と始めた。
その第１回目のメモ。
第1回 サーバ複数台構成、Nginxでリバースプロキシ構築
第2回 HAProxyでロードバランサ構築


1. 使用した環境 まず今回利用した環境は以下のとおり。
さくらVPSの1GBのプラン。
・メモリ：１GB
・CPU：仮想２コア
・HDD：100GB
・OS：CentOS7
・仮想化：Vagrant
→dockerなどもはじめ検討していたが、コンテナ型仮想化だとサーバ感がでないので、よりサーバとして意識できるVagrantを採用
【参考】
料金・サービス仕様 | VPS（仮想専用サーバ）は「さくらのVPS」
 2. 完成イメージ・物理イメージ 

 
 3. VagrantでWebサーバ２台分を構築する Vagrantの詳細な利用方法は公式ドキュメントをみてもらうとするが、セットアップまでのひととおりの流れと注意点のみ記載する。
Vagrant Documentation
今回はWebサーバ２台を仮想で実現するので、それぞれweb1, web2とする。
それぞれのディレクトリを作成。
## web1, web2のディレクトリ作成 $ pwd /home/vagrant $ mkdir web1 $ mkdir web2 ## 仮想化で利用するOSイメージをダウンロード $ vagrant box add ubuntu1310 ¥ http://opscode-vm-bento.s3.amazonaws.com/vagrant/virtualbox/opscode_ubuntu-13.10_chef-provisionerless.box ## web1サーバ構築 $ cd web1 $ vagrant init ubuntu1310 ## ほぼほぼデフォルトの設定だが以下２つだけは設定を行った。 $ vim Vagrantfile # (1)プライベートアドレスの割り当て。 config.</description>
    </item>
    
    <item>
      <title>CentOS7, iptables設定でハマった</title>
      <link>https://blog.mosuke.tech/entry/2014/09/20/180326/</link>
      <pubDate>Sat, 20 Sep 2014 18:03:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2014/09/20/180326/</guid>
      <description>最近VPSのOSをcentos7にしたのだが、なかなか手付かずでiptablesの設定も放置していた…
（sshの最低限の設定はしていたが、ほんとうに良くない…）
久しぶりに手が空いたので設定するかーと思いきや
まず/etc/sysconfig/iptablesがないし&amp;hellip;
Cent7からのsystemctlでiptablesのサービスを確認してもでてこないし…
# systemctl status iptables iptables.service Loaded: not-found (Reason: No such file or directory) Active: inactive (dead)  というわけで、調べてみると、まずiptables.serviceをインスールしないといけないとのこと。
そして、centos7からはfirewalldがデフォルトでオンになっているからオフにしないといけない。
（いけないわけではないけど両方使う意味が無いので。）
まずはiptables-serviceをインスールし、firewalldをオフ、iptablesをオンとした。
# yum install iptables-services # systemctl status firewalld Loaded: loaded (/usr/lib/systemd/system/firewalld.service; enabled) Active: active (running) since Sat 2014-09-20 17:47:11 JST; 4s ago Main PID: 11162 (firewalld) CGroup: /system.slice/firewalld.service └─11162 /usr/bin/python -Es /usr/sbin/firewalld --nofork --nopid # # systemctl stop firewalld # # systemctl status firewalld Loaded: loaded (/usr/lib/systemd/system/firewalld.</description>
    </item>
    
    <item>
      <title>D3.js、DBからのデータ連携方法の検討</title>
      <link>https://blog.mosuke.tech/entry/2014/09/15/135611/</link>
      <pubDate>Mon, 15 Sep 2014 13:56:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2014/09/15/135611/</guid>
      <description>最近D3.jsを使う機会があり、DBのデータを読み込ませる方法についていくつか検討した。
例えば以下の状況を考える。
【やりたいこと】
「DBに格納されているデータを使って、D3.jsで折れ線グラフを描く」
【DBの構造】
カラム名：型
date : datetime
value : int
※また、下記ではSinatra上で行っているが、他の言語でも同様のことがいえる。
(1) 簡単なAPIのようなものを利用する 先に結論から書くと、今まで次の(2)(3)のようなやりかたをやっていたのだけれど、
これが一番複雑にならずに良いと思ったということ。
Sinatra側で/csvにアクセスするとcsvファイルをダウンロードできるようにする。
#Sinatra側 get &#39;/csv&#39; do content_type &#39;application/csv&#39; attachment &#39;download.csv&#39; #DBからデータ取得(Activerecord利用) @data = Model.all() #出力するCSVデータの変数。csvヘッダーを先につけている。 @csv = &amp;quot;date,value\n&amp;quot; #DBのデータをCSVの形にして格納 @data.each do |d| @csv += d.date.to_s + &amp;quot;,&amp;quot; + d.value.to_s + &amp;quot;\n&amp;quot; end #csvtest.erbというビューに出力 erb :csvtest, :layout =&amp;gt; false end  csvtest.erb
```ruby ``` こうすることで&#34;http://*****/csv&#34;にアクセスするとcsvファイルとしてダウンロードできる状態になる。
条件指定をしてデータをダウンロードできるようにしたい場合はGETでパラメータ指定できるようにすれば良いと思う。
（SinatraでCSVファイルを生成するところのコードがナンセンスだと思っているので、もっといい方法があるはず…）
また、今回はcsvにしているがjsonなどの他の形式でも同様のことがいえる。
これをD3.js側で以下のように読み込ませる。
d3.csv(&amp;quot;/csv&amp;quot;, function(error, data) { (中略) }  d3.</description>
    </item>
    
    <item>
      <title>2014/8/2 Arduino会を実施の報告</title>
      <link>https://blog.mosuke.tech/entry/2014/09/13/214414/</link>
      <pubDate>Sat, 13 Sep 2014 21:44:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2014/09/13/214414/</guid>
      <description>2014年8月2日(土)に大学時代の研究室の同期・後輩を巻き込んでArduino勉強会を行った。
Arduinoをほとんど触ったことのないメンバーで実施したのだが、初心者でもプロダクトを作り上げられるところまでできた。
その勉強会の簡単な資料を掲載。</description>
    </item>
    
    <item>
      <title>【Ethernetシールド】Arduinoをネットワークへ繋げる(1)</title>
      <link>https://blog.mosuke.tech/entry/2014/09/13/111537/</link>
      <pubDate>Sat, 13 Sep 2014 11:15:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2014/09/13/111537/</guid>
      <description>Ethernetシールドを使ってネットワークへ繋げる。
とりあえずは、スモールステップでPCとの1対1の通信。
PCとの1対1通信 構成図

Arduinoソースコード
#include &amp;lt;SPI.h&amp;gt; #include &amp;lt;Ethernet.h&amp;gt; byte MAC[] = {0x90,0xA2,0xDA,0x0F,0x8B,0xA2}; byte IP[] = {192,168,11,3}; void setup() { Ethernet.begin(MAC,IP); } void loop() { }  疎通確認
======= ```  4f3b2c72e32161441359f6d2116c410a27a8faaf % ping 192.168.100.3 PING 192.168.100.3 (192.168.100.3): 56 data bytes Request timeout for icmp_seq 0 64 bytes from 192.168.100.3: icmp_seq=1 ttl=128 time=0.576 ms 64 bytes from 192.168.100.3: icmp_seq=2 ttl=128 time=0.425 ms 64 bytes from 192.168.100.3: icmp_seq=3 ttl=128 time=0.</description>
    </item>
    
    <item>
      <title>【これだけは抑えておきたい】Arduino入門まとめ</title>
      <link>https://blog.mosuke.tech/entry/2014/07/21/231946/</link>
      <pubDate>Mon, 21 Jul 2014 23:19:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2014/07/21/231946/</guid>
      <description>前々からArduino自体はもっていたし、ちょろっと触ったことはあったのだが、
大学時代の研究室の仲間とArduino勉強会をやることになり、それに向けて友人と再入門したのでそのときのメモ。
Arduino入門にこれだけは知っておきたい事項をプログラム形式にまとめた。
もちろん、不足部分もたくさんあるが、これだけわかれば楽しめるようになるかも。
以下が入門プログラム。
 LEDの点灯 PCとArduino間のシリアル通信 超音波距離センサの利用 超音波距離センサとLEDの組み合わせ Processingとの連携 電池でArduinoを動作させる  【用意したもの】
基本は以下の２つ
Arduinoをはじめようキット
  出版社/メーカー: スイッチサイエンス  メディア: エレクトロニクス  購入: 64人 クリック: 1,164回 この商品を含むブログ (51件) を見る     みんなのArduino入門：拡張キット
  出版社/メーカー: スイッチサイエンス  メディア: エレクトロニクス この商品を含むブログを見る     【参考にした書籍】
Arduinoをはじめよう 第2版 (Make:PROJECTS)
  作者: Massimo Banzi,船田巧  出版社/メーカー: オライリージャパン   発売日: 2012/03/16  メディア: 単行本（ソフトカバー）  購入: 9人 クリック: 27回 この商品を含むブログ (10件) を見る     みんなのArduino入門</description>
    </item>
    
    <item>
      <title>【雑記】VPSとお別れしました…</title>
      <link>https://blog.mosuke.tech/entry/2014/05/18/142838/</link>
      <pubDate>Sun, 18 May 2014 14:28:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2014/05/18/142838/</guid>
      <description>2012年07月18日から契約していたVPSサーバを本日解約した。
約２年間の契約していたわけだがちょっと振り返るといろいろあったなーと思う。
ぼくがまだ大学３年のとき、ソーシャルブックマークサービスをリリースしたのだが、
その時はまだエンジニア初心者で、VPSではなく共有サーバを利用していた。
そんなプロジェクトが終わり、次のサービス(Twitterの解析ツール)の開発で初めてVPS環境を与えられたのだが、
サーバ構築など右も左もわからなかったので、自分の勉強用環境としてVPSを契約。VPS環境で行ったこと&amp;gt;
プロダクトとしては次の３つくらい作った
・Amazonの最安値段通知サービス
自分が欲しい物について希望価格を登録しておくと、Amazonの中古価格が希望価格を下回った時に通知してくれるサービス。
個人的には結構好きだったのだが、運用がめんどくさくなって閉鎖。
・うみなう ーTwitter上の「海なう」画像を眺めようー
夏場になるとTwitterで「海なう」と水着画像を付けて投稿する人が多い。
海なうのキーワードで出てきた画像をただただ眺めるだけのサービス(笑)
・キニナル ーきになるあの二人のツイートをー
Twitter上で気になる二人のツイート時系列で並べて監視するサービス。
公開はしていなかったけどちょくちょく身内でみていた。
こんなくだらないサービスを趣味で作っていた。
その他、仕事で使う新しいツールの実験導入などに利用していた。
社会人になってからリモートで接続できるサーバを持っている必要がなくなってきたので解約することになったが、
いろいろと勉強になったし、思い出がおおかったなー。
またいつか契約する日があることを待ち望んで…</description>
    </item>
    
    <item>
      <title>ロードバランサをロードバランスする</title>
      <link>https://blog.mosuke.tech/entry/2014/05/17/221824/</link>
      <pubDate>Sat, 17 May 2014 22:18:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2014/05/17/221824/</guid>
      <description>ぼくはネットワーク構成については素人です。
大量のトラフィックをさばくためにロードバランサを使うがロードバランサが耐えられない量のトラフィックにはどうするか。
ちょっと疑問に思ったので素人なりにちょろっと考えてみた。</description>
    </item>
    
    <item>
      <title>ER図作成ツール-MySQL Workbench- を使ってみた</title>
      <link>https://blog.mosuke.tech/entry/2014/05/06/192424/</link>
      <pubDate>Tue, 06 May 2014 19:24:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2014/05/06/192424/</guid>
      <description>表題の通りER図作成ツールというものを使ってみた。
開発をやる現場では当たり前のように使われているのかもしれないけど、ぼくは使ったことなかったです。
ER図とかテーブル構造はExcelでまとめて管理していました。
きっかけは、ソフトウエアデザインを読んでいてER図作成ツールなるものがでてきたので、便利そうと思ってぐぐってみた。
調べると主なフリーのER図作成ツールは以下があるっぽい。
 DBDesigner A5-SQLmk-2 AmaterasERD  MySQL Workbench ERMaster  (参考)http://treeapps.hatenablog.com/entry/20110505/p1
この中でMacで使えそうなのがMySQL Workbenchしかなかったのでこちらを利用。
本当はMySQL以外のDBMSにも対応しているものがよかったのだけど…
ざっと使ってみた感じ以下３つがよいと感じた点。
1.ER図作成が容易ER図作成ツールなのであたりまえなのだが、今までExcelの図形など使っていて書いていたので簡単さに驚き。
属性についてもそれぞれ型やキー・制約などの設定も記述できる。
2.DDLにすぐにエクスポート可能作成したER図についてはDDLにすぐにエクスポートできる。
なので、ツールでER図さえきちんと書けば実際のDB、テーブル構築は一瞬で可能。
3.イメージ図としてエクスポート可能作成したER図をPNG形式にエクスポートできるので、メンバー等との共有が容易。
また、ドキュメントとして残す際にもとても使えると思う。
Excelのシート形式にエクスポートできたらなぁとも思う…
実際にMySQL Workbenchを使って簡単にER図を作成してみた。
・ユーザが飲んだお酒を記録していくWebシステムです。
・利用するにはシステムへのユーザ登録が必要。
・ユーザはお酒マスターから飲んだお酒を登録する。
→同じお酒を何度登録（飲んでも）よい
・お酒には産地（都道府県）を記入する。
・ユーザは本システムへログイン後のアクセスログが取得される。
こんなざっくりとした感じ。
作成したER図はこんな感じに出来上がります（PNG）。

DDLはこんな感じに。
-- MySQL Script generated by MySQL Workbench -- Tue May 6 19:16:11 2014 -- Model: New Model Version: 1.0 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0; SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0; SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE=&#39;TRADITIONAL,ALLOW_INVALID_DATES&#39;; -- ----------------------------------------------------- -- Schema mydb -- ----------------------------------------------------- CREATE SCHEMA IF NOT EXISTS `mydb` DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci ; USE `mydb` ; -- ----------------------------------------------------- -- Table `mydb`.</description>
    </item>
    
    <item>
      <title>データベース、隔離性水準とはなにか</title>
      <link>https://blog.mosuke.tech/entry/2014/03/30/004337/</link>
      <pubDate>Sun, 30 Mar 2014 00:43:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2014/03/30/004337/</guid>
      <description>データベースには、トランザクションが持つべき特性であるACID特性というものを備えている。
ACID特性
  A Atomicity 原始性   C Consistency 一貫性   I Isolation 隔離性   D Dirability 耐久性   ACID特性については詳しく書かないが、その中に隔離性という以下の特性がある。
「複数のトランザクションを並行して実行しても直列に実行した時と同じ結果になる。
また、トランザクション実行中は変更前の状態として見える。」
しかし、複数のトランザクションを並列で実行すると、その隔離性を満たさない現象が発生することがある。
その現象は主に３つある。
1. ダーティリード[概要]
あるトランザクションの処理中に、別のユーザがそのトランザクションでまだコミットしてないデータを読み込んでしまう現象のこと。
Q. 何が問題なの？
A. 例えばそのトランザクションがロールバックしたとすると、存在しない処理のデータを読み込んでいることになってしまう。[対策]
対策はシンプルで、まだコミットしていないデータは読み込めないようにするだけ。
つまり、データを更新するときは「排他ロック」をかけるようにするということ。

2. ノンリピータブルリード[概要]
同一トランザクション内で、一度読み込んだデータを再読み込みすると値が異なる現象。[対策]
データを読み込むときには「共有ロック」をかけるようにするということ。
共有ロック中は他トランザクションから更新はできなくなるのでノンリピータブルリードは起きない。

3. ファントムリード[概要]
あるトランザクションが複数行ある結果を返す検索条件で問合せを2度実行する間に、
コミットされた別のトランザクションによってその条件を満たす新しい行が挿入されたり、削除された行がでたりする現象。[対策]
検索結果の範囲に対してロックをかけること。
上記現象が起こらないようにトランザクションの分離レベルが用意されている。
トランザクションの分離レベルと、現象に発生の有無
  分離レベル ダーティリード ノンリピータブルリード ファントムリード   Read Uncommitted あり あり あり   Read Committed なし あり あり   Repeatable Read なし なし あり   Serializable なし なし なし   ここでひとつの疑問が…</description>
    </item>
    
    <item>
      <title>VLAN、Trunkを使った実践的な構成のメモ</title>
      <link>https://blog.mosuke.tech/entry/2014/03/09/212520/</link>
      <pubDate>Sun, 09 Mar 2014 21:25:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2014/03/09/212520/</guid>
      <description>VLAN、Trunkそれぞれを一部分的に切り出した説明はよくあるが、
それらが組み合わさった実践的な構成についてあまり書いてないので、例を考えて載せてみる。

(1) L2スイッチに２つのシステムのサーバ（それぞれが冗長構成）がある。
(2) それぞれのシステムはVLANで切られている
(3) L2-L2間、L3-L2間はTrunkを使用(Vlan100,200)
(4) L3インターフェースはサブインタフェース利用
(5) L3の冗長はHSRPを利用
ほかにいろいろ思いついたら、付け加えます。</description>
    </item>
    
    <item>
      <title>【SQLテーブル結合】INNER JOIN とWHERE結合の違いについて その２</title>
      <link>https://blog.mosuke.tech/entry/2014/02/28/233025/</link>
      <pubDate>Fri, 28 Feb 2014 23:30:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2014/02/28/233025/</guid>
      <description>前回の「【SQLテーブル結合】INNER JOIN とWHERE結合の違いについて」の続き。
やはりFROM句の後にテーブルを複数書いてWHEREで結合させるのはナンセンスだと思う。
例えば、内部結合に加え、外部結合もある場合を考える。
WHERE型
SELECT * FROM table1, table2 #←結合させたいもの LEFT OUTER JOIN ( ・ ・ &amp;lt;副問い合わせとか長いSQLが入ると想定&amp;gt; ・ ・ ) AS sub ON table1.id=sub.id AND table2.id=sub.id WHERE table1.id=table2.id #←結合条件  table1とtable2を内部結合させたいのに、
肝心の何と何を結合させるかの部分の&amp;rdquo;WHERE table1.id=table2.id&amp;rdquo;が離れてしまい、
SQL文全体として何をしたいかわかりずらい。
一方、INNER JOINなら
SELECT * FROM table1 INNER JOIN table2 ON table1.id=table2.id #←結合させたいものと結合条件が一緒 LEFT OUTER JOIN ( ・ ・ &amp;lt;副問い合わせとか長いSQLが入ると想定&amp;gt; ・ ・ ) AS sub ON table1.id=sub.id  上記のように、結合させたいテーブルと結合させる条件がくっつくため、
全体としてなにをしたいかわかりやすい気がする。
慣れの問題なのかもしれんが、圧倒的にINNER JOINだろ…</description>
    </item>
    
    <item>
      <title>【SQL テーブル結合】INNER JOIN とWHERE結合の違いについて</title>
      <link>https://blog.mosuke.tech/entry/2014/02/28/232846/</link>
      <pubDate>Fri, 28 Feb 2014 23:28:00 +0900</pubDate>
      
      <guid>https://blog.mosuke.tech/entry/2014/02/28/232846/</guid>
      <description>テーブルを内部結合するとき、いつもINNER JOINを使って書いていたが、
「FROM句のあとに複数のテーブルを書いてWHERE句で絞り込む」やり方があることを知った。
INNER JOIN型
SELECT * FROM table1 INNER JOIN table2 ON table1.id=table2.id  WHERE型
SELECT * FROM table1 table2 WHERE table1.id=table2.id  結果的には同じだし、調べる限りだとどちらが良いとかメリット・デメリットとかないそう。
個人的には「何に何を結合しているのかわかりやすい」という点からINNER JOIN型をおすすめしたいところ。
DBスペシャリストの本とか問題みてるとだいたいWHERE型なので戸惑う…</description>
    </item>
    
  </channel>
</rss>